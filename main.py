#!/usr/bin/env python3
"""
LangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµè„šæœ¬
ç”¨äºå¤„ç†æ•°æ®é›†å¹¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import pandas as pd
import pyarrow.parquet as pq
from typing import Dict, Any, List
from langgraph.graph import StateGraph, END
import logging
from pathlib import Path
import re
import numpy as np
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å®šä¹‰å·¥ä½œæµçŠ¶æ€ç±»å‹
from typing import TypedDict

class WorkflowState(TypedDict):
    data: pd.DataFrame
    data_path: str
    integrity_check_results: Dict[str, Any]
    errors: List[str]
    status: str
    metadata: Dict[str, Any]

# ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
def generate_anomaly_report(state: WorkflowState, data_completeness: float, duplicate_rows: int, 
                           numeric_columns: list, categorical_columns: list, datetime_columns: list, 
                           missing_values: pd.Series, missing_cells: int) -> None:
    """
    ç”Ÿæˆå¼‚å¸¸æ£€æµ‹MDæŠ¥å‘Š
    """
    report_content = f"""# å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
- **æ•°æ®æ–‡ä»¶**: {state['data_path']}
- **ç”Ÿæˆæ—¶é—´**: {state['metadata']['processing_timestamp']}
- **æ–‡ä»¶å¤§å°**: {state['metadata']['file_size_mb']:.2f} MB

## æ•°æ®åŸºæœ¬ä¿¡æ¯
- **æ•°æ®å½¢çŠ¶**: {state['data'].shape[0]} è¡Œ Ã— {state['data'].shape[1]} åˆ—
- **æ•°æ®å®Œæ•´æ€§**: {data_completeness:.2f}%
- **é‡å¤è¡Œæ•°**: {duplicate_rows}

## æ•°æ®ç±»å‹åˆ†å¸ƒ
- **æ•°å€¼åˆ—**: {len(numeric_columns)} ä¸ª
- **åˆ†ç±»åˆ—**: {len(categorical_columns)} ä¸ª  
- **æ—¥æœŸåˆ—**: {len(datetime_columns)} ä¸ª

## åˆ—ä¿¡æ¯è¯¦æƒ…
### æ•°å€¼åˆ—
{', '.join(numeric_columns) if numeric_columns else 'æ— '}

### åˆ†ç±»åˆ—
{', '.join(categorical_columns) if categorical_columns else 'æ— '}

### æ—¥æœŸåˆ—
{', '.join(datetime_columns) if datetime_columns else 'æ— '}

## å¼‚å¸¸æ£€æµ‹ç»“æœ
"""
    
    if missing_cells > 0:
        report_content += "\n### ç¼ºå¤±å€¼å¼‚å¸¸\n\n| åˆ—å | ç¼ºå¤±æ•°é‡ | ç¼ºå¤±æ¯”ä¾‹ |\n|------|----------|----------|\n"
        for col, missing_count in missing_values.items():
            if missing_count > 0:
                missing_pct = missing_count / len(state["data"]) * 100
                report_content += f"| {col} | {missing_count} | {missing_pct:.2f}% |\n"
    else:
        report_content += "\n### ç¼ºå¤±å€¼æ£€æŸ¥\nâœ… æœªå‘ç°ç¼ºå¤±å€¼\n"
    
    if duplicate_rows > 0:
        report_content += f"\n### é‡å¤æ•°æ®å¼‚å¸¸\nâš ï¸ å‘ç° {duplicate_rows} è¡Œé‡å¤æ•°æ®\n"
    else:
        report_content += "\n### é‡å¤æ•°æ®æ£€æŸ¥\nâœ… æœªå‘ç°é‡å¤æ•°æ®\n"
    
    # æ•°æ®è´¨é‡è¯„çº§
    if data_completeness >= 95:
        quality_grade = "ä¼˜ç§€ âœ…"
    elif data_completeness >= 90:
        quality_grade = "è‰¯å¥½ âš ï¸"
    elif data_completeness >= 80:
        quality_grade = "ä¸€èˆ¬ âš ï¸"
    else:
        quality_grade = "è¾ƒå·® âŒ"
    
    report_content += f"\n## æ•°æ®è´¨é‡è¯„ä¼°\n- **æ•´ä½“è¯„çº§**: {quality_grade}\n- **å®Œæ•´æ€§å¾—åˆ†**: {data_completeness:.2f}%\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/anomaly_detection_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“Š å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

def generate_complete_report(state: WorkflowState) -> None:
    """
    ç”Ÿæˆå®Œæ•´çš„ç»¼åˆæŠ¥å‘Šï¼Œæ•´åˆå¼‚å¸¸æ£€æµ‹å’Œç»“æ„æ£€æŸ¥ç»“æœ
    """
    from datetime import datetime
    import os
    
    try:
        # è¯»å–å„ä¸ªåˆ†ææŠ¥å‘Š
        anomaly_report_path = "/Users/zihao_/Documents/github/W35_workflow/anomaly_detection_report.md"
        structure_report_path = "/Users/zihao_/Documents/github/W35_workflow/structure_check_report.md"
        sales_agent_report_path = "/Users/zihao_/Documents/github/W35_workflow/sales_agent_analysis_report.md"
        time_interval_report_path = "/Users/zihao_/Documents/github/W35_workflow/time_interval_analysis_report.md"
        
        anomaly_content = ""
        structure_content = ""
        sales_agent_content = ""
        time_interval_content = ""
        
        if os.path.exists(anomaly_report_path):
            with open(anomaly_report_path, 'r', encoding='utf-8') as f:
                anomaly_content = f.read()
        
        if os.path.exists(structure_report_path):
            with open(structure_report_path, 'r', encoding='utf-8') as f:
                structure_content = f.read()
        
        if os.path.exists(sales_agent_report_path):
            with open(sales_agent_report_path, 'r', encoding='utf-8') as f:
                sales_agent_content = f.read()
        
        if os.path.exists(time_interval_report_path):
            with open(time_interval_report_path, 'r', encoding='utf-8') as f:
                time_interval_content = f.read()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        complete_report = f"""# W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµ - ç»¼åˆåˆ†ææŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}
- **å·¥ä½œæµç‰ˆæœ¬**: W35 Anomaly Detection Workflow
- **åˆ†æèŒƒå›´**: æ•°æ®è´¨é‡æ£€æµ‹ + ç»“æ„å¼‚å¸¸åˆ†æ + é”€å”®ä»£ç†åˆ†æ + æ—¶é—´é—´éš”åˆ†æ

---

{anomaly_content}

---

{structure_content}

---

{sales_agent_content}

---

{time_interval_content}

---

## ç»¼åˆç»“è®º

### æ•°æ®è´¨é‡çŠ¶å†µ
ä»æ•°æ®è´¨é‡æ£€æµ‹ç»“æœæ¥çœ‹ï¼Œæ•°æ®çš„åŸºæœ¬å®Œæ•´æ€§å’Œä¸€è‡´æ€§æƒ…å†µã€‚

### ç»“æ„å¼‚å¸¸çŠ¶å†µ
ä»ç»“æ„æ£€æŸ¥ç»“æœæ¥çœ‹ï¼ŒCM2è½¦å‹ç›¸å¯¹äºå†å²è½¦å‹åœ¨åœ°åŒºåˆ†å¸ƒã€æ¸ é“ç»“æ„ã€äººç¾¤ç»“æ„æ–¹é¢çš„å˜åŒ–æƒ…å†µã€‚

### é”€å”®ä»£ç†åˆ†æçŠ¶å†µ
ä»é”€å”®ä»£ç†åˆ†æç»“æœæ¥çœ‹ï¼Œä¸åŒè½¦å‹åœ¨é¢„å”®å‘¨æœŸä¸­æ¥è‡ªStore Agentçš„è®¢å•æ¯”ä¾‹æƒ…å†µã€‚

### é‡å¤ä¹°å®¶åˆ†æçŠ¶å†µ
ä»é‡å¤ä¹°å®¶åˆ†æç»“æœæ¥çœ‹ï¼Œä¸åŒè½¦å‹åœ¨é¢„å”®å‘¨æœŸä¸­é‡å¤è´­ä¹°ï¼ˆåŒä¸€èº«ä»½è¯å·ç å¯¹åº”å¤šä¸ªè®¢å•ï¼‰çš„æƒ…å†µã€‚

### æ—¶é—´é—´éš”åˆ†æçŠ¶å†µ
ä»æ—¶é—´é—´éš”åˆ†æç»“æœæ¥çœ‹ï¼Œä¸åŒè½¦å‹åœ¨æ”¯ä»˜åˆ°é€€æ¬¾ã€æ”¯ä»˜åˆ°åˆ†é…ç­‰å…³é”®ä¸šåŠ¡æµç¨‹çš„æ—¶é—´æ•ˆç‡è¡¨ç°ã€‚

### å»ºè®®æªæ–½
1. **æ•°æ®è´¨é‡æ–¹é¢**: æ ¹æ®å¼‚å¸¸æ£€æµ‹ç»“æœï¼Œå¯¹å‘ç°çš„æ•°æ®è´¨é‡é—®é¢˜è¿›è¡Œç›¸åº”å¤„ç†
2. **ç»“æ„å¼‚å¸¸æ–¹é¢**: å¯¹å‘ç°çš„ç»“æ„å¼‚å¸¸è¿›è¡Œæ·±å…¥åˆ†æï¼Œç¡®å®šæ˜¯å¦ä¸ºæ­£å¸¸çš„ä¸šåŠ¡å˜åŒ–æˆ–éœ€è¦å…³æ³¨çš„å¼‚å¸¸æƒ…å†µ
3. **é”€å”®ä»£ç†æ–¹é¢**: æ ¹æ®é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹åˆ†æç»“æœï¼Œè¯„ä¼°å„è½¦å‹çš„é”€å”®æ¸ é“æ•ˆæœ
4. **é‡å¤ä¹°å®¶æ–¹é¢**: æ ¹æ®é‡å¤ä¹°å®¶è®¢å•æ¯”ä¾‹åˆ†æç»“æœï¼Œè¯„ä¼°å®¢æˆ·å¿ è¯šåº¦å’Œå¤è´­è¡Œä¸ºæ¨¡å¼
5. **æ—¶é—´é—´éš”åˆ†ææ–¹é¢**: æ ¹æ®æ—¶é—´é—´éš”åˆ†æç»“æœï¼Œä¼˜åŒ–ä¸šåŠ¡æµç¨‹æ•ˆç‡ï¼Œå…³æ³¨å¼‚å¸¸æ—¶é—´é—´éš”æ¨¡å¼
6. **æŒç»­ç›‘æ§**: å»ºè®®å®šæœŸè¿è¡Œæ­¤å·¥ä½œæµï¼ŒæŒç»­ç›‘æ§æ•°æ®è´¨é‡å’Œç»“æ„å˜åŒ–

---

*æœ¬æŠ¥å‘Šç”± W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµè‡ªåŠ¨ç”Ÿæˆï¼Œæ•´åˆäº†æ•°æ®è´¨é‡æ£€æµ‹ã€ç»“æ„å¼‚å¸¸åˆ†æã€é”€å”®ä»£ç†åˆ†æå’Œæ—¶é—´é—´éš”åˆ†æçš„ç»¼åˆç»“æœ*
"""
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        complete_report_path = "/Users/zihao_/Documents/github/W35_workflow/complete_analysis_report.md"
        with open(complete_report_path, 'w', encoding='utf-8') as f:
            f.write(complete_report)
        
        logger.info(f"ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {complete_report_path}")
        print(f"\nğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {complete_report_path}")
        
        # æ›´æ–°çŠ¶æ€
        state["complete_report_path"] = complete_report_path
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆç»¼åˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        print(f"âŒ {error_msg}")

# æ›´æ–°READMEä¸­çš„mermaidå›¾ç¤ºèŠ‚ç‚¹
def update_readme_mermaid_node(state: WorkflowState) -> WorkflowState:
    """
    æ›´æ–°README.mdä¸­çš„mermaidå·¥ä½œæµå›¾ç¤º
    """
    logger.info("å¼€å§‹æ›´æ–°README.mdä¸­çš„mermaidå›¾ç¤º...")
    
    try:
        mermaid_content = """# W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµ

## å·¥ä½œæµæ¶æ„

```mermaid
flowchart TD
    A[å¼€å§‹] --> B[å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹]
    B --> C[ç»“æ„æ£€æŸ¥èŠ‚ç‚¹]
    C --> G[é”€å”®ä»£ç†åˆ†æèŠ‚ç‚¹]
    G --> H[æ—¶é—´é—´éš”åˆ†æèŠ‚ç‚¹]
    H --> D[ç”Ÿæˆç»¼åˆæŠ¥å‘Š]
    D --> E[æ›´æ–°READMEå›¾ç¤º]
    E --> F[ç»“æŸ]
    
    B --> B1[æ•°æ®è´¨é‡æ£€æŸ¥]
    B --> B2[ç¼ºå¤±å€¼æ£€æµ‹]
    B --> B3[é‡å¤æ•°æ®æ£€æµ‹]
    B --> B4[ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š]
    
    C --> C1[åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æŸ¥]
    C --> C2[æ¸ é“ç»“æ„å¼‚å¸¸æ£€æŸ¥]
    C --> C3[äººç¾¤ç»“æ„å¼‚å¸¸æ£€æŸ¥]
    C --> C4[ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š]
    
    G --> G1[è¯»å–é”€å”®ä»£ç†æ•°æ®]
    G --> G2[åˆ†æå„è½¦å‹è®¢å•æ¯”ä¾‹]
    G --> G3[ç”Ÿæˆé”€å”®ä»£ç†åˆ†ææŠ¥å‘Š]
    
    H --> H1[æ”¯ä»˜åˆ°é€€æ¬¾æ—¶é—´é—´éš”åˆ†æ]
    H --> H2[æ”¯ä»˜åˆ°åˆ†é…æ—¶é—´é—´éš”åˆ†æ]
    H --> H3[è½¦å‹å¯¹æ¯”åˆ†æ]
    H --> H4[ç”Ÿæˆæ—¶é—´é—´éš”åˆ†ææŠ¥å‘Š]
    
    D --> D1[æ•´åˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š]
    D --> D2[æ•´åˆç»“æ„æ£€æŸ¥æŠ¥å‘Š]
    D --> D3[æ•´åˆé”€å”®ä»£ç†åˆ†ææŠ¥å‘Š]
    D --> D4[æ•´åˆæ—¶é—´é—´éš”åˆ†ææŠ¥å‘Š]
    D --> D5[ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style G fill:#e3f2fd
    style H fill:#fce4ec
    style D fill:#fff3e0
    style E fill:#ffebee
    style F fill:#f1f8e9
```

## åŠŸèƒ½è¯´æ˜

### å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹
- æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
- æ•°æ®è¯»å–éªŒè¯
- åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
- ç¼ºå¤±å€¼æ£€æµ‹ä¸åˆ†æ
- é‡å¤æ•°æ®æ£€æµ‹
- æ•°æ®ç±»å‹åˆ†å¸ƒåˆ†æ
- å¼‚å¸¸å€¼è¯†åˆ«

### ç»“æ„æ£€æŸ¥èŠ‚ç‚¹
- åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹ï¼šå¯¹æ¯”CM2è½¦å‹ä¸å†å²è½¦å‹çš„åœ°åŒºè®¢å•åˆ†å¸ƒå˜åŒ–
- æ¸ é“ç»“æ„å¼‚å¸¸æ£€æµ‹ï¼šåˆ†æå„æ¸ é“é”€é‡å æ¯”çš„çªå˜æƒ…å†µ
- äººç¾¤ç»“æ„å¼‚å¸¸æ£€æµ‹ï¼šæ£€æµ‹æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„çš„å¤§å¹…å˜åŒ–
- åŸºäºä¸šåŠ¡å®šä¹‰çš„æ—¶é—´èŒƒå›´è¿›è¡Œæ•°æ®ç­›é€‰

### é”€å”®ä»£ç†åˆ†æèŠ‚ç‚¹
- è¯»å–é”€å”®ä»£ç†æ•°æ®ï¼šä»sales_info_data.jsonè·å–Store Agentä¿¡æ¯
- è®¢å•æ¯”ä¾‹åˆ†æï¼šè®¡ç®—å„è½¦å‹é¢„å”®å‘¨æœŸä¸­æ¥è‡ªStore Agentçš„è®¢å•æ¯”ä¾‹
- å¼‚å¸¸æ£€æµ‹ï¼šè¯†åˆ«é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹çš„å¼‚å¸¸æƒ…å†µ
- åŸºäºä¸šåŠ¡å®šä¹‰çš„é¢„å”®å‘¨æœŸè¿›è¡Œæ•°æ®ç­›é€‰

### æ—¶é—´é—´éš”åˆ†æèŠ‚ç‚¹
- æ”¯ä»˜åˆ°é€€æ¬¾æ—¶é—´é—´éš”åˆ†æï¼šè®¡ç®—å„è½¦å‹ä»æ”¯ä»˜åˆ°é€€æ¬¾çš„æ—¶é—´é—´éš”ç»Ÿè®¡
- æ”¯ä»˜åˆ°åˆ†é…æ—¶é—´é—´éš”åˆ†æï¼šè®¡ç®—å„è½¦å‹ä»æ”¯ä»˜åˆ°åˆ†é…çš„æ—¶é—´é—´éš”ç»Ÿè®¡
- è½¦å‹å¯¹æ¯”åˆ†æï¼šå¯¹æ¯”ä¸åŒè½¦å‹çš„æ—¶é—´æ•ˆç‡è¡¨ç°
- å¼‚å¸¸æ£€æµ‹ï¼šè¯†åˆ«æ—¶é—´é—´éš”çš„å¼‚å¸¸æ¨¡å¼å’Œè¶‹åŠ¿
- åŸºäºä¸šåŠ¡å®šä¹‰çš„é¢„å”®å‘¨æœŸè¿›è¡Œæ•°æ®ç­›é€‰

### ç»¼åˆæŠ¥å‘Šç”Ÿæˆ
- æ•´åˆå¼‚å¸¸æ£€æµ‹ã€ç»“æ„æ£€æŸ¥ã€é”€å”®ä»£ç†åˆ†æå’Œæ—¶é—´é—´éš”åˆ†æç»“æœ
- æä¾›æ•°æ®è´¨é‡ã€ç»“æ„å¼‚å¸¸ã€é”€å”®æ¸ é“å’Œæ—¶é—´æ•ˆç‡çš„ç»¼åˆè¯„ä¼°
- åŸºäºæ£€æµ‹ç»“æœæä¾›ç›¸åº”çš„å¤„ç†å»ºè®®

### æŠ¥å‘Šç”Ÿæˆ
- è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„MDæ ¼å¼å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
- ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
- ç”Ÿæˆé”€å”®ä»£ç†åˆ†ææŠ¥å‘Š
- ç”Ÿæˆæ—¶é—´é—´éš”åˆ†ææŠ¥å‘Š
- ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
- åŒ…å«æ•°æ®è´¨é‡è¯„çº§
- æä¾›å¯è§†åŒ–çš„å¼‚å¸¸ç»Ÿè®¡ä¿¡æ¯

### å·¥ä½œæµç‰¹æ€§
- åŸºäºLangGraphæ¡†æ¶æ„å»º
- æ¨¡å—åŒ–èŠ‚ç‚¹è®¾è®¡
- å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹ï¼ˆæ•°æ®è´¨é‡+ä¸šåŠ¡ç»“æ„ï¼‰
- å®Œæ•´çš„é”™è¯¯å¤„ç†æœºåˆ¶
- è¯¦ç»†çš„æ—¥å¿—è®°å½•

## ä½¿ç”¨æ–¹æ³•

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# è¿è¡Œå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ
python main.py
```

## è¾“å‡ºæ–‡ä»¶
- `anomaly_detection_report.md`: è¯¦ç»†çš„å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
- `structure_check_report.md`: ç»“æ„æ£€æŸ¥è¯¦ç»†æŠ¥å‘Š
- `sales_agent_analysis_report.md`: é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹åˆ†ææŠ¥å‘Š
- `time_interval_analysis_report.md`: æ—¶é—´é—´éš”åˆ†æè¯¦ç»†æŠ¥å‘Š
- `complete_analysis_report.md`: ç»¼åˆåˆ†ææŠ¥å‘Š
- æ§åˆ¶å°æ—¥å¿—: å®æ—¶å¤„ç†çŠ¶æ€ä¿¡æ¯

## é…ç½®æ–‡ä»¶
- `business_definition.json`: ä¸šåŠ¡æ—¶é—´èŒƒå›´å®šä¹‰ï¼Œç”¨äºç»“æ„æ£€æŸ¥çš„æ•°æ®ç­›é€‰
"""
        
        readme_path = "/Users/zihao_/Documents/github/W35_workflow/README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(mermaid_content)
        
        state["status"] = "readme_updated"
        logger.info("README.mdä¸­çš„mermaidå›¾ç¤ºæ›´æ–°å®Œæˆ")
        print(f"\nğŸ“ README.mdå·²æ›´æ–°: {readme_path}")
        
    except Exception as e:
        error_msg = f"æ›´æ–°README.mdè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹
def anomaly_detection_node(state: WorkflowState) -> WorkflowState:
    """
    å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹ - å·¥ä½œæµçš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
    æ£€æŸ¥æ•°æ®é›†çš„å¼‚å¸¸æƒ…å†µï¼ŒåŒ…æ‹¬ï¼š
    1. æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    2. æ•°æ®æ˜¯å¦å¯ä»¥æ­£å¸¸è¯»å–
    3. æ•°æ®åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    4. ç¼ºå¤±å€¼æ£€æŸ¥
    5. æ•°æ®ç±»å‹æ£€æŸ¥
    6. å¼‚å¸¸å€¼æ£€æµ‹
    """
    logger.info("å¼€å§‹æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
    
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        data_path = Path(state["data_path"])
        if not data_path.exists():
            error_msg = f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {state['data_path']}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["status"] = "failed"
            return state
        
        logger.info(f"æ•°æ®æ–‡ä»¶å­˜åœ¨: {state['data_path']}")
        
        # 2. è¯»å–æ•°æ®
        try:
            state["data"] = pd.read_parquet(state["data_path"])
            logger.info(f"æˆåŠŸè¯»å–æ•°æ®ï¼Œæ•°æ®å½¢çŠ¶: {state['data'].shape}")
        except Exception as e:
            error_msg = f"è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["status"] = "failed"
            return state
        
        # 3. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        state["integrity_check_results"]["shape"] = state["data"].shape
        state["integrity_check_results"]["columns"] = list(state["data"].columns)
        state["integrity_check_results"]["dtypes"] = state["data"].dtypes.to_dict()
        
        # 4. ç¼ºå¤±å€¼æ£€æŸ¥
        missing_values = state["data"].isnull().sum()
        state["integrity_check_results"]["missing_values"] = missing_values.to_dict()
        state["integrity_check_results"]["missing_percentage"] = (missing_values / len(state["data"]) * 100).to_dict()
        
        # 5. æ•°æ®ç±»å‹åˆ†å¸ƒ
        numeric_columns = state["data"].select_dtypes(include=['number']).columns.tolist()
        categorical_columns = state["data"].select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_columns = state["data"].select_dtypes(include=['datetime']).columns.tolist()
        
        state["integrity_check_results"]["numeric_columns"] = numeric_columns
        state["integrity_check_results"]["categorical_columns"] = categorical_columns
        state["integrity_check_results"]["datetime_columns"] = datetime_columns
        
        # 6. æ•°æ®è´¨é‡è¯„ä¼°
        total_cells = state["data"].shape[0] * state["data"].shape[1]
        missing_cells = state["data"].isnull().sum().sum()
        data_completeness = (total_cells - missing_cells) / total_cells * 100
        
        state["integrity_check_results"]["data_completeness_percentage"] = data_completeness
        
        # 7. åŸºæœ¬æè¿°æ€§ç»Ÿè®¡
        if numeric_columns:
            state["integrity_check_results"]["numeric_summary"] = state["data"][numeric_columns].describe().to_dict()
        
        # 8. é‡å¤è¡Œæ£€æŸ¥
        duplicate_rows = state["data"].duplicated().sum()
        state["integrity_check_results"]["duplicate_rows"] = duplicate_rows
        
        # è®°å½•å…ƒæ•°æ®
        state["metadata"]["file_size_mb"] = data_path.stat().st_size / (1024 * 1024)
        state["metadata"]["processing_timestamp"] = pd.Timestamp.now().isoformat()
        
        state["status"] = "anomaly_detection_completed"
        logger.info("å¼‚å¸¸æ£€æµ‹å®Œæˆ")
        
        # ç”ŸæˆMDæŠ¥å‘Š
        generate_anomaly_report(state, data_completeness, duplicate_rows, numeric_columns, categorical_columns, datetime_columns, missing_values, missing_cells)
        
    except Exception as e:
        error_msg = f"å¼‚å¸¸æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# ç»“æ„æ£€æŸ¥èŠ‚ç‚¹
def structure_check_node(state: WorkflowState) -> WorkflowState:
    """
    ç»“æ„æ£€æŸ¥èŠ‚ç‚¹ï¼šåˆ†æCM2è½¦å‹ç›¸å¯¹äºå†å²è½¦å‹çš„ç»“æ„å¼‚å¸¸
    """
    logger.info("å¼€å§‹æ‰§è¡Œç»“æ„æ£€æŸ¥...")
    
    try:
        # è¯»å–ä¸šåŠ¡å®šä¹‰æ–‡ä»¶
        business_def_path = "/Users/zihao_/Documents/github/W35_workflow/business_definition.json"
        with open(business_def_path, 'r', encoding='utf-8') as f:
            import json
            presale_periods = json.load(f)
        
        # è¯»å–æ•°æ®
        data_path = "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet"
        df = pd.read_parquet(data_path)
        
        # è½¬æ¢æ—¶é—´åˆ—
        df['Intention_Payment_Time'] = pd.to_datetime(df['Intention_Payment_Time'])
        
        # ç­›é€‰å„è½¦å‹æ•°æ®
        vehicle_data = {}
        for vehicle, period in presale_periods.items():
            start_date = pd.to_datetime(period['start'])
            end_date = pd.to_datetime(period['end'])
            mask = (df['Intention_Payment_Time'] >= start_date) & (df['Intention_Payment_Time'] <= end_date)
            vehicle_data[vehicle] = df[mask].copy()
        
        # åˆ†æç»“æœ
        anomalies = []
        
        # 1. åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æŸ¥
        region_anomalies = analyze_region_distribution(vehicle_data)
        if region_anomalies:
            anomalies.extend(region_anomalies)
        
        # 2. æ¸ é“ç»“æ„å¼‚å¸¸æ£€æŸ¥
        channel_anomalies = analyze_channel_structure(vehicle_data)
        if channel_anomalies:
            anomalies.extend(channel_anomalies)
        
        # 3. äººç¾¤ç»“æ„å¼‚å¸¸æ£€æŸ¥
        demographic_anomalies = analyze_demographic_structure(vehicle_data)
        if demographic_anomalies:
            anomalies.extend(demographic_anomalies)
        
        # 4. åŒæ¯”/ç¯æ¯”å¼‚å¸¸æ£€æŸ¥
        time_series_anomalies = analyze_time_series_anomalies(vehicle_data, presale_periods)
        if time_series_anomalies:
            anomalies.extend(time_series_anomalies)
        
        # å°†presale_periodsæ·»åŠ åˆ°stateä¸­
        state['presale_periods'] = presale_periods
        
        # ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
        try:
            generate_structure_report(state, vehicle_data, anomalies)
        except Exception as report_error:
    
            import traceback
            traceback.print_exc()
            raise report_error
        
        state["status"] = "structure_check_completed"
        logger.info("ç»“æ„æ£€æŸ¥å®Œæˆ")
        
    except Exception as e:
        error_msg = f"ç»“æ„æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

def sales_agent_analysis_node(state: WorkflowState) -> WorkflowState:
    """
    é”€å”®ä»£ç†åˆ†æèŠ‚ç‚¹ï¼šåˆ†æä¸åŒè½¦å‹åœ¨é¢„å”®å‘¨æœŸä¸­æ¥è‡ªStore Agentçš„è®¢å•æ¯”ä¾‹
    """
    logger.info("å¼€å§‹æ‰§è¡Œé”€å”®ä»£ç†åˆ†æ...")
    
    try:
        # è¯»å–ä¸šåŠ¡å®šä¹‰æ–‡ä»¶
        business_def_path = "/Users/zihao_/Documents/github/W35_workflow/business_definition.json"
        with open(business_def_path, 'r', encoding='utf-8') as f:
            import json
            presale_periods = json.load(f)
        
        # è¯»å–é”€å”®ä»£ç†æ•°æ®
        sales_info_path = "/Users/zihao_/Documents/coding/dataset/formatted/sales_info_data.json"
        try:
            with open(sales_info_path, 'r', encoding='utf-8') as f:
                sales_info_json = json.load(f)
                # æå–dataéƒ¨åˆ†
                sales_info_data = sales_info_json.get('data', [])
                logger.info(f"æˆåŠŸè¯»å–é”€å”®ä»£ç†æ•°æ®ï¼Œå…± {len(sales_info_data)} æ¡è®°å½•")
        except Exception as e:
            logger.warning(f"æ— æ³•è¯»å–é”€å”®ä»£ç†æ•°æ®æ–‡ä»¶: {str(e)}")
            sales_info_data = []
        
        # è¯»å–è®¢å•æ•°æ®
        data_path = "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet"
        df = pd.read_parquet(data_path)
        
        # è½¬æ¢æ—¶é—´åˆ—
        df['Intention_Payment_Time'] = pd.to_datetime(df['Intention_Payment_Time'])
        
        # é¢„å¤„ç†è®¢å•æ•°æ®ä¸­çš„ç›¸å…³å­—æ®µï¼ˆä¸€æ¬¡æ€§å¤„ç†ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤å¤„ç†ï¼‰
        df['clean_store_agent_name'] = df['Store Agent Name'].fillna('').astype(str).str.strip()
        df['clean_store_agent_id'] = df['Store Agent Id'].fillna('').astype(str).str.strip()
        df['clean_buyer_identity'] = df['Buyer Identity No'].fillna('').astype(str).str.strip()
        
        # ä»é”€å”®ä»£ç†æ•°æ®ä¸­æå–é”€å”®ä»£ç†ä¿¡æ¯å¹¶æ„å»ºå¿«é€ŸæŸ¥æ‰¾é›†åˆ
        # å­—æ®µæ˜ å°„ï¼šMember Name -> Store Agent Name, Member Code -> Store Agent Id, Id Card -> Buyer Identity No
        sales_agents_lookup = set()  # ä½¿ç”¨é›†åˆè¿›è¡Œå¿«é€ŸæŸ¥æ‰¾
        if isinstance(sales_info_data, list):
            for item in sales_info_data:
                if isinstance(item, dict):
                    # æå–ä¸‰ä¸ªå…³é”®å­—æ®µ
                    member_name = str(item.get('Member Name', '')).strip() if item.get('Member Name') else ''
                    member_code = str(item.get('Member Code', '')).strip() if item.get('Member Code') else ''
                    id_card = str(item.get('Id Card', '')).strip() if item.get('Id Card') else ''
                    
                    # åªæœ‰å½“ä¸‰ä¸ªå­—æ®µéƒ½éç©ºæ—¶æ‰æ·»åŠ åˆ°æŸ¥æ‰¾é›†åˆ
                    if member_name and member_code and id_card:
                        sales_agents_lookup.add((member_name, member_code, id_card))
        
        # åˆ†æå„è½¦å‹çš„é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹
        analysis_results = {}
        
        for vehicle, period in presale_periods.items():
            start_date = pd.to_datetime(period['start'])
            end_date = pd.to_datetime(period['end'])
            mask = (df['Intention_Payment_Time'] >= start_date) & (df['Intention_Payment_Time'] <= end_date)
            vehicle_df = df[mask].copy()
            
            if len(vehicle_df) == 0:
                analysis_results[vehicle] = {
                    'total_orders': 0,
                    'store_agent_orders': 0,
                    'store_agent_ratio': 0.0,
                    'details': 'æ— è®¢å•æ•°æ®'
                }
                continue
            
            total_orders = len(vehicle_df)
            
            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ£€æŸ¥è®¢å•æ˜¯å¦æ¥è‡ªé”€å”®ä»£ç†
            # åŒ¹é…æ¡ä»¶ï¼šStore Agent Name = Member Name AND Store Agent Id = Member Code AND Buyer Identity No = Id Card
            
            # åˆ›å»ºç»„åˆå­—æ®µç”¨äºå¿«é€ŸåŒ¹é…ï¼ˆä½¿ç”¨å·²é¢„å¤„ç†çš„å­—æ®µï¼‰
            agent_combos = list(zip(
                vehicle_df['clean_store_agent_name'],
                vehicle_df['clean_store_agent_id'], 
                vehicle_df['clean_buyer_identity']
            ))
            
            # ä½¿ç”¨é›†åˆäº¤é›†å¿«é€Ÿæ‰¾åˆ°åŒ¹é…çš„è®¢å•
            matched_combos = set(agent_combos) & sales_agents_lookup
            
            # è®¡ç®—é”€å”®ä»£ç†è®¢å•æ•°
            store_agent_orders = sum(1 for combo in agent_combos if combo in matched_combos)
            
            store_agent_ratio = store_agent_orders / total_orders if total_orders > 0 else 0.0
            
            analysis_results[vehicle] = {
                'total_orders': total_orders,
                'store_agent_orders': store_agent_orders,
                'store_agent_ratio': store_agent_ratio,
                'details': f'æ€»è®¢å•æ•°: {total_orders}, é”€å”®ä»£ç†è®¢å•æ•°: {store_agent_orders}, æ¯”ä¾‹: {store_agent_ratio:.2%}'
            }
        
        # åˆ†æé‡å¤ä¹°å®¶èº«ä»½è¯å·ç çš„è®¢å•æ¯”ä¾‹
        repeat_buyer_analysis = {}
        
        for vehicle, period in presale_periods.items():
            start_date = pd.to_datetime(period['start'])
            end_date = pd.to_datetime(period['end'])
            mask = (df['Intention_Payment_Time'] >= start_date) & (df['Intention_Payment_Time'] <= end_date)
            vehicle_df = df[mask].copy()
            
            if len(vehicle_df) == 0:
                repeat_buyer_analysis[vehicle] = {
                    'total_orders': 0,
                    'repeat_buyer_orders': 0,
                    'repeat_buyer_ratio': 0.0,
                    'unique_repeat_buyers': 0,
                    'details': 'æ— è®¢å•æ•°æ®'
                }
                continue
            
            total_orders = len(vehicle_df)
            
            # ç»Ÿè®¡æ¯ä¸ªBuyer Identity Noçš„è®¢å•æ•°é‡
            buyer_identity_counts = vehicle_df['Buyer Identity No'].value_counts()
            
            # æ‰¾å‡ºè®¢å•æ•°>=2çš„ä¹°å®¶èº«ä»½è¯å·ç 
            repeat_buyers = buyer_identity_counts[buyer_identity_counts >= 2]
            
            # è®¡ç®—è¿™äº›é‡å¤ä¹°å®¶çš„æ€»è®¢å•æ•°
            repeat_buyer_orders = repeat_buyers.sum()
            
            # è®¡ç®—æ¯”ä¾‹
            repeat_buyer_ratio = repeat_buyer_orders / total_orders if total_orders > 0 else 0.0
            
            repeat_buyer_analysis[vehicle] = {
                'total_orders': total_orders,
                'repeat_buyer_orders': repeat_buyer_orders,
                'repeat_buyer_ratio': repeat_buyer_ratio,
                'unique_repeat_buyers': len(repeat_buyers),
                'details': f'æ€»è®¢å•æ•°: {total_orders}, é‡å¤ä¹°å®¶è®¢å•æ•°: {repeat_buyer_orders}, æ¯”ä¾‹: {repeat_buyer_ratio:.2%}, é‡å¤ä¹°å®¶æ•°é‡: {len(repeat_buyers)}'
            }
        
        # å°†åˆ†æç»“æœä¿å­˜åˆ°stateä¸­
        state['sales_agent_analysis'] = analysis_results
        state['repeat_buyer_analysis'] = repeat_buyer_analysis
        
        # ç”Ÿæˆé”€å”®ä»£ç†åˆ†ææŠ¥å‘Š
        generate_sales_agent_report(state, analysis_results, len(sales_agents_lookup), repeat_buyer_analysis)
        
        state["status"] = "sales_agent_analysis_completed"
        logger.info("é”€å”®ä»£ç†åˆ†æå®Œæˆ")
        
    except Exception as e:
        error_msg = f"é”€å”®ä»£ç†åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

def analyze_time_intervals(state: WorkflowState) -> WorkflowState:
    """
    åˆ†æä¸åŒè½¦å‹åœ¨é¢„å”®å‘¨æœŸå†…è®¢å•çš„æ—¶é—´é—´éš”å·®å¼‚
    è®¡ç®—Intention_Payment_Timeä¸intention_refund_timeã€first_assign_timeä¹‹é—´çš„æ—¶é—´é—´éš”ï¼ˆå¤©ï¼‰
    """
    try:
        logger.info("å¼€å§‹æ—¶é—´é—´éš”åˆ†æ...")
        
        df = state["data"]
        
        # è¯»å–é¢„å”®å‘¨æœŸå®šä¹‰
        with open('/Users/zihao_/Documents/github/W35_workflow/business_definition.json', 'r', encoding='utf-8') as f:
            presale_periods = json.load(f)
        
        # ç¡®ä¿æ—¶é—´åˆ—ä¸ºdatetimeç±»å‹
        time_columns = ['Intention_Payment_Time', 'intention_refund_time', 'first_assign_time']
        for col in time_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        time_interval_analysis = {}
        
        for vehicle, period in presale_periods.items():
            start_date = pd.to_datetime(period['start'])
            end_date = pd.to_datetime(period['end'])
            
            # ç­›é€‰é¢„å”®å‘¨æœŸå†…çš„æ•°æ®
            mask = (df['Intention_Payment_Time'] >= start_date) & (df['Intention_Payment_Time'] <= end_date)
            vehicle_df = df[mask].copy()
            
            if len(vehicle_df) == 0:
                time_interval_analysis[vehicle] = {
                    'total_orders': 0,
                    'payment_to_refund_stats': {},
                    'payment_to_assign_stats': {},
                    'details': 'æ— è®¢å•æ•°æ®'
                }
                continue
            
            total_orders = len(vehicle_df)
            
            # è®¡ç®—Intention_Payment_Timeåˆ°intention_refund_timeçš„æ—¶é—´é—´éš”
            payment_to_refund_intervals = []
            if 'intention_refund_time' in vehicle_df.columns:
                valid_refund_mask = vehicle_df['intention_refund_time'].notna() & vehicle_df['Intention_Payment_Time'].notna()
                if valid_refund_mask.any():
                    intervals = (vehicle_df.loc[valid_refund_mask, 'intention_refund_time'] - 
                               vehicle_df.loc[valid_refund_mask, 'Intention_Payment_Time']).dt.days
                    payment_to_refund_intervals = intervals.tolist()
            
            # è®¡ç®—Intention_Payment_Timeåˆ°first_assign_timeçš„æ—¶é—´é—´éš”
            payment_to_assign_intervals = []
            if 'first_assign_time' in vehicle_df.columns:
                valid_assign_mask = vehicle_df['first_assign_time'].notna() & vehicle_df['Intention_Payment_Time'].notna()
                if valid_assign_mask.any():
                    intervals = (vehicle_df.loc[valid_assign_mask, 'first_assign_time'] - 
                               vehicle_df.loc[valid_assign_mask, 'Intention_Payment_Time']).dt.days
                    payment_to_assign_intervals = intervals.tolist()
            
            # ç»Ÿè®¡åˆ†æ
            payment_to_refund_stats = {}
            if payment_to_refund_intervals:
                payment_to_refund_stats = {
                    'count': len(payment_to_refund_intervals),
                    'mean': np.mean(payment_to_refund_intervals),
                    'median': np.median(payment_to_refund_intervals),
                    'std': np.std(payment_to_refund_intervals),
                    'min': np.min(payment_to_refund_intervals),
                    'max': np.max(payment_to_refund_intervals),
                    'q25': np.percentile(payment_to_refund_intervals, 25),
                    'q75': np.percentile(payment_to_refund_intervals, 75)
                }
            
            payment_to_assign_stats = {}
            if payment_to_assign_intervals:
                payment_to_assign_stats = {
                    'count': len(payment_to_assign_intervals),
                    'mean': np.mean(payment_to_assign_intervals),
                    'median': np.median(payment_to_assign_intervals),
                    'std': np.std(payment_to_assign_intervals),
                    'min': np.min(payment_to_assign_intervals),
                    'max': np.max(payment_to_assign_intervals),
                    'q25': np.percentile(payment_to_assign_intervals, 25),
                    'q75': np.percentile(payment_to_assign_intervals, 75)
                }
            
            time_interval_analysis[vehicle] = {
                'total_orders': total_orders,
                'payment_to_refund_stats': payment_to_refund_stats,
                'payment_to_assign_stats': payment_to_assign_stats,
                'details': f'æ€»è®¢å•æ•°: {total_orders}, é€€æ¬¾é—´éš”æ ·æœ¬æ•°: {len(payment_to_refund_intervals)}, åˆ†é…é—´éš”æ ·æœ¬æ•°: {len(payment_to_assign_intervals)}'
            }
        
        # ä¿å­˜åˆ†æç»“æœåˆ°state
        state['time_interval_analysis'] = time_interval_analysis
        
        # ç”Ÿæˆæ—¶é—´é—´éš”åˆ†ææŠ¥å‘Š
        generate_time_interval_report(state, time_interval_analysis)
        
        state["status"] = "time_interval_analysis_completed"
        logger.info("æ—¶é—´é—´éš”åˆ†æå®Œæˆ")
        
    except Exception as e:
        error_msg = f"æ—¶é—´é—´éš”åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

def generate_time_interval_report(state: WorkflowState, time_interval_analysis: dict):
    """
    ç”Ÿæˆæ—¶é—´é—´éš”åˆ†ææŠ¥å‘Š
    """
    report_content = []
    report_content.append("# è½¦å‹æ—¶é—´é—´éš”åˆ†ææŠ¥å‘Š\n")
    report_content.append("## åˆ†ææ¦‚è¿°\n")
    report_content.append("æœ¬æŠ¥å‘Šåˆ†æäº†ä¸åŒè½¦å‹åœ¨é¢„å”®å‘¨æœŸå†…è®¢å•çš„æ—¶é—´é—´éš”å·®å¼‚ï¼ŒåŒ…æ‹¬ï¼š\n")
    report_content.append("- Intention_Payment_Time åˆ° intention_refund_time çš„æ—¶é—´é—´éš”ï¼ˆå¤©ï¼‰\n")
    report_content.append("- Intention_Payment_Time åˆ° first_assign_time çš„æ—¶é—´é—´éš”ï¼ˆå¤©ï¼‰\n\n")
    
    # æ”¯ä»˜åˆ°é€€æ¬¾æ—¶é—´é—´éš”ç»Ÿè®¡è¡¨
    report_content.append("## æ”¯ä»˜åˆ°é€€æ¬¾æ—¶é—´é—´éš”ç»Ÿè®¡\n")
    report_content.append("| è½¦å‹ | æ ·æœ¬æ•° | å¹³å‡å€¼(å¤©) | ä¸­ä½æ•°(å¤©) | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ | 25%åˆ†ä½ | 75%åˆ†ä½ |\n")
    report_content.append("|------|--------|------------|------------|--------|--------|--------|---------|---------|\n")
    
    for vehicle, result in time_interval_analysis.items():
        refund_stats = result['payment_to_refund_stats']
        if refund_stats:
            report_content.append(f"| {vehicle} | {refund_stats['count']:,} | {refund_stats['mean']:.1f} | {refund_stats['median']:.1f} | {refund_stats['std']:.1f} | {refund_stats['min']:.0f} | {refund_stats['max']:.0f} | {refund_stats['q25']:.1f} | {refund_stats['q75']:.1f} |\n")
        else:
            report_content.append(f"| {vehicle} | 0 | - | - | - | - | - | - | - |\n")
    
    # æ”¯ä»˜åˆ°åˆ†é…æ—¶é—´é—´éš”ç»Ÿè®¡è¡¨
    report_content.append("\n## æ”¯ä»˜åˆ°åˆ†é…æ—¶é—´é—´éš”ç»Ÿè®¡\n")
    report_content.append("| è½¦å‹ | æ ·æœ¬æ•° | å¹³å‡å€¼(å¤©) | ä¸­ä½æ•°(å¤©) | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ | 25%åˆ†ä½ | 75%åˆ†ä½ |\n")
    report_content.append("|------|--------|------------|------------|--------|--------|--------|---------|---------|\n")
    
    for vehicle, result in time_interval_analysis.items():
        assign_stats = result['payment_to_assign_stats']
        if assign_stats:
            report_content.append(f"| {vehicle} | {assign_stats['count']:,} | {assign_stats['mean']:.1f} | {assign_stats['median']:.1f} | {assign_stats['std']:.1f} | {assign_stats['min']:.0f} | {assign_stats['max']:.0f} | {assign_stats['q25']:.1f} | {assign_stats['q75']:.1f} |\n")
        else:
            report_content.append(f"| {vehicle} | 0 | - | - | - | - | - | - | - |\n")
    
    # è¯¦ç»†åˆ†æ
    report_content.append("\n## è¯¦ç»†åˆ†æ\n")
    for vehicle, result in time_interval_analysis.items():
        report_content.append(f"### {vehicle}è½¦å‹\n")
        report_content.append(f"{result['details']}\n\n")
        
        refund_stats = result['payment_to_refund_stats']
        assign_stats = result['payment_to_assign_stats']
        
        if refund_stats:
            report_content.append(f"**æ”¯ä»˜åˆ°é€€æ¬¾æ—¶é—´é—´éš”ï¼š**\n")
            report_content.append(f"- å¹³å‡é—´éš”ï¼š{refund_stats['mean']:.1f}å¤©\n")
            report_content.append(f"- ä¸­ä½æ•°é—´éš”ï¼š{refund_stats['median']:.1f}å¤©\n")
            report_content.append(f"- é—´éš”èŒƒå›´ï¼š{refund_stats['min']:.0f}-{refund_stats['max']:.0f}å¤©\n\n")
        
        if assign_stats:
            report_content.append(f"**æ”¯ä»˜åˆ°åˆ†é…æ—¶é—´é—´éš”ï¼š**\n")
            report_content.append(f"- å¹³å‡é—´éš”ï¼š{assign_stats['mean']:.1f}å¤©\n")
            report_content.append(f"- ä¸­ä½æ•°é—´éš”ï¼š{assign_stats['median']:.1f}å¤©\n")
            report_content.append(f"- é—´éš”èŒƒå›´ï¼š{assign_stats['min']:.0f}-{assign_stats['max']:.0f}å¤©\n\n")
    
    # è½¦å‹å¯¹æ¯”åˆ†æ
    report_content.append("\n## è½¦å‹å¯¹æ¯”åˆ†æ\n")
    
    # æ”¶é›†æœ‰æ•ˆæ•°æ®çš„è½¦å‹
    refund_means = {}
    assign_means = {}
    
    for vehicle, result in time_interval_analysis.items():
        if result['payment_to_refund_stats']:
            refund_means[vehicle] = result['payment_to_refund_stats']['mean']
        if result['payment_to_assign_stats']:
            assign_means[vehicle] = result['payment_to_assign_stats']['mean']
    
    if refund_means:
        sorted_refund = sorted(refund_means.items(), key=lambda x: x[1])
        report_content.append("### æ”¯ä»˜åˆ°é€€æ¬¾æ—¶é—´é—´éš”æ’åºï¼ˆä»çŸ­åˆ°é•¿ï¼‰\n")
        for i, (vehicle, mean_days) in enumerate(sorted_refund, 1):
            report_content.append(f"{i}. {vehicle}: {mean_days:.1f}å¤©\n")
        report_content.append("\n")
    
    if assign_means:
        sorted_assign = sorted(assign_means.items(), key=lambda x: x[1])
        report_content.append("### æ”¯ä»˜åˆ°åˆ†é…æ—¶é—´é—´éš”æ’åºï¼ˆä»çŸ­åˆ°é•¿ï¼‰\n")
        for i, (vehicle, mean_days) in enumerate(sorted_assign, 1):
            report_content.append(f"{i}. {vehicle}: {mean_days:.1f}å¤©\n")
        report_content.append("\n")
    
    # å¼‚å¸¸æ£€æµ‹
    if len(refund_means) > 1:
        refund_values = list(refund_means.values())
        refund_mean = np.mean(refund_values)
        refund_std = np.std(refund_values)
        
        report_content.append("### æ”¯ä»˜åˆ°é€€æ¬¾æ—¶é—´é—´éš”å¼‚å¸¸æ£€æµ‹\n")
        report_content.append(f"å¹³å‡æ—¶é—´é—´éš”ï¼š{refund_mean:.1f}å¤©ï¼Œæ ‡å‡†å·®ï¼š{refund_std:.1f}å¤©\n\n")
        
        for vehicle, mean_days in refund_means.items():
            z_score = abs(mean_days - refund_mean) / refund_std if refund_std > 0 else 0
            if z_score > 2:  # è¶…è¿‡2ä¸ªæ ‡å‡†å·®è®¤ä¸ºå¼‚å¸¸
                report_content.append(f"- **{vehicle}å¼‚å¸¸**ï¼š{mean_days:.1f}å¤©ï¼ˆåç¦»å¹³å‡å€¼{abs(mean_days - refund_mean):.1f}å¤©ï¼‰\n")
        report_content.append("\n")
    
    if len(assign_means) > 1:
        assign_values = list(assign_means.values())
        assign_mean = np.mean(assign_values)
        assign_std = np.std(assign_values)
        
        report_content.append("### æ”¯ä»˜åˆ°åˆ†é…æ—¶é—´é—´éš”å¼‚å¸¸æ£€æµ‹\n")
        report_content.append(f"å¹³å‡æ—¶é—´é—´éš”ï¼š{assign_mean:.1f}å¤©ï¼Œæ ‡å‡†å·®ï¼š{assign_std:.1f}å¤©\n\n")
        
        for vehicle, mean_days in assign_means.items():
            z_score = abs(mean_days - assign_mean) / assign_std if assign_std > 0 else 0
            if z_score > 2:  # è¶…è¿‡2ä¸ªæ ‡å‡†å·®è®¤ä¸ºå¼‚å¸¸
                report_content.append(f"- **{vehicle}å¼‚å¸¸**ï¼š{mean_days:.1f}å¤©ï¼ˆåç¦»å¹³å‡å€¼{abs(mean_days - assign_mean):.1f}å¤©ï¼‰\n")
    
    # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
    report_path = "/Users/zihao_/Documents/github/W35_workflow/time_interval_analysis_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.writelines(report_content)
    
    logger.info(f"æ—¶é—´é—´éš”åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

def generate_sales_agent_report(state: WorkflowState, analysis_results: dict, total_agents: int, repeat_buyer_analysis: dict):
    """
    ç”Ÿæˆé”€å”®ä»£ç†åˆ†ææŠ¥å‘Š
    """
    report_content = []
    report_content.append("# é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹åˆ†ææŠ¥å‘Š\n")
    report_content.append("## åˆ†ææ¦‚è¿°\n")
    report_content.append("æœ¬æŠ¥å‘Šåˆ†æäº†ä¸åŒè½¦å‹åœ¨é¢„å”®å‘¨æœŸä¸­æ¥è‡ªStore Agentçš„è®¢å•æ¯”ä¾‹ã€‚\n\n")
    report_content.append("**åŒ¹é…æ¡ä»¶è¯´æ˜ï¼š**\n")
    report_content.append("- Store Agent Name = Member Name\n")
    report_content.append("- Store Agent Id = Member Code\n")
    report_content.append("- Buyer Identity No = Id Card\n")
    report_content.append("- ä¸‰ä¸ªå­—æ®µå¿…é¡»åŒæ—¶åŒ¹é…æ‰è®¤å®šä¸ºé”€å”®ä»£ç†è®¢å•\n")
    
    report_content.append("## å„è½¦å‹é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹\n")
    report_content.append("| è½¦å‹ | æ€»è®¢å•æ•° | é”€å”®ä»£ç†è®¢å•æ•° | é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹ |\n")
    report_content.append("|------|----------|----------------|------------------|\n")
    
    for vehicle, result in analysis_results.items():
        total = result['total_orders']
        agent_orders = result['store_agent_orders']
        ratio = result['store_agent_ratio']
        report_content.append(f"| {vehicle} | {total:,} | {agent_orders:,} | {ratio:.2%} |\n")
    
    report_content.append("\n## è¯¦ç»†åˆ†æ\n")
    for vehicle, result in analysis_results.items():
        report_content.append(f"### {vehicle}è½¦å‹\n")
        report_content.append(f"{result['details']}\n\n")
    
    # æ·»åŠ é‡å¤ä¹°å®¶åˆ†æéƒ¨åˆ†
    report_content.append("\n---\n\n")
    report_content.append("# é‡å¤ä¹°å®¶è®¢å•åˆ†ææŠ¥å‘Š\n")
    report_content.append("## åˆ†ææ¦‚è¿°\n")
    report_content.append("æœ¬éƒ¨åˆ†åˆ†æäº†ä¸åŒè½¦å‹é¢„å”®å‘¨æœŸä¸­é‡å¤ä¹°å®¶ï¼ˆBuyer Identity Noå¯¹åº”>=2ä¸ªè®¢å•ï¼‰çš„è®¢å•æ¯”ä¾‹æƒ…å†µã€‚\n")
    
    report_content.append("## å„è½¦å‹é‡å¤ä¹°å®¶è®¢å•æ¯”ä¾‹\n")
    report_content.append("| è½¦å‹ | æ€»è®¢å•æ•° | é‡å¤ä¹°å®¶è®¢å•æ•° | é‡å¤ä¹°å®¶è®¢å•æ¯”ä¾‹ | é‡å¤ä¹°å®¶æ•°é‡ |\n")
    report_content.append("|------|----------|----------------|------------------|--------------|\n")
    
    for vehicle, result in repeat_buyer_analysis.items():
        total = result['total_orders']
        repeat_orders = result['repeat_buyer_orders']
        ratio = result['repeat_buyer_ratio']
        unique_buyers = result['unique_repeat_buyers']
        report_content.append(f"| {vehicle} | {total:,} | {repeat_orders:,} | {ratio:.2%} | {unique_buyers:,} |\n")
    
    report_content.append("\n## é‡å¤ä¹°å®¶è¯¦ç»†åˆ†æ\n")
    for vehicle, result in repeat_buyer_analysis.items():
        report_content.append(f"### {vehicle}è½¦å‹\n")
        report_content.append(f"{result['details']}\n\n")
    
    # è®¡ç®—å¹³å‡æ¯”ä¾‹å’Œå¼‚å¸¸æ£€æµ‹
    ratios = [result['store_agent_ratio'] for result in analysis_results.values() if result['total_orders'] > 0]
    repeat_ratios = [result['repeat_buyer_ratio'] for result in repeat_buyer_analysis.values() if result['total_orders'] > 0]
    
    if ratios:
        avg_ratio = sum(ratios) / len(ratios)
        avg_repeat_ratio = sum(repeat_ratios) / len(repeat_ratios) if repeat_ratios else 0.0
        
        report_content.append(f"\n## æ€»ä½“ç»Ÿè®¡\n")
        report_content.append(f"- é”€å”®ä»£ç†æ€»æ•°: {total_agents:,}\n")
        report_content.append(f"- å¹³å‡é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹: {avg_ratio:.2%}\n")
        report_content.append(f"- å¹³å‡é‡å¤ä¹°å®¶è®¢å•æ¯”ä¾‹: {avg_repeat_ratio:.2%}\n")
        
        # å¼‚å¸¸æ£€æµ‹ï¼ˆåç¦»å¹³å‡å€¼è¶…è¿‡20%ï¼‰
        anomalies = []
        for vehicle, result in analysis_results.items():
            if result['total_orders'] > 0:
                deviation = abs(result['store_agent_ratio'] - avg_ratio)
                if deviation > 0.2:  # åç¦»è¶…è¿‡20%
                    anomalies.append(f"{vehicle}: {result['store_agent_ratio']:.2%} (åç¦»å¹³å‡å€¼ {deviation:.2%})")
        
        repeat_anomalies = []
        for vehicle, result in repeat_buyer_analysis.items():
            if result['total_orders'] > 0:
                deviation = abs(result['repeat_buyer_ratio'] - avg_repeat_ratio)
                if deviation > 0.2:  # åç¦»è¶…è¿‡20%
                    repeat_anomalies.append(f"{vehicle}: {result['repeat_buyer_ratio']:.2%} (åç¦»å¹³å‡å€¼ {deviation:.2%})")
        
        if anomalies:
            report_content.append("\n## é”€å”®ä»£ç†å¼‚å¸¸æ£€æµ‹\n")
            report_content.append("ä»¥ä¸‹è½¦å‹çš„é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹å­˜åœ¨å¼‚å¸¸ï¼ˆåç¦»å¹³å‡å€¼è¶…è¿‡20%ï¼‰:\n")
            for anomaly in anomalies:
                report_content.append(f"- {anomaly}\n")
        
        if repeat_anomalies:
            report_content.append("\n## é‡å¤ä¹°å®¶å¼‚å¸¸æ£€æµ‹\n")
            report_content.append("ä»¥ä¸‹è½¦å‹çš„é‡å¤ä¹°å®¶è®¢å•æ¯”ä¾‹å­˜åœ¨å¼‚å¸¸ï¼ˆåç¦»å¹³å‡å€¼è¶…è¿‡20%ï¼‰:\n")
            for anomaly in repeat_anomalies:
                report_content.append(f"- {anomaly}\n")
    
    # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
    report_path = "/Users/zihao_/Documents/github/W35_workflow/sales_agent_analysis_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.writelines(report_content)
    
    logger.info(f"é”€å”®ä»£ç†åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

# åœ°åŒºåˆ†å¸ƒå¼‚å¸¸åˆ†æ
def analyze_region_distribution(vehicle_data):
    """
    åˆ†æåœ°åŒºåˆ†å¸ƒå¼‚å¸¸
    """
    anomalies = []
    
    # è·å–CM2æ•°æ®
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåœ°åŒºåˆ†å¸ƒåˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    for region_col in ['Parent Region Name', 'License Province', 'license_city_level', 'License City']:
        if region_col not in cm2_data.columns:
            continue
            
        # CM2åœ°åŒºåˆ†å¸ƒ
        cm2_region_dist = cm2_data[region_col].value_counts(normalize=True)
        
        # å†å²è½¦å‹å¹³å‡åˆ†å¸ƒ
        historical_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                hist_dist = vehicle_data[vehicle][region_col].value_counts(normalize=True)
                historical_dists.append(hist_dist)
        
        if historical_dists:
            # è®¡ç®—å†å²å¹³å‡åˆ†å¸ƒ
            all_regions = set()
            for dist in historical_dists:
                all_regions.update(dist.index)
            all_regions.update(cm2_region_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_regions))
            for dist in historical_dists:
                for region in all_regions:
                    avg_historical[region] += dist.get(region, 0) / len(historical_dists)
            
            # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡20%ï¼‰
            for region in all_regions:
                cm2_ratio = cm2_region_dist.get(region, 0)
                hist_ratio = avg_historical.get(region, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.2:  # 20%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºè®¢å•å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
                elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„åœ°åŒºï¼Œå æ¯”è¶…è¿‡1%
                    anomalies.append(f"[å†å²å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºä¸ºæ–°å‡ºç°åŒºåŸŸï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼Œå†å²æ— æ•°æ®")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0:
        for region_col in ['Parent Region Name', 'License Province', 'license_city_level', 'License City']:
            if region_col not in cm2_data.columns or region_col not in cm1_data.columns:
                continue
                
            # CM2å’ŒCM1åœ°åŒºåˆ†å¸ƒ
            cm2_region_dist = cm2_data[region_col].value_counts(normalize=True)
            cm1_region_dist = cm1_data[region_col].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰åœ°åŒº
            all_regions = set(cm2_region_dist.index) | set(cm1_region_dist.index)
            
            # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡20%ï¼‰
            for region in all_regions:
                cm2_ratio = cm2_region_dist.get(region, 0)
                cm1_ratio = cm1_region_dist.get(region, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.2:  # 20%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºè®¢å•å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
                elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„åœ°åŒºï¼Œå æ¯”è¶…è¿‡1%
                    anomalies.append(f"[CM1å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºä¸ºæ–°å‡ºç°åŒºåŸŸï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼ŒCM1æ— æ•°æ®")
    
    # 3. CM2é€€è®¢ vs CM2æ•´ä½“å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    if cm2_data is not None and len(cm2_data) > 0:
        # ç­›é€‰é€€è®¢æ•°æ®ï¼ˆåŸºäºintention_refund_timeå­—æ®µä¸ä¸ºç©ºï¼‰
        refund_data = None
        if 'intention_refund_time' in cm2_data.columns:
            refund_data = cm2_data[cm2_data['intention_refund_time'].notna()]
        
        if refund_data is not None and len(refund_data) > 0:
            for region_col in ['Parent Region Name', 'License Province', 'license_city_level', 'License City']:
                if region_col not in cm2_data.columns:
                    continue
                    
                # CM2é€€è®¢å’Œæ•´ä½“åœ°åŒºåˆ†å¸ƒ
                refund_region_dist = refund_data[region_col].value_counts(normalize=True)
                overall_region_dist = cm2_data[region_col].value_counts(normalize=True)
                
                # è·å–æ‰€æœ‰åœ°åŒº
                all_regions = set(refund_region_dist.index) | set(overall_region_dist.index)
                
                # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡20%ï¼‰
                for region in all_regions:
                    refund_ratio = refund_region_dist.get(region, 0)
                    overall_ratio = overall_region_dist.get(region, 0)
                    
                    # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                    if overall_ratio > 0 and refund_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                        change_rate = abs(refund_ratio - overall_ratio) / overall_ratio
                        if change_rate > 0.2:  # 20%å˜åŒ–å¹…åº¦é˜ˆå€¼
                            change_direction = "å¢é•¿" if refund_ratio > overall_ratio else "ä¸‹é™"
                            anomalies.append(f"[é€€è®¢å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºè®¢å•å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2é€€è®¢ä¸º{refund_ratio:.2%}ï¼ŒCM2æ•´ä½“ä¸º{overall_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
                    elif refund_ratio > 0.01:  # æ–°å‡ºç°çš„åœ°åŒºï¼Œå æ¯”è¶…è¿‡1%
                        anomalies.append(f"[é€€è®¢å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºä¸ºé€€è®¢ç‰¹æœ‰åŒºåŸŸï¼šCM2é€€è®¢å æ¯”{refund_ratio:.2%}ï¼Œæ•´ä½“æ— æ˜¾è‘—æ•°æ®")
    
    # 4. CM2 å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æï¼ˆæ—¥ç¯æ¯”å¢é€Ÿï¼‰
    if 'intention_refund_time' in cm2_data.columns:
        # ç­›é€‰é€€è®¢æ•°æ®ï¼ˆåŸºäºintention_refund_timeå­—æ®µä¸ä¸ºç©ºï¼‰
        cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
        
        if len(cm2_refund_data) > 0:
            # è·å–å½“æ—¥æ—¥æœŸï¼ˆä¸è®¢å•æ•°æ®ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨è®¢å•æ•°æ®çš„æœ€æ–°æ—¥æœŸï¼‰
            cm2_order_data_copy = cm2_data.copy()
            cm2_order_data_copy['date'] = cm2_order_data_copy['Intention_Payment_Time'].dt.date
            latest_date = cm2_order_data_copy['date'].max()
            
            # è·å–å‰ä¸€æ—¥æ—¥æœŸ
            previous_date = latest_date - pd.Timedelta(days=1)
            
            # ç­›é€‰å½“æ—¥å’Œå‰ä¸€æ—¥é€€è®¢æ•°æ®
            cm2_daily_refund_data = cm2_refund_data[cm2_refund_data['intention_refund_time'].dt.date == latest_date]
            cm2_previous_refund_data = cm2_refund_data[cm2_refund_data['intention_refund_time'].dt.date == previous_date]
            
            if len(cm2_daily_refund_data) > 0:
                # è®¡ç®—å½“æ—¥å’Œå‰ä¸€æ—¥å„åœ°åŒºé€€è®¢æ•°é‡
                cm2_daily_refund_counts = cm2_daily_refund_data[region_col].value_counts()
                cm2_previous_refund_counts = cm2_previous_refund_data[region_col].value_counts() if len(cm2_previous_refund_data) > 0 else pd.Series()
                
                # è·å–æ‰€æœ‰åœ°åŒº
                all_regions = set(cm2_daily_refund_counts.index) | set(cm2_previous_refund_counts.index)
                
                # æ£€æŸ¥æ—¥ç¯æ¯”å¢é€Ÿå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
                for region in all_regions:
                    daily_count = cm2_daily_refund_counts.get(region, 0)
                    previous_count = cm2_previous_refund_counts.get(region, 0)
                    
                    # è®¡ç®—æ—¥ç¯æ¯”å¢é€Ÿ
                    if previous_count > 0 and daily_count >= 1:  # å½“æ—¥è‡³å°‘1å•é€€è®¢
                        change_rate = (daily_count - previous_count) / previous_count
                        if abs(change_rate) > 0.10:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                            change_direction = "å¢é•¿" if change_rate > 0 else "ä¸‹é™"
                            anomalies.append(f"[å½“æ—¥é€€è®¢å¼‚å¸¸]{region_col}ä¸­{region}åœ°åŒºå½“æ—¥é€€è®¢æ—¥ç¯æ¯”å¼‚å¸¸{change_direction}ï¼šå½“æ—¥{daily_count}å•ï¼Œå‰æ—¥{previous_count}å•ï¼Œå¢é€Ÿ{change_rate:.1%}")
                    elif daily_count >= 2 and previous_count == 0:  # æ–°å‡ºç°çš„åœ°åŒºï¼Œå½“æ—¥è‡³å°‘2å•
                        anomalies.append(f"[å½“æ—¥é€€è®¢å¼‚å¸¸]{region_col}ä¸­{region}åœ°åŒºå½“æ—¥æ–°å¢é€€è®¢ï¼šå½“æ—¥{daily_count}å•ï¼Œå‰æ—¥0å•")
    
    return anomalies

# æ¸ é“ç»“æ„å¼‚å¸¸åˆ†æ
def analyze_channel_structure(vehicle_data):
    """
    åˆ†ææ¸ é“ç»“æ„å¼‚å¸¸
    """
    anomalies = []
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ¸ é“ç»“æ„åˆ†æ"]
    
    channel_col = 'first_middle_channel_name'
    
    if channel_col not in cm2_data.columns:
        return [f"ç¼ºå°‘{channel_col}åˆ—ï¼Œæ— æ³•è¿›è¡Œæ¸ é“åˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    # CM2æ¸ é“åˆ†å¸ƒ
    cm2_channel_dist = cm2_data[channel_col].value_counts(normalize=True)
    
    # å†å²æ¸ é“åˆ†å¸ƒ
    historical_dists = []
    for vehicle in historical_vehicles:
        if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
            hist_dist = vehicle_data[vehicle][channel_col].value_counts(normalize=True)
            historical_dists.append(hist_dist)
    
    if historical_dists:
        # è®¡ç®—å†å²å¹³å‡åˆ†å¸ƒ
        all_channels = set()
        for dist in historical_dists:
            all_channels.update(dist.index)
        all_channels.update(cm2_channel_dist.index)
        
        avg_historical = pd.Series(0.0, index=list(all_channels))
        for dist in historical_dists:
            for channel in all_channels:
                avg_historical[channel] += dist.get(channel, 0) / len(historical_dists)
        
        # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
        for channel in all_channels:
            cm2_ratio = cm2_channel_dist.get(channel, 0)
            hist_ratio = avg_historical.get(channel, 0)
            
            # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
            if hist_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                if change_rate > 0.15:  # 15%å˜åŒ–å¹…åº¦é˜ˆå€¼
                    change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                    anomalies.append(f"[å†å²å¯¹æ¯”]æ¸ é“{channel}é”€é‡å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
            elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„æ¸ é“ï¼Œå æ¯”è¶…è¿‡1%
                anomalies.append(f"[å†å²å¯¹æ¯”]æ¸ é“{channel}ä¸ºæ–°å‡ºç°æ¸ é“ï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼Œå†å²æ— æ•°æ®")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0 and channel_col in cm1_data.columns:
        # CM2å’ŒCM1æ¸ é“åˆ†å¸ƒ
        cm2_channel_dist = cm2_data[channel_col].value_counts(normalize=True)
        cm1_channel_dist = cm1_data[channel_col].value_counts(normalize=True)
        
        # è·å–æ‰€æœ‰æ¸ é“
        all_channels = set(cm2_channel_dist.index) | set(cm1_channel_dist.index)
        
        # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
        for channel in all_channels:
            cm2_ratio = cm2_channel_dist.get(channel, 0)
            cm1_ratio = cm1_channel_dist.get(channel, 0)
            
            # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
            if cm1_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                if change_rate > 0.15:  # 15%å˜åŒ–å¹…åº¦é˜ˆå€¼
                    change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                    anomalies.append(f"[CM1å¯¹æ¯”]æ¸ é“{channel}é”€é‡å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
            elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„æ¸ é“ï¼Œå æ¯”è¶…è¿‡1%
                anomalies.append(f"[CM1å¯¹æ¯”]æ¸ é“{channel}ä¸ºæ–°å‡ºç°æ¸ é“ï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼ŒCM1æ— æ•°æ®")
    
    # 3. CM2é€€è®¢ vs CM2æ•´ä½“å·®å¼‚åˆ†æ
    if 'intention_refund_time' in cm2_data.columns:
        # ç­›é€‰é€€è®¢æ•°æ®ï¼ˆåŸºäºintention_refund_timeå­—æ®µä¸ä¸ºç©ºï¼‰
        cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()]
        
        if len(cm2_refund_data) > 0:
            # è®¡ç®—é€€è®¢å’Œæ•´ä½“çš„æ¸ é“åˆ†å¸ƒ
            cm2_refund_channel_dist = cm2_refund_data[channel_col].value_counts(normalize=True)
            cm2_overall_channel_dist = cm2_data[channel_col].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰æ¸ é“
            all_channels = set(cm2_refund_channel_dist.index) | set(cm2_overall_channel_dist.index)
            
            # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡20%ï¼‰
            for channel in all_channels:
                refund_ratio = cm2_refund_channel_dist.get(channel, 0)
                overall_ratio = cm2_overall_channel_dist.get(channel, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if overall_ratio > 0 and refund_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                    change_rate = abs(refund_ratio - overall_ratio) / overall_ratio
                    if change_rate > 0.20:  # 20%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if refund_ratio > overall_ratio else "ä¸‹é™"
                        anomalies.append(f"[é€€è®¢å¯¹æ¯”]æ¸ é“{channel}é€€è®¢å æ¯”å¼‚å¸¸{change_direction}ï¼šé€€è®¢ä¸º{refund_ratio:.2%}ï¼Œæ•´ä½“ä¸º{overall_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
                elif refund_ratio > 0.01:  # æ–°å‡ºç°çš„æ¸ é“ï¼Œå æ¯”è¶…è¿‡1%
                    anomalies.append(f"[é€€è®¢å¯¹æ¯”]æ¸ é“{channel}åœ¨é€€è®¢ä¸­ä¸ºæ–°å‡ºç°æ¸ é“ï¼šé€€è®¢å æ¯”{refund_ratio:.2%}ï¼Œæ•´ä½“å æ¯”{overall_ratio:.2%}")
    
    # 4. CM2 å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æï¼ˆæ—¥ç¯æ¯”å¢é€Ÿï¼‰
    if 'intention_refund_time' in cm2_data.columns:
        # ç­›é€‰é€€è®¢æ•°æ®ï¼ˆåŸºäºintention_refund_timeå­—æ®µä¸ä¸ºç©ºï¼‰
        cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
        
        if len(cm2_refund_data) > 0:
            # è·å–å½“æ—¥æ—¥æœŸï¼ˆä¸è®¢å•æ•°æ®ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨è®¢å•æ•°æ®çš„æœ€æ–°æ—¥æœŸï¼‰
            cm2_order_data_copy = cm2_data.copy()
            cm2_order_data_copy['date'] = cm2_order_data_copy['Intention_Payment_Time'].dt.date
            latest_date = cm2_order_data_copy['date'].max()
            
            # è·å–å‰ä¸€æ—¥æ—¥æœŸ
            previous_date = latest_date - pd.Timedelta(days=1)
            
            # ç­›é€‰å½“æ—¥å’Œå‰ä¸€æ—¥é€€è®¢æ•°æ®
            cm2_daily_refund_data = cm2_refund_data[cm2_refund_data['intention_refund_time'].dt.date == latest_date]
            cm2_previous_refund_data = cm2_refund_data[cm2_refund_data['intention_refund_time'].dt.date == previous_date]
            
            if len(cm2_daily_refund_data) > 0:
                # è®¡ç®—å½“æ—¥å’Œå‰ä¸€æ—¥å„æ¸ é“é€€è®¢æ•°é‡
                cm2_daily_refund_counts = cm2_daily_refund_data[channel_col].value_counts()
                cm2_previous_refund_counts = cm2_previous_refund_data[channel_col].value_counts() if len(cm2_previous_refund_data) > 0 else pd.Series()
                
                # è·å–æ‰€æœ‰æ¸ é“
                all_channels = set(cm2_daily_refund_counts.index) | set(cm2_previous_refund_counts.index)
                
                # æ£€æŸ¥æ—¥ç¯æ¯”å¢é€Ÿå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
                for channel in all_channels:
                    daily_count = cm2_daily_refund_counts.get(channel, 0)
                    previous_count = cm2_previous_refund_counts.get(channel, 0)
                    
                    # è®¡ç®—æ—¥ç¯æ¯”å¢é€Ÿ
                    if previous_count > 0 and daily_count >= 1:  # å½“æ—¥è‡³å°‘1å•é€€è®¢
                        change_rate = (daily_count - previous_count) / previous_count
                        if abs(change_rate) > 0.10:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                            change_direction = "å¢é•¿" if change_rate > 0 else "ä¸‹é™"
                            anomalies.append(f"[å½“æ—¥é€€è®¢å¼‚å¸¸]æ¸ é“{channel}å½“æ—¥é€€è®¢æ—¥ç¯æ¯”å¼‚å¸¸{change_direction}ï¼šå½“æ—¥{daily_count}å•ï¼Œå‰æ—¥{previous_count}å•ï¼Œå¢é€Ÿ{change_rate:.1%}")
                    elif daily_count >= 2 and previous_count == 0:  # æ–°å‡ºç°çš„æ¸ é“ï¼Œå½“æ—¥è‡³å°‘2å•
                        anomalies.append(f"[å½“æ—¥é€€è®¢å¼‚å¸¸]æ¸ é“{channel}å½“æ—¥æ–°å¢é€€è®¢ï¼šå½“æ—¥{daily_count}å•ï¼Œå‰æ—¥0å•")
    
    return anomalies

# äººç¾¤ç»“æ„å¼‚å¸¸åˆ†æ
def analyze_demographic_structure(vehicle_data):
    """
    åˆ†æäººç¾¤ç»“æ„å¼‚å¸¸
    """
    anomalies = []
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œäººç¾¤ç»“æ„åˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    # 1.1 æ€§åˆ«æ¯”ä¾‹åˆ†æ
    if 'order_gender' in cm2_data.columns:
        cm2_gender_dist = cm2_data['order_gender'].value_counts(normalize=True)
        
        historical_gender_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                hist_dist = vehicle_data[vehicle]['order_gender'].value_counts(normalize=True)
                historical_gender_dists.append(hist_dist)
        
        if historical_gender_dists:
            all_genders = set()
            for dist in historical_gender_dists:
                all_genders.update(dist.index)
            all_genders.update(cm2_gender_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_genders))
            for dist in historical_gender_dists:
                for gender in all_genders:
                    avg_historical[gender] += dist.get(gender, 0) / len(historical_gender_dists)
            
            # æ£€æŸ¥æ€§åˆ«æ¯”ä¾‹å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for gender in all_genders:
                cm2_ratio = cm2_gender_dist.get(gender, 0)
                hist_ratio = avg_historical.get(gender, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0:
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]æ€§åˆ«{gender}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    # 1.2 å¹´é¾„æ®µç»“æ„åˆ†æ
    if 'buyer_age' in cm2_data.columns:
        # å®šä¹‰å¹´é¾„æ®µ
        def age_group(age):
            if pd.isna(age):
                return 'æœªçŸ¥'
            elif age < 25:
                return '25å²ä»¥ä¸‹'
            elif age < 35:
                return '25-34å²'
            elif age < 45:
                return '35-44å²'
            elif age < 55:
                return '45-54å²'
            else:
                return '55å²ä»¥ä¸Š'
        
        cm2_data_copy = cm2_data.copy()
        cm2_data_copy['age_group'] = cm2_data_copy['buyer_age'].apply(age_group)
        cm2_age_dist = cm2_data_copy['age_group'].value_counts(normalize=True)
        
        historical_age_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                vehicle_data_copy = vehicle_data[vehicle].copy()
                vehicle_data_copy['age_group'] = vehicle_data_copy['buyer_age'].apply(age_group)
                hist_dist = vehicle_data_copy['age_group'].value_counts(normalize=True)
                historical_age_dists.append(hist_dist)
        
        if historical_age_dists:
            all_age_groups = set()
            for dist in historical_age_dists:
                all_age_groups.update(dist.index)
            all_age_groups.update(cm2_age_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_age_groups))
            for dist in historical_age_dists:
                for age_group_name in all_age_groups:
                    avg_historical[age_group_name] += dist.get(age_group_name, 0) / len(historical_age_dists)
            
            # æ£€æŸ¥å¹´é¾„æ®µå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for age_group_name in all_age_groups:
                cm2_ratio = cm2_age_dist.get(age_group_name, 0)
                hist_ratio = avg_historical.get(age_group_name, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0:
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]å¹´é¾„æ®µ{age_group_name}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0:
        # 2.1 æ€§åˆ«æ¯”ä¾‹åˆ†æ
        if 'order_gender' in cm2_data.columns and 'order_gender' in cm1_data.columns:
            cm2_gender_dist = cm2_data['order_gender'].value_counts(normalize=True)
            cm1_gender_dist = cm1_data['order_gender'].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰æ€§åˆ«
            all_genders = set(cm2_gender_dist.index) | set(cm1_gender_dist.index)
            
            # æ£€æŸ¥æ€§åˆ«æ¯”ä¾‹å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for gender in all_genders:
                cm2_ratio = cm2_gender_dist.get(gender, 0)
                cm1_ratio = cm1_gender_dist.get(gender, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0:
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]æ€§åˆ«{gender}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
        
        # 2.2 å¹´é¾„æ®µç»“æ„åˆ†æ
        if 'buyer_age' in cm2_data.columns and 'buyer_age' in cm1_data.columns:
            # å®šä¹‰å¹´é¾„æ®µ
            def age_group(age):
                if pd.isna(age):
                    return 'æœªçŸ¥'
                elif age < 25:
                    return '25å²ä»¥ä¸‹'
                elif age < 35:
                    return '25-34å²'
                elif age < 45:
                    return '35-44å²'
                elif age < 55:
                    return '45-54å²'
                else:
                    return '55å²ä»¥ä¸Š'
            
            cm2_data_copy = cm2_data.copy()
            cm2_data_copy['age_group'] = cm2_data_copy['buyer_age'].apply(age_group)
            cm2_age_dist = cm2_data_copy['age_group'].value_counts(normalize=True)
            
            cm1_data_copy = cm1_data.copy()
            cm1_data_copy['age_group'] = cm1_data_copy['buyer_age'].apply(age_group)
            cm1_age_dist = cm1_data_copy['age_group'].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰å¹´é¾„æ®µ
            all_age_groups = set(cm2_age_dist.index) | set(cm1_age_dist.index)
            
            # æ£€æŸ¥å¹´é¾„æ®µå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for age_group_name in all_age_groups:
                cm2_ratio = cm2_age_dist.get(age_group_name, 0)
                cm1_ratio = cm1_age_dist.get(age_group_name, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0:
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]å¹´é¾„æ®µ{age_group_name}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    # 3. CM2é€€è®¢ vs CM2æ•´ä½“å·®å¼‚åˆ†æ
    if 'intention_refund_time' in cm2_data.columns:
        # ç­›é€‰é€€è®¢æ•°æ®ï¼ˆåŸºäºintention_refund_timeå­—æ®µä¸ä¸ºç©ºï¼‰
        cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()]
        
        if len(cm2_refund_data) > 0:
            # 3.1 æ€§åˆ«æ¯”ä¾‹åˆ†æ
            if 'order_gender' in cm2_data.columns:
                cm2_refund_gender_dist = cm2_refund_data['order_gender'].value_counts(normalize=True)
                cm2_overall_gender_dist = cm2_data['order_gender'].value_counts(normalize=True)
                
                # è·å–æ‰€æœ‰æ€§åˆ«
                all_genders = set(cm2_refund_gender_dist.index) | set(cm2_overall_gender_dist.index)
                
                # æ£€æŸ¥æ€§åˆ«æ¯”ä¾‹å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
                for gender in all_genders:
                    refund_ratio = cm2_refund_gender_dist.get(gender, 0)
                    overall_ratio = cm2_overall_gender_dist.get(gender, 0)
                    
                    # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                    if overall_ratio > 0:
                        change_rate = abs(refund_ratio - overall_ratio) / overall_ratio
                        if change_rate > 0.15:  # 15%å˜åŒ–å¹…åº¦é˜ˆå€¼
                            change_direction = "å¢é•¿" if refund_ratio > overall_ratio else "ä¸‹é™"
                            anomalies.append(f"[é€€è®¢å¯¹æ¯”]æ€§åˆ«{gender}é€€è®¢æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šé€€è®¢ä¸º{refund_ratio:.2%}ï¼Œæ•´ä½“ä¸º{overall_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
            
            # 3.2 å¹´é¾„æ®µç»“æ„åˆ†æ
            if 'buyer_age' in cm2_data.columns:
                # å®šä¹‰å¹´é¾„æ®µ
                def age_group(age):
                    if pd.isna(age):
                        return 'æœªçŸ¥'
                    elif age < 25:
                        return '25å²ä»¥ä¸‹'
                    elif age < 35:
                        return '25-34å²'
                    elif age < 45:
                        return '35-44å²'
                    elif age < 55:
                        return '45-54å²'
                    else:
                        return '55å²ä»¥ä¸Š'
                
                cm2_refund_data_copy = cm2_refund_data.copy()
                cm2_refund_data_copy['age_group'] = cm2_refund_data_copy['buyer_age'].apply(age_group)
                cm2_refund_age_dist = cm2_refund_data_copy['age_group'].value_counts(normalize=True)
                
                cm2_overall_data_copy = cm2_data.copy()
                cm2_overall_data_copy['age_group'] = cm2_overall_data_copy['buyer_age'].apply(age_group)
                cm2_overall_age_dist = cm2_overall_data_copy['age_group'].value_counts(normalize=True)
                
                # è·å–æ‰€æœ‰å¹´é¾„æ®µ
                all_age_groups = set(cm2_refund_age_dist.index) | set(cm2_overall_age_dist.index)
                
                # æ£€æŸ¥å¹´é¾„æ®µå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
                for age_group_name in all_age_groups:
                    refund_ratio = cm2_refund_age_dist.get(age_group_name, 0)
                    overall_ratio = cm2_overall_age_dist.get(age_group_name, 0)
                    
                    # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                    if overall_ratio > 0:
                        change_rate = abs(refund_ratio - overall_ratio) / overall_ratio
                        if change_rate > 0.15:  # 15%å˜åŒ–å¹…åº¦é˜ˆå€¼
                            change_direction = "å¢é•¿" if refund_ratio > overall_ratio else "ä¸‹é™"
                            anomalies.append(f"[é€€è®¢å¯¹æ¯”]å¹´é¾„æ®µ{age_group_name}é€€è®¢æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šé€€è®¢ä¸º{refund_ratio:.2%}ï¼Œæ•´ä½“ä¸º{overall_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    # 4. CM2 å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æï¼ˆæ—¥ç¯æ¯”å¢é€Ÿï¼‰
    if 'intention_refund_time' in cm2_data.columns:
        # ç­›é€‰é€€è®¢æ•°æ®ï¼ˆåŸºäºintention_refund_timeå­—æ®µä¸ä¸ºç©ºï¼‰
        cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
        
        if len(cm2_refund_data) > 0:
            # è·å–å½“æ—¥æ—¥æœŸï¼ˆä¸è®¢å•æ•°æ®ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨è®¢å•æ•°æ®çš„æœ€æ–°æ—¥æœŸï¼‰
            cm2_order_data_copy = cm2_data.copy()
            cm2_order_data_copy['date'] = cm2_order_data_copy['Intention_Payment_Time'].dt.date
            latest_date = cm2_order_data_copy['date'].max()
            
            # è·å–å‰ä¸€æ—¥æ—¥æœŸ
            previous_date = latest_date - pd.Timedelta(days=1)
            
            # ç­›é€‰å½“æ—¥å’Œå‰ä¸€æ—¥é€€è®¢æ•°æ®
            cm2_daily_refund_data = cm2_refund_data[cm2_refund_data['intention_refund_time'].dt.date == latest_date]
            cm2_previous_refund_data = cm2_refund_data[cm2_refund_data['intention_refund_time'].dt.date == previous_date]
            
            if len(cm2_daily_refund_data) > 0:
                # æ€§åˆ«åˆ†å¸ƒæ—¥ç¯æ¯”å¯¹æ¯”
                if 'gender' in cm2_data.columns:
                    cm2_daily_refund_gender_counts = cm2_daily_refund_data['gender'].value_counts()
                    cm2_previous_refund_gender_counts = cm2_previous_refund_data['gender'].value_counts() if len(cm2_previous_refund_data) > 0 else pd.Series()
                    
                    # è·å–æ‰€æœ‰æ€§åˆ«
                    all_genders = set(cm2_daily_refund_gender_counts.index) | set(cm2_previous_refund_gender_counts.index)
                    
                    # æ£€æŸ¥æ—¥ç¯æ¯”å¢é€Ÿå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
                    for gender in all_genders:
                        daily_count = cm2_daily_refund_gender_counts.get(gender, 0)
                        previous_count = cm2_previous_refund_gender_counts.get(gender, 0)
                        
                        # è®¡ç®—æ—¥ç¯æ¯”å¢é€Ÿ
                        if previous_count > 0 and daily_count >= 1:  # å½“æ—¥è‡³å°‘1å•é€€è®¢
                            change_rate = (daily_count - previous_count) / previous_count
                            if abs(change_rate) > 0.10:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                                change_direction = "å¢é•¿" if change_rate > 0 else "ä¸‹é™"
                                anomalies.append(f"[å½“æ—¥é€€è®¢å¼‚å¸¸]æ€§åˆ«{gender}å½“æ—¥é€€è®¢æ—¥ç¯æ¯”å¼‚å¸¸{change_direction}ï¼šå½“æ—¥{daily_count}å•ï¼Œå‰æ—¥{previous_count}å•ï¼Œå¢é€Ÿ{change_rate:.1%}")
                        elif daily_count >= 2 and previous_count == 0:  # æ–°å‡ºç°çš„æ€§åˆ«ï¼Œå½“æ—¥è‡³å°‘2å•
                            anomalies.append(f"[å½“æ—¥é€€è®¢å¼‚å¸¸]æ€§åˆ«{gender}å½“æ—¥æ–°å¢é€€è®¢ï¼šå½“æ—¥{daily_count}å•ï¼Œå‰æ—¥0å•")
                
                # å¹´é¾„æ®µåˆ†å¸ƒæ—¥ç¯æ¯”å¯¹æ¯”
                if 'age' in cm2_data.columns:
                    # å®šä¹‰å¹´é¾„æ®µåˆ†ç»„å‡½æ•°
                    def get_age_group(age):
                        if pd.isna(age):
                            return 'æœªçŸ¥'
                        elif age < 25:
                            return '25å²ä»¥ä¸‹'
                        elif age < 35:
                            return '25-34å²'
                        elif age < 45:
                            return '35-44å²'
                        elif age < 55:
                            return '45-54å²'
                        else:
                            return '55å²ä»¥ä¸Š'
                    
                    # è®¡ç®—å¹´é¾„æ®µåˆ†å¸ƒ
                    cm2_daily_refund_data['age_group'] = cm2_daily_refund_data['age'].apply(get_age_group)
                    cm2_previous_refund_data['age_group'] = cm2_previous_refund_data['age'].apply(get_age_group) if len(cm2_previous_refund_data) > 0 else pd.Series()
                    
                    cm2_daily_refund_age_counts = cm2_daily_refund_data['age_group'].value_counts()
                    cm2_previous_refund_age_counts = cm2_previous_refund_data['age_group'].value_counts() if len(cm2_previous_refund_data) > 0 else pd.Series()
                    
                    # è·å–æ‰€æœ‰å¹´é¾„æ®µ
                    all_age_groups = set(cm2_daily_refund_age_counts.index) | set(cm2_previous_refund_age_counts.index)
                    
                    # æ£€æŸ¥æ—¥ç¯æ¯”å¢é€Ÿå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
                    for age_group in all_age_groups:
                        daily_count = cm2_daily_refund_age_counts.get(age_group, 0)
                        previous_count = cm2_previous_refund_age_counts.get(age_group, 0)
                        
                        # è®¡ç®—æ—¥ç¯æ¯”å¢é€Ÿ
                        if previous_count > 0 and daily_count >= 1:  # å½“æ—¥è‡³å°‘1å•é€€è®¢
                            change_rate = (daily_count - previous_count) / previous_count
                            if abs(change_rate) > 0.10:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                                change_direction = "å¢é•¿" if change_rate > 0 else "ä¸‹é™"
                                anomalies.append(f"[å½“æ—¥é€€è®¢å¼‚å¸¸]å¹´é¾„æ®µ{age_group}å½“æ—¥é€€è®¢æ—¥ç¯æ¯”å¼‚å¸¸{change_direction}ï¼šå½“æ—¥{daily_count}å•ï¼Œå‰æ—¥{previous_count}å•ï¼Œå¢é€Ÿ{change_rate:.1%}")
                        elif daily_count >= 2 and previous_count == 0:  # æ–°å‡ºç°çš„å¹´é¾„æ®µï¼Œå½“æ—¥è‡³å°‘2å•
                            anomalies.append(f"[å½“æ—¥é€€è®¢å¼‚å¸¸]å¹´é¾„æ®µ{age_group}å½“æ—¥æ–°å¢é€€è®¢ï¼šå½“æ—¥{daily_count}å•ï¼Œå‰æ—¥0å•")
    
    return anomalies

# åŒæ¯”/ç¯æ¯”å¼‚å¸¸åˆ†æ
def analyze_time_series_anomalies(vehicle_data, presale_periods):
    """
    åˆ†æCM2è½¦å‹çš„åŒæ¯”/ç¯æ¯”å¼‚å¸¸
    åŒ…æ‹¬ï¼šæ—¥ç¯æ¯”ã€åŒå‘¨æœŸå¯¹æ¯”ã€ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”
    """
    anomalies = []
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒæ¯”/ç¯æ¯”åˆ†æ"]
    
    # è·å–CM2çš„èµ·å§‹æ—¥æœŸ
    cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
    
    # å‡†å¤‡CM2æ¯æ—¥è®¢å•æ•°æ®
    cm2_data_copy = cm2_data.copy()
    cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
    cm2_daily = cm2_data_copy.groupby('date').size().reset_index(name='orders')
    cm2_daily['date'] = pd.to_datetime(cm2_daily['date'])
    cm2_daily = cm2_daily.sort_values('date')
    
    # 1. æ—¥ç¯æ¯”å¼‚å¸¸æ£€æŸ¥ï¼ˆCM2å†…éƒ¨å¯¹æ¯”ï¼‰
    for i in range(1, len(cm2_daily)):
        current_orders = cm2_daily.iloc[i]['orders']
        previous_orders = cm2_daily.iloc[i-1]['orders']
        current_date = cm2_daily.iloc[i]['date']
        
        if previous_orders > 0:
            change_rate = (current_orders - previous_orders) / previous_orders
            if abs(change_rate) > 0.5:  # 50%é˜ˆå€¼
                change_type = "éª¤å¢" if change_rate > 0 else "éª¤é™"
                anomalies.append(f"[æ—¥ç¯æ¯”]{current_date.strftime('%Y-%m-%d')}è®¢å•é‡å¼‚å¸¸{change_type}ï¼šå½“æ—¥{current_orders}å•ï¼Œå‰æ—¥{previous_orders}å•ï¼Œå˜åŒ–å¹…åº¦{change_rate*100:.1f}%")
    
    # 2. åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸æ£€æŸ¥ï¼ˆCM2 vs CM0, CM1, DM0, DM1ï¼‰
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        
        # å¯¹æ¯”ç›¸åŒç›¸å¯¹å¤©æ•°çš„è®¢å•é‡
        for _, cm2_row in cm2_daily.iterrows():
            cm2_date = cm2_row['date']
            cm2_orders = cm2_row['orders']
            
            # è®¡ç®—ç›¸å¯¹äºèµ·å§‹æ—¥çš„å¤©æ•°
            days_from_start = (cm2_date - cm2_start).days
            
            # æ‰¾åˆ°å¯¹åº”çš„å†å²è½¦å‹æ—¥æœŸ
            target_date = vehicle_start + pd.Timedelta(days=days_from_start)
            
            # æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„è®¢å•é‡
            vehicle_orders_on_date = vehicle_daily[vehicle_daily['date'] == target_date]
            
            if not vehicle_orders_on_date.empty:
                vehicle_orders = vehicle_orders_on_date.iloc[0]['orders']
                
                if vehicle_orders > 0:
                    change_rate = (cm2_orders - vehicle_orders) / vehicle_orders
                    if abs(change_rate) > 1.0:  # 100%é˜ˆå€¼
                        change_type = "éª¤å¢" if change_rate > 0 else "éª¤é™"
                        anomalies.append(f"[åŒå‘¨æœŸå¯¹æ¯”]CM2åœ¨{cm2_date.strftime('%Y-%m-%d')}ç›¸å¯¹{vehicle}åŒæœŸå¼‚å¸¸{change_type}ï¼šCM2ä¸º{cm2_orders}å•ï¼Œ{vehicle}åŒæœŸä¸º{vehicle_orders}å•ï¼Œå˜åŒ–å¹…åº¦{change_rate*100:.1f}%")
    
    # 3. ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸æ£€æŸ¥
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        
        # è®¡ç®—ç´¯è®¡è®¢å•é‡å¯¹æ¯”
        for i, cm2_row in cm2_daily.iterrows():
            cm2_date = cm2_row['date']
            
            # è®¡ç®—ç›¸å¯¹äºèµ·å§‹æ—¥çš„å¤©æ•°
            days_from_start = (cm2_date - cm2_start).days
            
            # CM2ç´¯è®¡è®¢å•é‡ï¼ˆä»èµ·å§‹æ—¥åˆ°å½“å‰æ—¥ï¼‰
            cm2_cumulative = cm2_daily[cm2_daily['date'] <= cm2_date]['orders'].sum()
            
            # å†å²è½¦å‹å¯¹åº”æœŸé—´çš„ç´¯è®¡è®¢å•é‡
            target_end_date = vehicle_start + pd.Timedelta(days=days_from_start)
            vehicle_cumulative_data = vehicle_daily[
                (vehicle_daily['date'] >= vehicle_start) & 
                (vehicle_daily['date'] <= target_end_date)
            ]
            
            if not vehicle_cumulative_data.empty:
                vehicle_cumulative = vehicle_cumulative_data['orders'].sum()
                
                if vehicle_cumulative > 0:
                    change_rate = (cm2_cumulative - vehicle_cumulative) / vehicle_cumulative
                    if abs(change_rate) > 1.0:  # 100%é˜ˆå€¼
                        change_type = "éª¤å¢" if change_rate > 0 else "éª¤é™"
                        anomalies.append(f"[ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”]CM2æˆªè‡³{cm2_date.strftime('%Y-%m-%d')}ç´¯è®¡è®¢å•ç›¸å¯¹{vehicle}åŒæœŸå¼‚å¸¸{change_type}ï¼šCM2ç´¯è®¡{cm2_cumulative}å•ï¼Œ{vehicle}åŒæœŸç´¯è®¡{vehicle_cumulative}å•ï¼Œå˜åŒ–å¹…åº¦{change_rate*100:.1f}%")
    
    return anomalies

# ç”Ÿæˆæ—¥ç¯æ¯”æè¿°æ•°æ®
def generate_post_launch_lock_analysis(vehicle_data, presale_periods):
    """
    ç”Ÿæˆå‘å¸ƒä¼šåç¬¬Næ—¥é”å•æ•°æ®å¯¹æ¯”åˆ†æ
    åŒ…æ‹¬ï¼šCM2è½¦å‹é”å•æ•°ã€å°è®¢ç•™å­˜é”å•æ•°ã€åŒæœŸå…¶ä»–è½¦å‹å¯¹æ¯”ã€ç´¯è®¡å¯¹æ¯”
    
    ç¬¬Næ—¥è®¡ç®—é€»è¾‘ï¼š
    - CM2: N = max(Lock_Time) - CM2æœ€å¤§æ—¥æœŸçš„æ—¶é—´å·®
    - å…¶ä»–è½¦å‹: ä½¿ç”¨CM2çš„Nå€¼ï¼ŒåŒ¹é…å„è‡ªæœ€å¤§æ—¥æœŸ + (N-1) å¯¹åº”çš„lock_time
    """
    analysis_data = {
        'cm2_lock_orders': {},
        'cm2_small_order_retention': {},
        'other_vehicles_lock_orders': {},
        'other_vehicles_small_order_retention': {},
        'cumulative_lock_orders': {},
        'cumulative_small_order_retention': {}
    }
    
    # è·å–å„è½¦å‹çš„æœ€å¤§æ—¥æœŸ
    max_dates = {}
    for vehicle, period in presale_periods.items():
        max_dates[vehicle] = pd.to_datetime(period['end'])
    
    # é¦–å…ˆå¤„ç†CM2è½¦å‹æ•°æ®ï¼Œè®¡ç®—åŸºå‡†Nå€¼
    cm2_n_days = None
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is not None and len(cm2_data) > 0:
        cm2_max_date = max_dates.get('CM2')
        if cm2_max_date is not None:
            # è·å–æœ‰Lock_Timeçš„è®¢å•
            cm2_lock_data = cm2_data[cm2_data['Lock_Time'].notna()].copy()
            
            if not cm2_lock_data.empty:
                # è®¡ç®—ç¬¬Næ—¥ï¼šN = (å½“å‰æ—¶é—´æˆ³-1å¤©) - CM2çš„endæ—¥æœŸ
                from datetime import datetime, timedelta
                current_timestamp = datetime.now()
                target_date = current_timestamp - timedelta(days=1)
                cm2_n_days = (target_date - cm2_max_date.to_pydatetime()).days
                
                # è®¡ç®—ç›®æ ‡é”å•æ—¥æœŸï¼šCM2çš„endæ—¥æœŸ + Nå¤©
                target_lock_date = cm2_max_date + pd.Timedelta(days=cm2_n_days)
                
                # CM2é”å•æ•°ï¼ˆLock_Timeç­‰äºç›®æ ‡é”å•æ—¥æœŸçš„å½“æ—¥è®¢å•æ•°ï¼‰
                cm2_daily_lock_data = cm2_lock_data[
                    cm2_lock_data['Lock_Time'].dt.date == target_lock_date.date()
                ]
                cm2_lock_count = len(cm2_daily_lock_data)
                
                # CM2å°è®¢ç•™å­˜é”å•æ•°ï¼ˆå½“æ—¥é”å•ä¸­åŒæ—¶å«æœ‰Intention_Payment_Timeä¸”Intention_Payment_Timeå°äºæœ€å¤§æ—¥æœŸï¼‰
                cm2_small_retention = cm2_daily_lock_data[
                    (cm2_daily_lock_data['Intention_Payment_Time'].notna()) & 
                    (cm2_daily_lock_data['Intention_Payment_Time'] < cm2_max_date)
                ]
                cm2_small_retention_count = len(cm2_small_retention)
                
                analysis_data['cm2_lock_orders'] = {
                    'n_days': cm2_n_days,
                    'max_lock_time': target_lock_date.strftime('%Y-%m-%d'),
                    'lock_orders_count': cm2_lock_count
                }
                
                analysis_data['cm2_small_order_retention'] = {
                    'n_days': cm2_n_days,
                    'small_retention_count': cm2_small_retention_count
                }
    
    # å¤„ç†å…¶ä»–è½¦å‹çš„åŒæœŸæ•°æ®ï¼ˆä½¿ç”¨CM2çš„Nå€¼ï¼‰
    if cm2_n_days is not None:
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
                continue
            if vehicle not in max_dates:
                continue
                
            vehicle_data_copy = vehicle_data[vehicle].copy()
            vehicle_max_date = max_dates[vehicle]
            
            # è®¡ç®—è¯¥è½¦å‹å¯¹åº”çš„ç›®æ ‡æ—¥æœŸï¼šæœ€å¤§æ—¥æœŸ + N
            target_date = vehicle_max_date + pd.Timedelta(days=cm2_n_days)
            
            # è·å–æœ‰Lock_Timeçš„è®¢å•ï¼Œå¹¶ç­›é€‰Lock_Timeç­‰äºç›®æ ‡æ—¥æœŸçš„è®¢å•
            vehicle_lock_data = vehicle_data_copy[
                (vehicle_data_copy['Lock_Time'].notna()) & 
                (vehicle_data_copy['Lock_Time'].dt.date == target_date.date())
            ].copy()
            
            # é”å•æ•°
            vehicle_lock_count = len(vehicle_lock_data)
            
            # å°è®¢ç•™å­˜é”å•æ•°
            vehicle_small_retention = vehicle_lock_data[
                (vehicle_lock_data['Intention_Payment_Time'].notna()) & 
                (vehicle_lock_data['Intention_Payment_Time'] < vehicle_max_date)
            ]
            vehicle_small_retention_count = len(vehicle_small_retention)
            
            analysis_data['other_vehicles_lock_orders'][vehicle] = {
                'n_days': cm2_n_days,
                'target_date': target_date.strftime('%Y-%m-%d'),
                'lock_orders_count': vehicle_lock_count
            }
            
            analysis_data['other_vehicles_small_order_retention'][vehicle] = {
                'n_days': cm2_n_days,
                'small_retention_count': vehicle_small_retention_count
            }
    
    # è®¡ç®—ç´¯è®¡æ•°æ®ï¼ˆä»ç¬¬0æ—¥åˆ°ç¬¬Næ—¥çš„çœŸæ­£ç´¯è®¡å€¼ï¼‰
    cumulative_lock_by_vehicle = {}
    cumulative_small_by_vehicle = {}
    total_lock_orders = 0
    total_small_retention = 0
    
    # CM2ç´¯è®¡ï¼ˆä»ç¬¬0æ—¥åˆ°ç¬¬Næ—¥ï¼‰
    if cm2_n_days is not None and cm2_data is not None and len(cm2_data) > 0:
        cm2_max_date = max_dates.get('CM2')
        if cm2_max_date is not None:
            cm2_lock_data = cm2_data[cm2_data['Lock_Time'].notna()].copy()
            if not cm2_lock_data.empty:
                # è®¡ç®—ä»ç¬¬0æ—¥åˆ°ç¬¬Næ—¥çš„ç´¯è®¡é”å•æ•°
                start_date = cm2_max_date  # ç¬¬0æ—¥
                end_date = cm2_max_date + pd.Timedelta(days=cm2_n_days)  # ç¬¬Næ—¥
                
                # ç´¯è®¡é”å•æ•°ï¼šLock_Timeåœ¨ç¬¬0æ—¥åˆ°ç¬¬Næ—¥ä¹‹é—´çš„æ‰€æœ‰è®¢å•
                cm2_cumulative_lock_data = cm2_lock_data[
                    (cm2_lock_data['Lock_Time'].dt.date >= start_date.date()) &
                    (cm2_lock_data['Lock_Time'].dt.date <= end_date.date())
                ]
                cm2_cumulative_lock_count = len(cm2_cumulative_lock_data)
                
                # ç´¯è®¡å°è®¢ç•™å­˜é”å•æ•°
                cm2_cumulative_small_retention = cm2_cumulative_lock_data[
                    (cm2_cumulative_lock_data['Intention_Payment_Time'].notna()) & 
                    (cm2_cumulative_lock_data['Intention_Payment_Time'] < cm2_max_date)
                ]
                cm2_cumulative_small_count = len(cm2_cumulative_small_retention)
                
                cumulative_lock_by_vehicle['CM2'] = cm2_cumulative_lock_count
                cumulative_small_by_vehicle['CM2'] = cm2_cumulative_small_count
                total_lock_orders += cm2_cumulative_lock_count
                total_small_retention += cm2_cumulative_small_count
    
    # å…¶ä»–è½¦å‹ç´¯è®¡ï¼ˆä»ç¬¬0æ—¥åˆ°ç¬¬Næ—¥ï¼‰
    if cm2_n_days is not None:
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
                continue
            if vehicle not in max_dates:
                continue
                
            vehicle_data_copy = vehicle_data[vehicle].copy()
            vehicle_max_date = max_dates[vehicle]
            
            # è®¡ç®—ä»ç¬¬0æ—¥åˆ°ç¬¬Næ—¥çš„ç´¯è®¡é”å•æ•°
            start_date = vehicle_max_date  # ç¬¬0æ—¥
            end_date = vehicle_max_date + pd.Timedelta(days=cm2_n_days)  # ç¬¬Næ—¥
            
            # è·å–æœ‰Lock_Timeçš„è®¢å•ï¼Œå¹¶ç­›é€‰Lock_Timeåœ¨ç¬¬0æ—¥åˆ°ç¬¬Næ—¥ä¹‹é—´çš„è®¢å•
            vehicle_cumulative_lock_data = vehicle_data_copy[
                (vehicle_data_copy['Lock_Time'].notna()) & 
                (vehicle_data_copy['Lock_Time'].dt.date >= start_date.date()) &
                (vehicle_data_copy['Lock_Time'].dt.date <= end_date.date())
            ].copy()
            
            # ç´¯è®¡é”å•æ•°
            vehicle_cumulative_lock_count = len(vehicle_cumulative_lock_data)
            
            # ç´¯è®¡å°è®¢ç•™å­˜é”å•æ•°
            vehicle_cumulative_small_retention = vehicle_cumulative_lock_data[
                (vehicle_cumulative_lock_data['Intention_Payment_Time'].notna()) & 
                (vehicle_cumulative_lock_data['Intention_Payment_Time'] < vehicle_max_date)
            ]
            vehicle_cumulative_small_count = len(vehicle_cumulative_small_retention)
            
            cumulative_lock_by_vehicle[vehicle] = vehicle_cumulative_lock_count
            cumulative_small_by_vehicle[vehicle] = vehicle_cumulative_small_count
            total_lock_orders += vehicle_cumulative_lock_count
            total_small_retention += vehicle_cumulative_small_count
    
    analysis_data['cumulative_lock_orders'] = {
        'total_count': total_lock_orders,
        'by_vehicle': cumulative_lock_by_vehicle
    }
    
    analysis_data['cumulative_small_order_retention'] = {
        'total_count': total_small_retention,
        'by_vehicle': cumulative_small_by_vehicle
    }
    
    return analysis_data

def generate_time_series_description(vehicle_data, presale_periods):
    """
    ç”ŸæˆCM2è½¦å‹çš„æ—¥ç¯æ¯”æè¿°æ•°æ®
    åŒ…æ‹¬ï¼šCM2æ—¥è®¢å•æ•°ã€åŒæœŸå…¶ä»–è½¦å‹æ—¥è®¢å•æ•°ã€ç´¯è®¡è®¢å•æ•°å¯¹æ¯”
    """
    description_data = {
        'cm2_daily': [],
        'cm2_cumulative': [],
        'comparison_daily': {},
        'comparison_cumulative': {}
    }
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return description_data
    
    # æ£€æŸ¥presale_periodsæ˜¯å¦æœ‰æ•ˆ
    if not presale_periods or 'CM2' not in presale_periods:
        return description_data
    
    # è·å–CM2çš„èµ·å§‹æ—¥æœŸ
    cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
    
    # å‡†å¤‡CM2æ¯æ—¥è®¢å•æ•°æ®
    cm2_data_copy = cm2_data.copy()
    cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
    cm2_daily = cm2_data_copy.groupby('date').size().reset_index(name='orders')
    cm2_daily['date'] = pd.to_datetime(cm2_daily['date'])
    cm2_daily = cm2_daily.sort_values('date')
    
    # è®¡ç®—CM2ç´¯è®¡è®¢å•æ•°
    cm2_daily['cumulative'] = cm2_daily['orders'].cumsum()
    
    # å­˜å‚¨CM2æ•°æ®ï¼ˆä»…ä¿ç•™æœ€åä¸€å¤©ï¼‰
    if not cm2_daily.empty:
        last_row = cm2_daily.iloc[-1]
        days_from_start = (last_row['date'] - cm2_start).days + 1
        description_data['cm2_daily'].append({
            'date': last_row['date'].strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'orders': last_row['orders']
        })
        description_data['cm2_cumulative'].append({
            'date': last_row['date'].strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'cumulative_orders': last_row['cumulative']
        })
    
    # å¤„ç†å…¶ä»–è½¦å‹çš„åŒæœŸæ•°æ®
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
        
        # æ£€æŸ¥è¯¥è½¦å‹çš„presale_periodsæ˜¯å¦å­˜åœ¨
        if vehicle not in presale_periods:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        vehicle_daily['cumulative'] = vehicle_daily['orders'].cumsum()
        
        description_data['comparison_daily'][vehicle] = []
        description_data['comparison_cumulative'][vehicle] = []
        
        # å¯¹æ¯”ç›¸åŒç›¸å¯¹å¤©æ•°çš„æ•°æ®ï¼ˆä»…ä¿ç•™æœ€åä¸€å¤©ï¼‰
        if not cm2_daily.empty:
            cm2_row = cm2_daily.iloc[-1]
            cm2_date = cm2_row['date']
            days_from_start = (cm2_date - cm2_start).days
            
            # æ‰¾åˆ°å¯¹åº”çš„å†å²è½¦å‹æ—¥æœŸ
            target_date = vehicle_start + pd.Timedelta(days=days_from_start)
            
            # æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„è®¢å•é‡
            vehicle_orders_on_date = vehicle_daily[vehicle_daily['date'] == target_date]
            
            if not vehicle_orders_on_date.empty:
                vehicle_orders = vehicle_orders_on_date.iloc[0]['orders']
                vehicle_cumulative = vehicle_daily[vehicle_daily['date'] <= target_date]['orders'].sum()
                
                description_data['comparison_daily'][vehicle].append({
                    'cm2_date': cm2_date.strftime('%Y-%m-%d'),
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start + 1,
                    'orders': vehicle_orders
                })
                description_data['comparison_cumulative'][vehicle].append({
                    'cm2_date': cm2_date.strftime('%Y-%m-%d'),
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start + 1,
                    'cumulative_orders': vehicle_cumulative
                })
    
    return description_data

# ç”Ÿæˆé€€è®¢æè¿°æ•°æ®
def generate_refund_description(vehicle_data, presale_periods):
    """
    ç”ŸæˆCM2è½¦å‹çš„é€€è®¢æè¿°æ•°æ®
    åŒ…æ‹¬ï¼šCM2æ—¥é€€è®¢æ•°ã€åŒæœŸå…¶ä»–è½¦å‹æ—¥é€€è®¢æ•°ã€ç´¯è®¡é€€è®¢æ•°å¯¹æ¯”
    """

    
    refund_data = {
        'cm2_daily': [],
        'cm2_cumulative': [],
        'comparison_daily': {},
        'comparison_cumulative': {}
    }
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return refund_data
    
    # æ£€æŸ¥presale_periodsæ˜¯å¦æœ‰æ•ˆ
    if not presale_periods or 'CM2' not in presale_periods:
        return refund_data
    
    # è¿‡æ»¤æœ‰é€€è®¢æ—¶é—´çš„æ•°æ®
    cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
    if len(cm2_refund_data) == 0:
        return refund_data
    
    # è·å–CM2çš„èµ·å§‹æ—¥æœŸ
    cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
    
    # å‡†å¤‡CM2æ¯æ—¥é€€è®¢æ•°æ®
    cm2_refund_data['refund_date'] = cm2_refund_data['intention_refund_time'].dt.date
    cm2_daily_refund = cm2_refund_data.groupby('refund_date').size().reset_index(name='refunds')
    cm2_daily_refund['refund_date'] = pd.to_datetime(cm2_daily_refund['refund_date'])
    cm2_daily_refund = cm2_daily_refund.sort_values('refund_date')
    
    # è®¡ç®—CM2ç´¯è®¡é€€è®¢æ•°
    cm2_daily_refund['cumulative'] = cm2_daily_refund['refunds'].cumsum()
    
    # å­˜å‚¨CM2æ•°æ®ï¼ˆä»…ä¿ç•™æœ€åä¸€å¤©ï¼‰
    if not cm2_daily_refund.empty:
        last_row = cm2_daily_refund.iloc[-1]
        days_from_start = (last_row['refund_date'] - cm2_start).days + 1
        refund_data['cm2_daily'].append({
            'date': last_row['refund_date'].strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'refunds': last_row['refunds']
        })
        refund_data['cm2_cumulative'].append({
            'date': last_row['refund_date'].strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'cumulative_refunds': last_row['cumulative']
        })
    
    # å¤„ç†å…¶ä»–è½¦å‹çš„åŒæœŸé€€è®¢æ•°æ®
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
        
        # æ£€æŸ¥è¯¥è½¦å‹çš„presale_periodsæ˜¯å¦å­˜åœ¨
        if vehicle not in presale_periods:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
        
        if len(vehicle_refund_data) == 0:
            continue
            
        vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
        vehicle_daily_refund = vehicle_refund_data.groupby('refund_date').size().reset_index(name='refunds')
        vehicle_daily_refund['refund_date'] = pd.to_datetime(vehicle_daily_refund['refund_date'])
        vehicle_daily_refund = vehicle_daily_refund.sort_values('refund_date')
        vehicle_daily_refund['cumulative'] = vehicle_daily_refund['refunds'].cumsum()
        
        refund_data['comparison_daily'][vehicle] = []
        refund_data['comparison_cumulative'][vehicle] = []
        
        # å¯¹æ¯”ç›¸åŒç›¸å¯¹å¤©æ•°çš„æ•°æ®ï¼ˆä»…ä¿ç•™æœ€åä¸€å¤©ï¼‰
        if not cm2_daily_refund.empty:
            cm2_row = cm2_daily_refund.iloc[-1]
            cm2_refund_date = cm2_row['refund_date']
            days_from_start = (cm2_refund_date - cm2_start).days
            
            # æ‰¾åˆ°å¯¹åº”çš„å†å²è½¦å‹æ—¥æœŸ
            target_date = vehicle_start + pd.Timedelta(days=days_from_start)
            
            # æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„é€€è®¢é‡
            vehicle_refunds_on_date = vehicle_daily_refund[vehicle_daily_refund['refund_date'] == target_date]
            
            if not vehicle_refunds_on_date.empty:
                vehicle_refunds = vehicle_refunds_on_date.iloc[0]['refunds']
                vehicle_cumulative = vehicle_daily_refund[vehicle_daily_refund['refund_date'] <= target_date]['refunds'].sum()
                
                refund_data['comparison_daily'][vehicle].append({
                    'cm2_date': cm2_refund_date.strftime('%Y-%m-%d'),
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start + 1,
                    'refunds': vehicle_refunds
                })
                refund_data['comparison_cumulative'][vehicle].append({
                    'cm2_date': cm2_refund_date.strftime('%Y-%m-%d'),
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start + 1,
                    'cumulative_refunds': vehicle_cumulative
                })
    
    return refund_data

def generate_previous_day_data(vehicle_data, presale_periods):
    """
    ç”ŸæˆCM2è½¦å‹å‰ä¸€å¤©çš„è®¢å•å’Œé€€è®¢æ•°æ®
    åŒ…æ‹¬ï¼šCM2å‰ä¸€å¤©è®¢å•æ•°ã€é€€è®¢æ•°ï¼ŒåŒæœŸå…¶ä»–è½¦å‹å¯¹æ¯”ï¼Œä»¥åŠç´¯è®¡æ•°æ®
    """
    previous_day_data = {
        'cm2_previous_day': None,
        'comparison_previous_day': {},
        'cm2_cumulative_previous': None,
        'comparison_cumulative_previous': {}
    }
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return previous_day_data
    
    # æ£€æŸ¥presale_periodsæ˜¯å¦æœ‰æ•ˆ
    if not presale_periods or 'CM2' not in presale_periods:
        return previous_day_data
    
    # è·å–CM2çš„èµ·å§‹æ—¥æœŸ
    cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
    
    # å‡†å¤‡CM2æ¯æ—¥è®¢å•æ•°æ®
    cm2_data_copy = cm2_data.copy()
    cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
    cm2_daily = cm2_data_copy.groupby('date').size().reset_index(name='orders')
    cm2_daily['date'] = pd.to_datetime(cm2_daily['date'])
    cm2_daily = cm2_daily.sort_values('date')
    cm2_daily['cumulative'] = cm2_daily['orders'].cumsum()
    
    # å‡†å¤‡CM2é€€è®¢æ•°æ®
    cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
    if len(cm2_refund_data) > 0:
        cm2_refund_data['refund_date'] = cm2_refund_data['intention_refund_time'].dt.date
        cm2_daily_refund = cm2_refund_data.groupby('refund_date').size().reset_index(name='refunds')
        cm2_daily_refund['refund_date'] = pd.to_datetime(cm2_daily_refund['refund_date'])
        cm2_daily_refund = cm2_daily_refund.sort_values('refund_date')
        cm2_daily_refund['cumulative_refunds'] = cm2_daily_refund['refunds'].cumsum()
    else:
        cm2_daily_refund = pd.DataFrame(columns=['refund_date', 'refunds', 'cumulative_refunds'])
    
    # è·å–å‰ä¸€å¤©æ•°æ®ï¼ˆå€’æ•°ç¬¬äºŒå¤©ï¼‰
    if len(cm2_daily) >= 2:
        previous_day_row = cm2_daily.iloc[-2]  # å€’æ•°ç¬¬äºŒå¤©
        previous_day_date = previous_day_row['date']
        days_from_start = (previous_day_date - cm2_start).days + 1
        
        # è·å–å‰ä¸€å¤©çš„é€€è®¢æ•°
        previous_day_refunds = 0
        if len(cm2_daily_refund) > 0:
            refund_on_date = cm2_daily_refund[cm2_daily_refund['refund_date'] == previous_day_date]
            if not refund_on_date.empty:
                previous_day_refunds = refund_on_date.iloc[0]['refunds']
        
        previous_day_data['cm2_previous_day'] = {
            'date': previous_day_date.strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'orders': previous_day_row['orders'],
            'refunds': previous_day_refunds
        }
        
        # è·å–å‰ä¸€å¤©çš„ç´¯è®¡æ•°æ®
        cumulative_orders = previous_day_row['cumulative']
        cumulative_refunds = 0
        if len(cm2_daily_refund) > 0:
            cumulative_refunds = cm2_daily_refund[cm2_daily_refund['refund_date'] <= previous_day_date]['refunds'].sum()
        
        previous_day_data['cm2_cumulative_previous'] = {
            'day_n': days_from_start,
            'cumulative_orders': cumulative_orders,
            'cumulative_refunds': cumulative_refunds
        }
        
        # å¤„ç†å…¶ä»–è½¦å‹çš„åŒæœŸæ•°æ®
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
                continue
            
            # æ£€æŸ¥è¯¥è½¦å‹çš„presale_periodsæ˜¯å¦å­˜åœ¨
            if vehicle not in presale_periods:
                continue
                
            vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
            
            # å‡†å¤‡è½¦å‹è®¢å•æ•°æ®
            vehicle_data_copy = vehicle_data[vehicle].copy()
            vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
            vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
            vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
            vehicle_daily = vehicle_daily.sort_values('date')
            vehicle_daily['cumulative'] = vehicle_daily['orders'].cumsum()
            
            # å‡†å¤‡è½¦å‹é€€è®¢æ•°æ®
            vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
            if len(vehicle_refund_data) > 0:
                vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
                vehicle_daily_refund = vehicle_refund_data.groupby('refund_date').size().reset_index(name='refunds')
                vehicle_daily_refund['refund_date'] = pd.to_datetime(vehicle_daily_refund['refund_date'])
                vehicle_daily_refund = vehicle_daily_refund.sort_values('refund_date')
            else:
                vehicle_daily_refund = pd.DataFrame(columns=['refund_date', 'refunds'])
            
            # æ‰¾åˆ°å¯¹åº”çš„å†å²è½¦å‹æ—¥æœŸï¼ˆå‰ä¸€å¤©ï¼‰
            target_date = vehicle_start + pd.Timedelta(days=days_from_start-1)
            
            # æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„è®¢å•é‡å’Œé€€è®¢é‡
            vehicle_orders_on_date = vehicle_daily[vehicle_daily['date'] == target_date]
            vehicle_refunds_on_date = 0
            if len(vehicle_daily_refund) > 0:
                refund_on_date = vehicle_daily_refund[vehicle_daily_refund['refund_date'] == target_date]
                if not refund_on_date.empty:
                    vehicle_refunds_on_date = refund_on_date.iloc[0]['refunds']
            
            if not vehicle_orders_on_date.empty:
                vehicle_orders = vehicle_orders_on_date.iloc[0]['orders']
                vehicle_cumulative_orders = vehicle_daily[vehicle_daily['date'] <= target_date]['orders'].sum()
                vehicle_cumulative_refunds = 0
                if len(vehicle_daily_refund) > 0:
                    vehicle_cumulative_refunds = vehicle_daily_refund[vehicle_daily_refund['refund_date'] <= target_date]['refunds'].sum()
                
                previous_day_data['comparison_previous_day'][vehicle] = {
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start,
                    'orders': vehicle_orders,
                    'refunds': vehicle_refunds_on_date
                }
                
                previous_day_data['comparison_cumulative_previous'][vehicle] = {
                    'day_n': days_from_start,
                    'cumulative_orders': vehicle_cumulative_orders,
                    'cumulative_refunds': vehicle_cumulative_refunds
                }
    
    return previous_day_data

# ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
def generate_structure_report(state, vehicle_data, anomalies):
    """
    ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
    """
    from datetime import datetime
    
    report_content = f"""# ç»“æ„æ£€æŸ¥æŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}
- **åˆ†æè½¦å‹**: CM2 vs å†å²è½¦å‹(CM0, DM0, CM1) + CM2 vs CM1ç›´æ¥å¯¹æ¯”
- **æ£€æŸ¥ç»´åº¦**: åœ°åŒºåˆ†å¸ƒã€æ¸ é“ç»“æ„ã€äººç¾¤ç»“æ„

## æ•°æ®æ¦‚å†µ
"""
    
    # æ·»åŠ å„è½¦å‹æ•°æ®é‡ç»Ÿè®¡
    for vehicle, data in vehicle_data.items():
        report_content += f"- **{vehicle}è½¦å‹**: {len(data)} æ¡è®¢å•æ•°æ®\n"
    
    report_content += "\n## ç»“æ„å¼‚å¸¸æ£€æµ‹ç»“æœ\n\n"
    
    # åˆ†ç±»å¼‚å¸¸ - åŒºåˆ†å†å²å¯¹æ¯”ã€CM1å¯¹æ¯”ã€é€€è®¢å¯¹æ¯”å’Œå½“æ—¥é€€è®¢å¼‚å¸¸
    region_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and any(region_type in a for region_type in ['Parent Region Name', 'License Province', 'license_city_level', 'License City'])]
    region_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and any(region_type in a for region_type in ['Parent Region Name', 'License Province', 'license_city_level', 'License City'])]
    region_anomalies_refund = [a for a in anomalies if '[é€€è®¢å¯¹æ¯”]' in a and any(region_type in a for region_type in ['Parent Region Name', 'License Province', 'license_city_level', 'License City'])]
    region_anomalies_daily = [a for a in anomalies if '[å½“æ—¥é€€è®¢å¼‚å¸¸]' in a and any(region_type in a for region_type in ['Parent Region Name', 'License Province', 'license_city_level', 'License City', 'åœ°åŒº'])]
    
    channel_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and 'æ¸ é“' in a]
    channel_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and 'æ¸ é“' in a]
    channel_anomalies_refund = [a for a in anomalies if '[é€€è®¢å¯¹æ¯”]' in a and 'æ¸ é“' in a]
    channel_anomalies_daily = [a for a in anomalies if '[å½“æ—¥é€€è®¢å¼‚å¸¸]' in a and 'æ¸ é“' in a]
    
    demographic_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and any(demo_type in a for demo_type in ['æ€§åˆ«', 'å¹´é¾„æ®µ'])]
    demographic_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and any(demo_type in a for demo_type in ['æ€§åˆ«', 'å¹´é¾„æ®µ'])]
    demographic_anomalies_refund = [a for a in anomalies if '[é€€è®¢å¯¹æ¯”]' in a and any(demo_type in a for demo_type in ['æ€§åˆ«', 'å¹´é¾„æ®µ'])]
    demographic_anomalies_daily = [a for a in anomalies if '[å½“æ—¥é€€è®¢å¼‚å¸¸]' in a and any(demo_type in a for demo_type in ['æ€§åˆ«', 'å¹´é¾„æ®µ'])]
    
    # åŒæ¯”/ç¯æ¯”å¼‚å¸¸
    time_series_anomalies_daily = [a for a in anomalies if '[æ—¥ç¯æ¯”]' in a]
    time_series_anomalies_period = [a for a in anomalies if '[åŒå‘¨æœŸå¯¹æ¯”]' in a]
    time_series_anomalies_cumulative = [a for a in anomalies if '[ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”]' in a]
    
    # åœ°åŒºåˆ†å¸ƒå¼‚å¸¸
    report_content += "### ğŸŒ åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if region_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°åœ°åŒºåˆ†å¸ƒå¼‚å¸¸:**\n\n"
        report_content += "| åºå· | åœ°åŒºç±»å‹ | åœ°åŒºåç§° | CM2å æ¯” | å†å²å¹³å‡å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|---------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(region_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            region_type_match = re.search(r'(Parent Region Name|License Province|license_city_level|License City)ä¸­(.+?)åœ°åŒº', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            hist_match = re.search(r'å†å²å¹³å‡ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            region_type = region_type_match.group(1) if region_type_match else ""
            region_name = region_type_match.group(2) if region_type_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            hist_ratio = hist_match.group(1) if hist_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {region_type} | {region_name} | {cm2_ratio} | {hist_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… åœ°åŒºåˆ†å¸ƒæ­£å¸¸:** æ‰€æœ‰åœ°åŒºè®¢å•å æ¯”å˜åŒ–å‡åœ¨20%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if region_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°åœ°åŒºåˆ†å¸ƒå¼‚å¸¸:**\n\n"
        report_content += "| åºå· | åœ°åŒºç±»å‹ | åœ°åŒºåç§° | CM2å æ¯” | CM1å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|---------|---------|----------|----------|\n"
        
        for i, anomaly in enumerate(region_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            region_type_match = re.search(r'(Parent Region Name|License Province|license_city_level|License City)ä¸­(.+?)åœ°åŒº', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            cm1_match = re.search(r'CM1ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            region_type = region_type_match.group(1) if region_type_match else ""
            region_name = region_type_match.group(2) if region_type_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            cm1_ratio = cm1_match.group(1) if cm1_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {region_type} | {region_name} | {cm2_ratio} | {cm1_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… åœ°åŒºåˆ†å¸ƒæ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰åœ°åŒºè®¢å•å æ¯”å˜åŒ–å‡åœ¨20%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM2é€€è®¢å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2é€€è®¢ vs CM2æ•´ä½“å¯¹æ¯”\n\n"
    if region_anomalies_refund:
        report_content += "**ğŸš¨ å‘ç°åœ°åŒºåˆ†å¸ƒå¼‚å¸¸:**\n\n"
        report_content += "| åºå· | åœ°åŒºç±»å‹ | åœ°åŒºåç§° | CM2é€€è®¢å æ¯” | CM2æ•´ä½“å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|-------------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(region_anomalies_refund, 1):
            clean_anomaly = anomaly.replace('[é€€è®¢å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            region_type_match = re.search(r'(Parent Region Name|License Province|license_city_level|License City)ä¸­(.+?)åœ°åŒº', clean_anomaly)
            refund_match = re.search(r'CM2é€€è®¢ä¸º([\d.]+%)', clean_anomaly)
            overall_match = re.search(r'CM2æ•´ä½“ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            region_type = region_type_match.group(1) if region_type_match else ""
            region_name = region_type_match.group(2) if region_type_match else ""
            refund_ratio = refund_match.group(1) if refund_match else ""
            overall_ratio = overall_match.group(1) if overall_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {region_type} | {region_name} | {refund_ratio} | {overall_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… åœ°åŒºåˆ†å¸ƒæ­£å¸¸:** CM2é€€è®¢ç”¨æˆ·åœ°åŒºåˆ†å¸ƒä¸æ•´ä½“åˆ†å¸ƒå˜åŒ–å‡åœ¨20%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM2å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æ
    report_content += "\n#### ğŸ“… CM2å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æ\n\n"
    if region_anomalies_daily:
        report_content += "**ğŸš¨ å‘ç°å½“æ—¥é€€è®¢åœ°åŒºå¼‚å¸¸:**\n\n"
        report_content += "| åºå· | åœ°åŒºåç§° | å½“æ—¥é€€è®¢æ•°é‡ | å‰æ—¥é€€è®¢æ•°é‡ | æ—¥ç¯æ¯”å¢é€Ÿ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|-------------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(region_anomalies_daily, 1):
            clean_anomaly = anomaly.replace('[å½“æ—¥é€€è®¢å¼‚å¸¸]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            region_match = re.search(r'ä¸­(.+?)åœ°åŒºå½“æ—¥é€€è®¢æ—¥ç¯æ¯”å¼‚å¸¸', clean_anomaly)
            daily_match = re.search(r'å½“æ—¥(\d+)å•', clean_anomaly)
            previous_match = re.search(r'å‰æ—¥(\d+)å•', clean_anomaly)
            change_match = re.search(r'å¢é€Ÿ([+-]?[\d.]+%)', clean_anomaly)
            
            # å¤„ç†æ–°å¢åœ°åŒºçš„æƒ…å†µ
            if not region_match:
                region_match = re.search(r'ä¸­(.+?)åœ°åŒºå½“æ—¥æ–°å¢é€€è®¢', clean_anomaly)
            
            region_name = region_match.group(1) if region_match else ""
            daily_count = daily_match.group(1) if daily_match else ""
            previous_count = previous_match.group(1) if previous_match else "0"
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            elif "æ–°å¢é€€è®¢" in clean_anomaly:
                anomaly_type = "ğŸ†• æ–°å¢"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {region_name} | {daily_count} | {previous_count} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… å½“æ—¥é€€è®¢åœ°åŒºæ­£å¸¸:** å½“æ—¥é€€è®¢åœ°åŒºæ—¥ç¯æ¯”å¢é€Ÿå‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # æ¸ é“ç»“æ„å¼‚å¸¸
    report_content += "\n### ğŸ›’ æ¸ é“ç»“æ„å¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if channel_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°æ¸ é“ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æ¸ é“åç§° | CM2å æ¯” | å†å²å¹³å‡å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|---------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(channel_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            channel_match = re.search(r'æ¸ é“(.+?)é”€é‡å æ¯”', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            hist_match = re.search(r'å†å²å¹³å‡ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            channel_name = channel_match.group(1) if channel_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            hist_ratio = hist_match.group(1) if hist_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {channel_name} | {cm2_ratio} | {hist_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… æ¸ é“ç»“æ„æ­£å¸¸:** æ‰€æœ‰æ¸ é“é”€é‡å æ¯”å˜åŒ–å‡åœ¨15%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if channel_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°æ¸ é“ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æ¸ é“åç§° | CM2å æ¯” | CM1å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|---------|---------|----------|----------|\n"
        
        for i, anomaly in enumerate(channel_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            channel_match = re.search(r'æ¸ é“(.+?)é”€é‡å æ¯”', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            cm1_match = re.search(r'CM1ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            channel_name = channel_match.group(1) if channel_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            cm1_ratio = cm1_match.group(1) if cm1_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {channel_name} | {cm2_ratio} | {cm1_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… æ¸ é“ç»“æ„æ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰æ¸ é“é”€é‡å æ¯”å˜åŒ–å‡åœ¨15%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM2é€€è®¢å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2é€€è®¢ vs CM2æ•´ä½“å¯¹æ¯”\n\n"
    if channel_anomalies_refund:
        report_content += "**ğŸš¨ å‘ç°æ¸ é“ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æ¸ é“åç§° | CM2é€€è®¢å æ¯” | CM2æ•´ä½“å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|-------------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(channel_anomalies_refund, 1):
            clean_anomaly = anomaly.replace('[é€€è®¢å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            channel_match = re.search(r'æ¸ é“(.+?)é€€è®¢å æ¯”å¼‚å¸¸', clean_anomaly)
            refund_match = re.search(r'é€€è®¢ä¸º([\d.]+%)', clean_anomaly)
            overall_match = re.search(r'æ•´ä½“ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            channel_name = channel_match.group(1) if channel_match else ""
            refund_ratio = refund_match.group(1) if refund_match else ""
            overall_ratio = overall_match.group(1) if overall_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {channel_name} | {refund_ratio} | {overall_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… æ¸ é“ç»“æ„æ­£å¸¸:** CM2é€€è®¢ç”¨æˆ·æ¸ é“åˆ†å¸ƒä¸æ•´ä½“åˆ†å¸ƒå˜åŒ–å‡åœ¨15%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM2å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æ
    report_content += "\n#### ğŸ“… CM2å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æ\n\n"
    if channel_anomalies_daily:
        report_content += "**ğŸš¨ å‘ç°å½“æ—¥é€€è®¢æ¸ é“å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æ¸ é“åç§° | å½“æ—¥é€€è®¢æ•°é‡ | å‰æ—¥é€€è®¢æ•°é‡ | æ—¥ç¯æ¯”å¢é€Ÿ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|-------------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(channel_anomalies_daily, 1):
            clean_anomaly = anomaly.replace('[å½“æ—¥é€€è®¢å¼‚å¸¸]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            channel_match = re.search(r'æ¸ é“(.+?)å½“æ—¥é€€è®¢æ—¥ç¯æ¯”å¼‚å¸¸', clean_anomaly)
            daily_match = re.search(r'å½“æ—¥(\d+)å•', clean_anomaly)
            previous_match = re.search(r'å‰æ—¥(\d+)å•', clean_anomaly)
            change_match = re.search(r'å¢é€Ÿ([+-]?[\d.]+%)', clean_anomaly)
            
            # å¤„ç†æ–°å¢æ¸ é“çš„æƒ…å†µ
            if not channel_match:
                channel_match = re.search(r'æ¸ é“(.+?)å½“æ—¥æ–°å¢é€€è®¢', clean_anomaly)
            
            channel_name = channel_match.group(1) if channel_match else ""
            daily_count = daily_match.group(1) if daily_match else ""
            previous_count = previous_match.group(1) if previous_match else "0"
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            elif "æ–°å¢é€€è®¢" in clean_anomaly:
                anomaly_type = "ğŸ†• æ–°å¢"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {channel_name} | {daily_count} | {previous_count} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… å½“æ—¥é€€è®¢æ¸ é“æ­£å¸¸:** å½“æ—¥é€€è®¢æ¸ é“æ—¥ç¯æ¯”å¢é€Ÿå‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # äººç¾¤ç»“æ„å¼‚å¸¸
    report_content += "\n### ğŸ‘¥ äººç¾¤ç»“æ„å¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if demographic_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°äººç¾¤ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | äººç¾¤ç±»å‹ | äººç¾¤åç§° | CM2å æ¯” | å†å²å¹³å‡å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|---------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(demographic_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            demo_match = re.search(r'(æ€§åˆ«|å¹´é¾„æ®µ)(.+?)æ¯”ä¾‹å¼‚å¸¸', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            hist_match = re.search(r'å†å²å¹³å‡ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            demo_type = demo_match.group(1) if demo_match else ""
            demo_name = demo_match.group(2) if demo_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            hist_ratio = hist_match.group(1) if hist_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {demo_type} | {demo_name} | {cm2_ratio} | {hist_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… äººç¾¤ç»“æ„æ­£å¸¸:** æ‰€æœ‰æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–å‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if demographic_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°äººç¾¤ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | äººç¾¤ç±»å‹ | äººç¾¤åç§° | CM2å æ¯” | CM1å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|---------|---------|----------|----------|\n"
        
        for i, anomaly in enumerate(demographic_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            demo_match = re.search(r'(æ€§åˆ«|å¹´é¾„æ®µ)(.+?)æ¯”ä¾‹å¼‚å¸¸', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            cm1_match = re.search(r'CM1ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            demo_type = demo_match.group(1) if demo_match else ""
            demo_name = demo_match.group(2) if demo_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            cm1_ratio = cm1_match.group(1) if cm1_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {demo_type} | {demo_name} | {cm2_ratio} | {cm1_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… äººç¾¤ç»“æ„æ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–å‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM2é€€è®¢å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2é€€è®¢ vs CM2æ•´ä½“å¯¹æ¯”\n\n"
    if demographic_anomalies_refund:
        report_content += "**ğŸš¨ å‘ç°äººç¾¤ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | äººç¾¤ç±»å‹ | äººç¾¤åç§° | CM2é€€è®¢å æ¯” | CM2æ•´ä½“å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|-------------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(demographic_anomalies_refund, 1):
            clean_anomaly = anomaly.replace('[é€€è®¢å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            demo_match = re.search(r'(æ€§åˆ«|å¹´é¾„æ®µ)(.+?)é€€è®¢æ¯”ä¾‹å¼‚å¸¸', clean_anomaly)
            refund_match = re.search(r'é€€è®¢ä¸º([\d.]+%)', clean_anomaly)
            overall_match = re.search(r'æ•´ä½“ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            demo_type = demo_match.group(1) if demo_match else ""
            demo_name = demo_match.group(2) if demo_match else ""
            refund_ratio = refund_match.group(1) if refund_match else ""
            overall_ratio = overall_match.group(1) if overall_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {demo_type} | {demo_name} | {refund_ratio} | {overall_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… äººç¾¤ç»“æ„æ­£å¸¸:** CM2é€€è®¢ç”¨æˆ·äººç¾¤ç»“æ„ä¸æ•´ä½“ç»“æ„å˜åŒ–å‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM2å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æ
    report_content += "\n#### ğŸ“… CM2å½“æ—¥é€€è®¢å¼‚å¸¸åˆ†æ\n\n"
    if demographic_anomalies_daily:
        report_content += "**ğŸš¨ å‘ç°å½“æ—¥é€€è®¢äººç¾¤å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | äººç¾¤ç±»å‹ | äººç¾¤åç§° | å½“æ—¥é€€è®¢æ•°é‡ | å‰æ—¥é€€è®¢æ•°é‡ | æ—¥ç¯æ¯”å¢é€Ÿ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|-------------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(demographic_anomalies_daily, 1):
            clean_anomaly = anomaly.replace('[å½“æ—¥é€€è®¢å¼‚å¸¸]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            demo_match = re.search(r'(æ€§åˆ«|å¹´é¾„æ®µ)(.+?)å½“æ—¥é€€è®¢æ—¥ç¯æ¯”å¼‚å¸¸', clean_anomaly)
            daily_match = re.search(r'å½“æ—¥(\d+)å•', clean_anomaly)
            previous_match = re.search(r'å‰æ—¥(\d+)å•', clean_anomaly)
            change_match = re.search(r'å¢é€Ÿ([+-]?[\d.]+%)', clean_anomaly)
            
            # å¤„ç†æ–°å¢äººç¾¤çš„æƒ…å†µ
            if not demo_match:
                demo_match = re.search(r'(æ€§åˆ«|å¹´é¾„æ®µ)(.+?)å½“æ—¥æ–°å¢é€€è®¢', clean_anomaly)
            
            demo_type = demo_match.group(1) if demo_match else ""
            demo_name = demo_match.group(2) if demo_match else ""
            daily_count = daily_match.group(1) if daily_match else ""
            previous_count = previous_match.group(1) if previous_match else "0"
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            elif "æ–°å¢é€€è®¢" in clean_anomaly:
                anomaly_type = "ğŸ†• æ–°å¢"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {demo_type} | {demo_name} | {daily_count} | {previous_count} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… å½“æ—¥é€€è®¢äººç¾¤æ­£å¸¸:** å½“æ—¥é€€è®¢äººç¾¤æ—¥ç¯æ¯”å¢é€Ÿå‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # åŒæ¯”/ç¯æ¯”å¼‚å¸¸
    report_content += "\n### ğŸ“ˆ åŒæ¯”/ç¯æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    
    # ç”Ÿæˆæ—¥ç¯æ¯”æè¿°æ•°æ®
    time_series_desc = generate_time_series_description(vehicle_data, state.get('presale_periods', {}))
    
    # ç”Ÿæˆå‘å¸ƒä¼šåç¬¬Næ—¥é”å•æ•°æ®å¯¹æ¯”åˆ†æ
    lock_analysis = generate_post_launch_lock_analysis(vehicle_data, state.get('presale_periods', {}))
    
    # å°†æ—¶é—´åºåˆ—æ•°æ®å­˜å‚¨åˆ°stateä¸­ï¼Œä¾›é€€è®¢ç‡æ—¥ç¯æ¯”å¼‚å¸¸æ£€æµ‹ä½¿ç”¨
    if 'time_series_data' not in state:
        state['time_series_data'] = {}
    
    # å‡†å¤‡CM2æ¯æ—¥æ•°æ®ç”¨äºé€€è®¢ç‡è®¡ç®—
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is not None and len(cm2_data) > 0:
        presale_periods = state.get('presale_periods', {})
        if presale_periods and 'CM2' in presale_periods:
            cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
            
            # å‡†å¤‡CM2æ¯æ—¥è®¢å•æ•°æ®
            cm2_data_copy = cm2_data.copy()
            cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
            cm2_daily = cm2_data_copy.groupby('date').size().reset_index(name='orders')
            cm2_daily['date'] = pd.to_datetime(cm2_daily['date'])
            cm2_daily = cm2_daily.sort_values('date')
            cm2_daily['cumulative_orders'] = cm2_daily['orders'].cumsum()
            
            # å‡†å¤‡CM2é€€è®¢æ•°æ®
            cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
            if len(cm2_refund_data) > 0:
                cm2_refund_data['refund_date'] = cm2_refund_data['intention_refund_time'].dt.date
                cm2_daily_refund = cm2_refund_data.groupby('refund_date').size().reset_index(name='refunds')
                cm2_daily_refund['refund_date'] = pd.to_datetime(cm2_daily_refund['refund_date'])
                cm2_daily_refund = cm2_daily_refund.sort_values('refund_date')
                cm2_daily_refund['cumulative_refunds'] = cm2_daily_refund['refunds'].cumsum()
                
                # åˆå¹¶è®¢å•å’Œé€€è®¢æ•°æ®
                cm2_daily = cm2_daily.merge(cm2_daily_refund[['refund_date', 'cumulative_refunds']], 
                                          left_on='date', right_on='refund_date', how='left')
                cm2_daily['cumulative_refunds'] = cm2_daily['cumulative_refunds'].fillna(method='ffill').fillna(0)
            else:
                cm2_daily['cumulative_refunds'] = 0
            
            state['time_series_data']['cm2_daily_data'] = cm2_daily
    
    # ç”Ÿæˆé€€è®¢æè¿°æ•°æ®
    refund_desc = generate_refund_description(vehicle_data, state.get('presale_periods', {}))
    
    # æ—¥ç¯æ¯”æè¿°
    report_content += "#### ğŸ“Š æ—¥ç¯æ¯”æè¿°\n\n"
    
    # CM2å½“æ—¥æ•°æ®
    if time_series_desc['cm2_daily'] and time_series_desc['cm2_cumulative']:
        daily_data = time_series_desc['cm2_daily'][-1]  # æœ€æ–°ä¸€å¤©
        cumulative_data = time_series_desc['cm2_cumulative'][-1]  # æœ€æ–°ä¸€å¤©ç´¯è®¡
        report_content += f"**CM2è½¦å‹å½“æ—¥æ•°æ® (ç¬¬{daily_data['day_n']}æ—¥, {daily_data['date']}):**\n\n"
        report_content += f"- è®¢å•æ•°: {daily_data['orders']}å•\n"
        
        # è·å–å½“æ—¥é€€è®¢æ•°ï¼ˆç¡®ä¿ä¸è®¢å•æ•°æ®æ—¥æœŸä¸€è‡´ï¼‰
        current_date = daily_data['date']
        current_refunds = 0
        
        # ä»CM2é€€è®¢æ•°æ®ä¸­æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„é€€è®¢æ•°
        cm2_data = vehicle_data.get('CM2')
        if cm2_data is not None:
            cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
            if len(cm2_refund_data) > 0:
                cm2_refund_data['refund_date'] = cm2_refund_data['intention_refund_time'].dt.date
                refunds_on_date = cm2_refund_data[cm2_refund_data['refund_date'] == pd.to_datetime(current_date).date()]
                if len(refunds_on_date) > 0:
                    current_refunds = len(refunds_on_date)
        
        report_content += f"- é€€è®¢æ•°: {current_refunds}å•\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹å½“æ—¥æ•°æ®å¯¹æ¯”
    report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹å½“æ—¥æ•°æ®å¯¹æ¯”:**\n\n"
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if (vehicle in time_series_desc['comparison_daily'] and 
            time_series_desc['comparison_daily'][vehicle] and
            vehicle in time_series_desc['comparison_cumulative'] and 
            time_series_desc['comparison_cumulative'][vehicle]):
            
            daily_data = time_series_desc['comparison_daily'][vehicle][-1]
            
            # è·å–é€€è®¢æ•°ï¼ˆç¡®ä¿ä¸è®¢å•æ•°æ®æ—¥æœŸä¸€è‡´ï¼‰
            refund_count = 0
            vehicle_date = daily_data['vehicle_date']
            
            # ä»å¯¹åº”è½¦å‹çš„é€€è®¢æ•°æ®ä¸­æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„é€€è®¢æ•°
            vehicle_data_for_refund = vehicle_data.get(vehicle)
            if vehicle_data_for_refund is not None:
                vehicle_refund_data = vehicle_data_for_refund[vehicle_data_for_refund['intention_refund_time'].notna()].copy()
                if len(vehicle_refund_data) > 0:
                    vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
                    refunds_on_date = vehicle_refund_data[vehicle_refund_data['refund_date'] == pd.to_datetime(vehicle_date).date()]
                    if len(refunds_on_date) > 0:
                        refund_count = len(refunds_on_date)
            
            report_content += f"- **{vehicle}è½¦å‹** (ç¬¬{daily_data['day_n']}æ—¥, {daily_data['vehicle_date']}): è®¢å•{daily_data['orders']}å•, é€€è®¢{refund_count}å•\n"
    
    # CM2ç´¯è®¡æ•°æ® - ä½¿ç”¨ä¸é€€è®¢ç‡æ—¥ç¯æ¯”æ£€æµ‹ç›¸åŒçš„æ•°æ®æºç¡®ä¿ä¸€è‡´æ€§
    time_series_data = state.get('time_series_data', {})
    if time_series_data and 'cm2_daily_data' in time_series_data:
        cm2_daily_data = time_series_data['cm2_daily_data']
        if len(cm2_daily_data) > 0:
            # è·å–æœ€åä¸€å¤©çš„æ•°æ®
            last_day_data = cm2_daily_data.iloc[-1]
            cumulative_orders = int(last_day_data['cumulative_orders'])
            cumulative_refunds = int(last_day_data['cumulative_refunds'])
            
            # è®¡ç®—ä»é¢„å”®å¼€å§‹çš„å¤©æ•°
            presale_periods = state.get('presale_periods', {})
            if presale_periods and 'CM2' in presale_periods:
                cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
                last_date = pd.to_datetime(last_day_data['date'])
                day_n = (last_date - cm2_start).days + 1
                
                refund_rate = 0
                if cumulative_orders > 0:
                    refund_rate = (cumulative_refunds / cumulative_orders) * 100
                
                report_content += f"\n**CM2è½¦å‹ç¬¬{day_n}æ—¥ç´¯è®¡æ•°æ®:**\n\n"
                report_content += f"- ç´¯è®¡è®¢å•æ•°: {cumulative_orders}å•\n"
                report_content += f"- ç´¯è®¡é€€è®¢æ•°: {cumulative_refunds}å•\n"
                report_content += f"- é€€è®¢ç‡: {refund_rate:.2f}%\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹ç´¯è®¡æ•°æ®å¯¹æ¯”
    report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹ç¬¬Næ—¥ç´¯è®¡æ•°æ®å¯¹æ¯”:**\n\n"
    
    # è·å–CM2çš„å½“å‰å¤©æ•° - ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®æº
    cm2_current_day = None
    time_series_data = state.get('time_series_data', {})
    if time_series_data and 'cm2_daily_data' in time_series_data:
        cm2_daily_data = time_series_data['cm2_daily_data']
        if len(cm2_daily_data) > 0:
            presale_periods = state.get('presale_periods', {})
            if presale_periods and 'CM2' in presale_periods:
                cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
                last_date = pd.to_datetime(cm2_daily_data.iloc[-1]['date'])
                cm2_current_day = (last_date - cm2_start).days + 1
    
    # ä½¿ç”¨ä¸é€€è®¢ç‡è¡¨æ ¼ç›¸åŒçš„æ•°æ®æºæ¥ç¡®ä¿ä¸€è‡´æ€§
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
            
        presale_periods = state.get('presale_periods', {})
        if vehicle not in presale_periods or cm2_current_day is None:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        
        # å‡†å¤‡è½¦å‹è®¢å•æ•°æ®
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        vehicle_daily['cumulative_orders'] = vehicle_daily['orders'].cumsum()
        
        # å‡†å¤‡è½¦å‹é€€è®¢æ•°æ®
        vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
        if len(vehicle_refund_data) > 0:
            vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
            vehicle_daily_refund = vehicle_refund_data.groupby('refund_date').size().reset_index(name='refunds')
            vehicle_daily_refund['refund_date'] = pd.to_datetime(vehicle_daily_refund['refund_date'])
            vehicle_daily_refund = vehicle_daily_refund.sort_values('refund_date')
            vehicle_daily_refund['cumulative_refunds'] = vehicle_daily_refund['refunds'].cumsum()
            
            # åˆå¹¶è®¢å•å’Œé€€è®¢æ•°æ®
            vehicle_daily = vehicle_daily.merge(vehicle_daily_refund[['refund_date', 'cumulative_refunds']], 
                                              left_on='date', right_on='refund_date', how='left')
            vehicle_daily['cumulative_refunds'] = vehicle_daily['cumulative_refunds'].ffill().fillna(0)
        else:
            vehicle_daily['cumulative_refunds'] = 0
        
        # è®¡ç®—ä»é¢„å”®å¼€å§‹çš„å¤©æ•°
        vehicle_daily['days_from_start'] = (vehicle_daily['date'] - vehicle_start).dt.days
        
        # æ‰¾åˆ°å¯¹åº”CM2å½“å‰å¤©æ•°çš„æ•°æ®
        target_day = cm2_current_day - 1  # CM2çš„ç¬¬Næ—¥å¯¹åº”å…¶ä»–è½¦å‹çš„ç¬¬N-1æ—¥
        vehicle_target_data = vehicle_daily[vehicle_daily['days_from_start'] == target_day]
        
        if not vehicle_target_data.empty:
            target_row = vehicle_target_data.iloc[-1]  # å–æœ€åä¸€è¡Œæ•°æ®
            cumulative_orders = int(target_row['cumulative_orders'])
            cumulative_refunds = int(target_row['cumulative_refunds'])
            
            if cumulative_orders > 0:
                refund_rate = (cumulative_refunds / cumulative_orders) * 100
                report_content += f"- **{vehicle}è½¦å‹** (ç¬¬{target_day + 1}æ—¥): ç´¯è®¡è®¢å•{cumulative_orders}å•, ç´¯è®¡é€€è®¢{cumulative_refunds}å•, é€€è®¢ç‡{refund_rate:.2f}%\n"
    
    # å‰ä¸€å¤©æ•°æ®æè¿°
    report_content += "\n#### ğŸ“… å‰ä¸€å¤©æ•°æ®åˆ†æ\n\n"
    
    # ç”Ÿæˆå‰ä¸€å¤©æ•°æ®
    previous_day_data = generate_previous_day_data(vehicle_data, state.get('presale_periods', {}))
    
    # CM2å‰ä¸€å¤©è®¢å•æ•°å’Œé€€è®¢æ•°
    if previous_day_data['cm2_previous_day']:
        cm2_prev = previous_day_data['cm2_previous_day']
        report_content += f"**CM2è½¦å‹å‰ä¸€å¤©æ•°æ® (ç¬¬{cm2_prev['day_n']}æ—¥, {cm2_prev['date']}):**\n\n"
        report_content += f"- è®¢å•æ•°: {cm2_prev['orders']}å•\n"
        report_content += f"- é€€è®¢æ•°: {cm2_prev['refunds']}å•\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹å‰ä¸€å¤©æ•°æ®å¯¹æ¯”
    if previous_day_data['comparison_previous_day']:
        report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹å‰ä¸€å¤©æ•°æ®å¯¹æ¯”:**\n\n"
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in previous_day_data['comparison_previous_day']:
                data = previous_day_data['comparison_previous_day'][vehicle]
                report_content += f"- **{vehicle}è½¦å‹** (ç¬¬{data['day_n']}æ—¥, {data['vehicle_date']}): è®¢å•{data['orders']}å•, é€€è®¢{data['refunds']}å•\n"
    
    # CM2å‰N-1æ—¥ç´¯è®¡æ•°æ®
    if previous_day_data['cm2_cumulative_previous']:
        cm2_cum_prev = previous_day_data['cm2_cumulative_previous']
        report_content += f"\n**CM2è½¦å‹å‰{cm2_cum_prev['day_n']}æ—¥ç´¯è®¡æ•°æ®:**\n\n"
        report_content += f"- ç´¯è®¡è®¢å•æ•°: {cm2_cum_prev['cumulative_orders']}å•\n"
        report_content += f"- ç´¯è®¡é€€è®¢æ•°: {cm2_cum_prev['cumulative_refunds']}å•\n"
        
        # è®¡ç®—å‰N-1æ—¥é€€è®¢ç‡
        if cm2_cum_prev['cumulative_orders'] > 0:
            previous_refund_rate = (cm2_cum_prev['cumulative_refunds'] / cm2_cum_prev['cumulative_orders']) * 100
            report_content += f"- é€€è®¢ç‡: {previous_refund_rate:.2f}%\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹å‰N-1æ—¥ç´¯è®¡æ•°æ®å¯¹æ¯”
    if previous_day_data['comparison_cumulative_previous']:
        report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹å‰N-1æ—¥ç´¯è®¡æ•°æ®å¯¹æ¯”:**\n\n"
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in previous_day_data['comparison_cumulative_previous']:
                data = previous_day_data['comparison_cumulative_previous'][vehicle]
                vehicle_refund_rate = 0
                if data['cumulative_orders'] > 0:
                    vehicle_refund_rate = (data['cumulative_refunds'] / data['cumulative_orders']) * 100
                report_content += f"- **{vehicle}è½¦å‹** (ç¬¬{data['day_n']}æ—¥): ç´¯è®¡è®¢å•{data['cumulative_orders']}å•, ç´¯è®¡é€€è®¢{data['cumulative_refunds']}å•, é€€è®¢ç‡{vehicle_refund_rate:.2f}%\n"

    # æ—¥ç¯æ¯”å¼‚å¸¸
    report_content += "\n#### ğŸ“… æ—¥ç¯æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    if time_series_anomalies_daily:
        report_content += "**ğŸš¨ å‘ç°æ—¥ç¯æ¯”å¼‚å¸¸:**\n\n"
        for i, anomaly in enumerate(time_series_anomalies_daily, 1):
            clean_anomaly = anomaly.replace('[æ—¥ç¯æ¯”]', '')
            report_content += f"{i}. {clean_anomaly}\n"
    else:
        report_content += "**âœ… æ—¥ç¯æ¯”æ­£å¸¸:** CM2è½¦å‹æ—¥è®¢å•é‡å˜åŒ–å‡åœ¨50%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # é€€è®¢ç‡æ—¥ç¯æ¯”å¼‚å¸¸æ£€æµ‹
    report_content += "\n#### ğŸ“Š é€€è®¢ç‡æ—¥ç¯æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    
    # è·å–å½“å‰æ—¥æœŸ(ç¬¬Næ—¥)çš„é€€è®¢ç‡æ•°æ®
    time_series_data = state.get('time_series_data', {})
    if time_series_data and 'cm2_daily_data' in time_series_data:
        cm2_daily_data = time_series_data['cm2_daily_data']
        if len(cm2_daily_data) > 0:
            # è·å–æœ€åä¸€å¤©çš„é€€è®¢ç‡
            last_day_data = cm2_daily_data.iloc[-1]
            current_refund_rate = 0
            if last_day_data['cumulative_orders'] > 0:
                current_refund_rate = (last_day_data['cumulative_refunds'] / last_day_data['cumulative_orders']) * 100
            
            # è·å–å‰ä¸€å¤©çš„é€€è®¢ç‡
            previous_refund_rate = 0
            if previous_day_data['cm2_cumulative_previous'] and previous_day_data['cm2_cumulative_previous']['cumulative_orders'] > 0:
                previous_refund_rate = (previous_day_data['cm2_cumulative_previous']['cumulative_refunds'] / previous_day_data['cm2_cumulative_previous']['cumulative_orders']) * 100
            
            # è®¡ç®—CM2é€€è®¢ç‡å˜åŒ–å¹…åº¦
            cm2_refund_rate_change = 0
            if previous_refund_rate > 0:
                cm2_refund_rate_change = ((current_refund_rate - previous_refund_rate) / previous_refund_rate) * 100
            
            # è®¡ç®—å†å²è½¦å‹çš„é€€è®¢ç‡æ—¥ç¯æ¯”å˜åŒ–å¹…åº¦ä½œä¸ºåŸºå‡†
            historical_changes = []
            for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
                if vehicle in previous_day_data['comparison_cumulative_previous']:
                    # è·å–å†å²è½¦å‹å½“å‰æ—¥æœŸçš„é€€è®¢ç‡ï¼ˆæ¨¡æ‹Ÿç¬¬Næ—¥ï¼‰
                    vehicle_current_data = previous_day_data['comparison_cumulative_previous'][vehicle]
                    vehicle_current_refund_rate = 0
                    if vehicle_current_data['cumulative_orders'] > 0:
                        vehicle_current_refund_rate = (vehicle_current_data['cumulative_refunds'] / vehicle_current_data['cumulative_orders']) * 100
                    
                    # è·å–å†å²è½¦å‹å‰ä¸€å¤©çš„é€€è®¢ç‡ï¼ˆæ¨¡æ‹Ÿç¬¬N-1æ—¥ï¼‰
                    # è¿™é‡Œéœ€è¦è®¡ç®—ç¬¬N-1æ—¥çš„ç´¯è®¡æ•°æ®
                    if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                        vehicle_start = pd.to_datetime(state.get('presale_periods', {}).get(vehicle, {}).get('start'))
                        if vehicle_start is not None:
                            target_date_n_minus_1 = vehicle_start + pd.Timedelta(days=previous_day_data['cm2_cumulative_previous']['day_n']-2)
                            
                            # è®¡ç®—ç¬¬N-1æ—¥çš„ç´¯è®¡è®¢å•å’Œé€€è®¢
                            vehicle_data_copy = vehicle_data[vehicle].copy()
                            vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
                            vehicle_orders_n_minus_1 = vehicle_data_copy[pd.to_datetime(vehicle_data_copy['date']) <= target_date_n_minus_1].shape[0]
                            
                            vehicle_refunds_n_minus_1 = 0
                            vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
                            if len(vehicle_refund_data) > 0:
                                vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
                                vehicle_refunds_n_minus_1 = vehicle_refund_data[pd.to_datetime(vehicle_refund_data['refund_date']) <= target_date_n_minus_1].shape[0]
                            
                            vehicle_previous_refund_rate = 0
                            if vehicle_orders_n_minus_1 > 0:
                                vehicle_previous_refund_rate = (vehicle_refunds_n_minus_1 / vehicle_orders_n_minus_1) * 100
                            
                            # è®¡ç®—å†å²è½¦å‹çš„é€€è®¢ç‡å˜åŒ–å¹…åº¦
                            if vehicle_previous_refund_rate > 0:
                                vehicle_change = ((vehicle_current_refund_rate - vehicle_previous_refund_rate) / vehicle_previous_refund_rate) * 100
                                historical_changes.append(abs(vehicle_change))
            
            # è®¡ç®—å†å²è½¦å‹å˜åŒ–å¹…åº¦çš„å¹³å‡å€¼ä½œä¸ºåŸºå‡†
            if historical_changes:
                avg_historical_change = sum(historical_changes) / len(historical_changes)
                threshold = max(20, avg_historical_change * 1.5)  # é˜ˆå€¼ä¸º20%æˆ–å†å²å¹³å‡å˜åŒ–çš„1.5å€ï¼Œå–è¾ƒå¤§å€¼
                
                # æ£€æµ‹CM2å¼‚å¸¸
                if abs(cm2_refund_rate_change) > threshold:
                    if cm2_refund_rate_change > 0:
                        report_content += f"**ğŸš¨ å‘ç°é€€è®¢ç‡å¼‚å¸¸éª¤å¢:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                        report_content += f"**ğŸ“Š å†å²åŸºå‡†:** å†å²è½¦å‹å¹³å‡å˜åŒ–å¹…åº¦{avg_historical_change:.1f}%ï¼Œå¼‚å¸¸é˜ˆå€¼{threshold:.1f}%\n"
                    else:
                        report_content += f"**ğŸš¨ å‘ç°é€€è®¢ç‡å¼‚å¸¸éª¤é™:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                        report_content += f"**ğŸ“Š å†å²åŸºå‡†:** å†å²è½¦å‹å¹³å‡å˜åŒ–å¹…åº¦{avg_historical_change:.1f}%ï¼Œå¼‚å¸¸é˜ˆå€¼{threshold:.1f}%\n"
                else:
                    report_content += f"**âœ… é€€è®¢ç‡æ—¥ç¯æ¯”æ­£å¸¸:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                    report_content += f"**ğŸ“Š å†å²åŸºå‡†:** å†å²è½¦å‹å¹³å‡å˜åŒ–å¹…åº¦{avg_historical_change:.1f}%ï¼Œå¼‚å¸¸é˜ˆå€¼{threshold:.1f}%\n"
            else:
                # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨å›ºå®š20%é˜ˆå€¼
                if abs(cm2_refund_rate_change) > 20:
                    if cm2_refund_rate_change > 0:
                        report_content += f"**ğŸš¨ å‘ç°é€€è®¢ç‡å¼‚å¸¸éª¤å¢:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                    else:
                        report_content += f"**ğŸš¨ å‘ç°é€€è®¢ç‡å¼‚å¸¸éª¤é™:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                else:
                    report_content += f"**âœ… é€€è®¢ç‡æ—¥ç¯æ¯”æ­£å¸¸:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                report_content += f"**ğŸ“Š ä½¿ç”¨å›ºå®šé˜ˆå€¼:** 20%ï¼ˆç¼ºå°‘å†å²è½¦å‹æ•°æ®ï¼‰\n"
        else:
            report_content += "**âš ï¸ æ— æ³•è¿›è¡Œé€€è®¢ç‡æ—¥ç¯æ¯”æ£€æµ‹:** ç¼ºå°‘æ—¶é—´åºåˆ—æ•°æ®\n"
    else:
        report_content += "**âš ï¸ æ— æ³•è¿›è¡Œé€€è®¢ç‡æ—¥ç¯æ¯”æ£€æµ‹:** ç¼ºå°‘æ—¶é—´åºåˆ—æ•°æ®\n"
    
    # æ·»åŠ å„è½¦å‹æ¯æ—¥é€€è®¢ç‡å¯¹æ¯”è¡¨æ ¼
    report_content += "\n#### ğŸ“ˆ å„è½¦å‹æ¯æ—¥é€€è®¢ç‡å¯¹æ¯”è¡¨\n\n"
    
    # æ„å»ºé€€è®¢ç‡å¯¹æ¯”è¡¨æ ¼
    refund_rate_table_data = {}
    max_days = 0
    
    # å¤„ç†æ‰€æœ‰è½¦å‹çš„é€€è®¢ç‡æ•°æ®
    for vehicle in ['CM2', 'CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
            
        presale_periods = state.get('presale_periods', {})
        if vehicle not in presale_periods:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        
        # å‡†å¤‡è½¦å‹è®¢å•æ•°æ®
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        vehicle_daily['cumulative_orders'] = vehicle_daily['orders'].cumsum()
        
        # å‡†å¤‡è½¦å‹é€€è®¢æ•°æ®
        vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
        if len(vehicle_refund_data) > 0:
            vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
            vehicle_daily_refund = vehicle_refund_data.groupby('refund_date').size().reset_index(name='refunds')
            vehicle_daily_refund['refund_date'] = pd.to_datetime(vehicle_daily_refund['refund_date'])
            vehicle_daily_refund = vehicle_daily_refund.sort_values('refund_date')
            vehicle_daily_refund['cumulative_refunds'] = vehicle_daily_refund['refunds'].cumsum()
            
            # åˆå¹¶è®¢å•å’Œé€€è®¢æ•°æ®
            vehicle_daily = vehicle_daily.merge(vehicle_daily_refund[['refund_date', 'cumulative_refunds']], 
                                              left_on='date', right_on='refund_date', how='left')
            vehicle_daily['cumulative_refunds'] = vehicle_daily['cumulative_refunds'].ffill().fillna(0)
        else:
            vehicle_daily['cumulative_refunds'] = 0
        
        # è®¡ç®—æ¯æ—¥é€€è®¢ç‡
        vehicle_daily['refund_rate'] = 0.0
        vehicle_daily.loc[vehicle_daily['cumulative_orders'] > 0, 'refund_rate'] = (
            vehicle_daily['cumulative_refunds'] / vehicle_daily['cumulative_orders'] * 100
        )
        
        # è®¡ç®—ä»é¢„å”®å¼€å§‹çš„å¤©æ•°
        vehicle_daily['days_from_start'] = (vehicle_daily['date'] - vehicle_start).dt.days
        
        # å­˜å‚¨æ•°æ®
        refund_rate_table_data[vehicle] = {}
        for _, row in vehicle_daily.iterrows():
            day_num = row['days_from_start']
            if day_num >= 0:  # åªåŒ…å«é¢„å”®å¼€å§‹åçš„æ•°æ®
                refund_rate_table_data[vehicle][day_num] = row['refund_rate']
                max_days = max(max_days, day_num)
    
    # ç”Ÿæˆè¡¨æ ¼ï¼ˆè½¦å‹ä½œä¸ºåˆ—ï¼Œæ—¥æœŸä½œä¸ºè¡Œï¼‰
    if refund_rate_table_data and max_days > 0:
        # è¡¨å¤´
        header = "| æ—¥æœŸ |"
        separator = "|------|"
        vehicles_with_data = []
        for vehicle in ['CM2', 'CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in refund_rate_table_data:
                vehicles_with_data.append(vehicle)
                header += f" **{vehicle}** |"
                separator += "-------|"
        
        report_content += header + "\n"
        report_content += separator + "\n"
        
        # è¡¨æ ¼å†…å®¹ï¼ˆæŒ‰æ—¥æœŸè¡Œå±•ç¤ºï¼‰
        for day in range(max_days + 1):  # å®Œæ•´å±•ç¤ºæ‰€æœ‰å¤©æ•°
            row = f"| ç¬¬{day}æ—¥ |"
            for vehicle in vehicles_with_data:
                if day in refund_rate_table_data[vehicle]:
                    rate = refund_rate_table_data[vehicle][day]
                    row += f" {rate:.2f}% |"
                else:
                    row += " - |"
            report_content += row + "\n"
        
        report_content += "\n*æ³¨ï¼šè¡¨æ ¼æ˜¾ç¤ºå„è½¦å‹ä»é¢„å”®å¼€å§‹æ¯æ—¥çš„ç´¯è®¡é€€è®¢ç‡ï¼Œ'-' è¡¨ç¤ºè¯¥æ—¥æ— æ•°æ®*\n\n"
        
        # ç”Ÿæˆå½’ä¸€åŒ–æ—¶é—´å¯¹æ¯”è¡¨æ ¼
        report_content += "#### ğŸ“ˆ å„è½¦å‹å½’ä¸€åŒ–é¢„å”®å‘¨æœŸé€€è®¢ç‡å¯¹æ¯”è¡¨\n\n"
        
        # è®¡ç®—æ¯ä¸ªè½¦å‹çš„é¢„å”®å‘¨æœŸé•¿åº¦å’Œå½’ä¸€åŒ–æ•°æ®
        normalized_data = {}
        presale_periods = state.get('presale_periods', {})
        
        for vehicle in vehicles_with_data:
            vehicle_days = list(refund_rate_table_data[vehicle].keys())
            if vehicle_days and vehicle in presale_periods:
                # ä½¿ç”¨é¢„å”®å‘¨æœŸå®šä¹‰è®¡ç®—æ€»å¤©æ•°
                vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
                vehicle_end = pd.to_datetime(presale_periods[vehicle]['end'])
                total_presale_days = (vehicle_end - vehicle_start).days
                
                normalized_data[vehicle] = {}
                
                # ä¸ºæ¯ä¸ªç™¾åˆ†æ¯”ç‚¹è®¡ç®—å¯¹åº”çš„å¤©æ•°å’Œé€€è®¢ç‡
                max_available_day = max(vehicle_days) if vehicle_days else 0
                max_available_pct = int((max_available_day / total_presale_days) * 100) if total_presale_days > 0 else 0
                
                for pct in range(0, 101, 10):  # 0%, 10%, 20%, ..., 100%
                    target_day = int(total_presale_days * pct / 100)
                    
                    # å¦‚æœç›®æ ‡ç™¾åˆ†æ¯”è¶…å‡ºå½“å‰å¯ç”¨æ•°æ®èŒƒå›´ï¼Œè·³è¿‡
                    if pct > max_available_pct and vehicle == 'CM2':
                        continue  # ä¸æ·»åŠ æ•°æ®ï¼Œåç»­ä¼šæ˜¾ç¤ºä¸º "-"
                    
                    # æ‰¾åˆ°æœ€æ¥è¿‘çš„æœ‰æ•°æ®çš„å¤©æ•°
                    closest_day = None
                    min_diff = float('inf')
                    for day in vehicle_days:
                        if day <= target_day:  # åªè€ƒè™‘ä¸è¶…è¿‡ç›®æ ‡å¤©æ•°çš„æ•°æ®
                            diff = abs(day - target_day)
                            if diff < min_diff:
                                min_diff = diff
                                closest_day = day
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸è¶…è¿‡ç›®æ ‡å¤©æ•°çš„æ•°æ®ï¼Œå–æœ€å°çš„å¤©æ•°
                    if closest_day is None and vehicle_days:
                        closest_day = min([d for d in vehicle_days if d >= 0])
                    
                    if closest_day is not None:
                        normalized_data[vehicle][pct] = refund_rate_table_data[vehicle][closest_day]
        
        # ç”Ÿæˆå½’ä¸€åŒ–è¡¨æ ¼
        if normalized_data:
            # è¡¨å¤´
            norm_header = "| é¢„å”®è¿›åº¦ |"
            norm_separator = "|-------|"
            for vehicle in vehicles_with_data:
                if vehicle in normalized_data:
                    norm_header += f" **{vehicle}** |"
                    norm_separator += "-------|"
            
            report_content += norm_header + "\n"
            report_content += norm_separator + "\n"
            
            # è¡¨æ ¼å†…å®¹
            for pct in range(0, 101, 10):
                row = f"| {pct}% |"
                for vehicle in vehicles_with_data:
                    if vehicle in normalized_data and pct in normalized_data[vehicle]:
                        rate = normalized_data[vehicle][pct]
                        row += f" {rate:.2f}% |"
                    else:
                        row += " - |"
                report_content += row + "\n"
            
            report_content += "\n*æ³¨ï¼šè¡¨æ ¼æ˜¾ç¤ºå„è½¦å‹åœ¨é¢„å”®å‘¨æœŸä¸åŒè¿›åº¦ä¸‹çš„ç´¯è®¡é€€è®¢ç‡ï¼Œä¾¿äºè·¨è½¦å‹å¯¹æ¯”*\n"
        else:
            report_content += "**âš ï¸ æ— æ³•ç”Ÿæˆå½’ä¸€åŒ–å¯¹æ¯”è¡¨:** ç¼ºå°‘å½’ä¸€åŒ–æ•°æ®\n"
    else:
        report_content += "**âš ï¸ æ— æ³•ç”Ÿæˆé€€è®¢ç‡å¯¹æ¯”è¡¨:** ç¼ºå°‘è½¦å‹æ•°æ®\n"
    
    # æ·»åŠ æ¯æ—¥è®¢å•ç´¯è®¡é€€è®¢æƒ…å†µè¡¨æ ¼
    report_content += "\n#### ğŸ“Š æ¯æ—¥è®¢å•ç´¯è®¡é€€è®¢æƒ…å†µ\n\n"
    report_content += f"ä»¥ä¸‹è¡¨æ ¼å±•ç¤ºå„è½¦å‹ä»é¢„å”®å¼€å§‹æ¯æ—¥çš„è®¢å•æ•°å’Œå½“æ—¥è®¢å•ä¸­çš„é€€è®¢æ•°ï¼ˆç»Ÿä¸€æŒ‰CM2ç¬¬{cm2_current_day if cm2_current_day is not None else 'N'}æ—¥è§‚å¯Ÿæ—¶é—´ç‚¹è®¡ç®—ï¼‰ï¼Œä»¥ä¾¿æ¨ªå‘å¯¹æ¯”å„è½¦å‹åŒå‘¨æœŸæ¯æ—¥è®¢å•çš„é€€è®¢ç‡è¡¨ç°ã€‚\n\n"
    
    # æ„å»ºæ¯æ—¥è®¢å•ç´¯è®¡é€€è®¢æƒ…å†µè¡¨æ ¼æ•°æ®
    order_refund_table_data = {}
    max_order_days = 0
    
    # è·å–CM2å½“å‰çš„è§‚å¯Ÿæ—¶é—´ç‚¹ï¼ˆå¤©æ•°ï¼‰
    cm2_current_day = None
    if 'CM2' in vehicle_data and len(vehicle_data['CM2']) > 0:
        presale_periods = state.get('presale_periods', {})
        if 'CM2' in presale_periods:
            cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
            cm2_data_copy = vehicle_data['CM2'].copy()
            cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
            cm2_latest_date = cm2_data_copy['date'].max()
            cm2_current_day = (pd.to_datetime(cm2_latest_date) - cm2_start).days
    
    # å¤„ç†æ‰€æœ‰è½¦å‹çš„è®¢å•å’Œé€€è®¢æ•°æ®
    for vehicle in ['CM2', 'CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
            
        presale_periods = state.get('presale_periods', {})
        if vehicle not in presale_periods:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        
        # å‡†å¤‡è½¦å‹è®¢å•æ•°æ®
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        
        # å‡†å¤‡è½¦å‹é€€è®¢æ•°æ®
        vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
        
        # è®¡ç®—ä»é¢„å”®å¼€å§‹çš„å¤©æ•°
        vehicle_daily['days_from_start'] = (vehicle_daily['date'] - vehicle_start).dt.days
        
        # å­˜å‚¨æ¯æ—¥è®¢å•æ•°æ®
        order_refund_table_data[vehicle] = {}
        
        # ç¡®å®šè§‚å¯Ÿæˆªæ­¢æ—¶é—´ç‚¹
        observation_day = cm2_current_day if cm2_current_day is not None else vehicle_daily['days_from_start'].max()
        observation_cutoff_date = vehicle_start + pd.Timedelta(days=observation_day)
        
        for _, row in vehicle_daily.iterrows():
            day_num = row['days_from_start']
            if day_num >= 0:  # åªåŒ…å«é¢„å”®å¼€å§‹åçš„æ•°æ®
                # è®¡ç®—å½“æ—¥è®¢å•ä¸­æœ‰å¤šå°‘åœ¨è§‚å¯Ÿæˆªæ­¢æ—¶é—´ç‚¹å‰é€€è®¢äº†
                daily_order_refunds = 0
                if len(vehicle_refund_data) > 0:
                    # è·å–å½“æ—¥ä¸‹å•çš„ç”¨æˆ·ID
                    current_date = row['date'].date()
                    daily_orders = vehicle_data_copy[vehicle_data_copy['date'] == current_date]
                    
                    if len(daily_orders) > 0:
                        # è®¡ç®—è¿™äº›å½“æ—¥è®¢å•ä¸­æœ‰å¤šå°‘åœ¨è§‚å¯Ÿæˆªæ­¢æ—¶é—´ç‚¹å‰é€€è®¢äº†
                        daily_order_ids = set(daily_orders.index)
                        # åªè€ƒè™‘åœ¨è§‚å¯Ÿæˆªæ­¢æ—¶é—´ç‚¹å‰çš„é€€è®¢
                        refunded_before_cutoff = vehicle_refund_data[
                            vehicle_refund_data['intention_refund_time'] <= observation_cutoff_date
                        ]
                        refunded_order_ids = set(refunded_before_cutoff.index)
                        daily_order_refunds = len(daily_order_ids.intersection(refunded_order_ids))
                
                # è®¡ç®—é€€è®¢ç‡
                refund_rate = (daily_order_refunds / row['orders'] * 100) if row['orders'] > 0 else 0
                
                order_refund_table_data[vehicle][day_num] = {
                    'daily_orders': row['orders'],
                    'daily_order_refunds': daily_order_refunds,
                    'refund_rate': refund_rate,
                    'refund_situation': f"{row['orders']}è®¢å•/{daily_order_refunds}é€€è®¢({refund_rate:.1f}%)"
                }
                max_order_days = max(max_order_days, day_num)
    
    # ç”Ÿæˆæ¯æ—¥è®¢å•ç´¯è®¡é€€è®¢æƒ…å†µè¡¨æ ¼
    if order_refund_table_data and max_order_days > 0:
        # è¡¨å¤´
        order_header = "| æ—¥æœŸ |"
        order_separator = "|------|"
        vehicles_with_order_data = []
        for vehicle in ['CM2', 'CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in order_refund_table_data:
                vehicles_with_order_data.append(vehicle)
                order_header += f" **{vehicle}** |"
                order_separator += "-------|"
        
        report_content += order_header + "\n"
        report_content += order_separator + "\n"
        
        # è¡¨æ ¼å†…å®¹ï¼ˆæŒ‰æ—¥æœŸè¡Œå±•ç¤ºï¼‰
        for day in range(max_order_days + 1):  # å®Œæ•´å±•ç¤ºæ‰€æœ‰å¤©æ•°
            row = f"| ç¬¬{day}æ—¥ |"
            for vehicle in vehicles_with_order_data:
                if day in order_refund_table_data[vehicle]:
                    situation = order_refund_table_data[vehicle][day]['refund_situation']
                    row += f" {situation} |"
                else:
                    row += " - |"
            report_content += row + "\n"
        
        report_content += f"\n*æ³¨ï¼šè¡¨æ ¼æ˜¾ç¤ºå„è½¦å‹ä»é¢„å”®å¼€å§‹æ¯æ—¥çš„è®¢å•æ•°å’Œå½“æ—¥è®¢å•ä¸­çš„é€€è®¢æ•°ï¼Œæ ¼å¼ä¸º'Xè®¢å•/Yé€€è®¢(é€€è®¢ç‡%)'ã€‚æ‰€æœ‰è½¦å‹çš„é€€è®¢æ•°ç»Ÿä¸€æŒ‰CM2ç¬¬{cm2_current_day if cm2_current_day is not None else 'N'}æ—¥è§‚å¯Ÿæ—¶é—´ç‚¹è®¡ç®—ï¼Œä¾¿äºæ¨ªå‘å¯¹æ¯”å„è½¦å‹åŒå‘¨æœŸæ¯æ—¥è®¢å•çš„é€€è®¢ç‡è¡¨ç°*\n\n"
    else:
        report_content += "**âš ï¸ æ— æ³•ç”Ÿæˆæ¯æ—¥è®¢å•ç´¯è®¡é€€è®¢æƒ…å†µè¡¨:** ç¼ºå°‘è½¦å‹æ•°æ®\n\n"
    
    # åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸
    report_content += "\n#### ğŸ”„ åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    if time_series_anomalies_period:
        report_content += "**ğŸš¨ å‘ç°åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æ—¥æœŸ | CM2è®¢å•æ•° | å¯¹æ¯”è½¦å‹ | å¯¹æ¯”è½¦å‹è®¢å•æ•° | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|------|----------|----------|----------------|----------|----------|\n"
        for i, anomaly in enumerate(time_series_anomalies_period, 1):
            clean_anomaly = anomaly.replace('[åŒå‘¨æœŸå¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            parts = clean_anomaly.split('ï¼š')
            if len(parts) >= 2:
                desc_part = parts[0]
                data_part = parts[1]
                
                # æå–æ—¥æœŸ
                date_match = re.search(r'(\d{4}-\d{2}-\d{2})', desc_part)
                date = date_match.group(1) if date_match else ""
                
                # æå–å¯¹æ¯”è½¦å‹
                vehicle_match = re.search(r'ç›¸å¯¹(CM\d|DM\d)åŒæœŸ', desc_part)
                compare_vehicle = vehicle_match.group(1) if vehicle_match else ""
                
                # æå–å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
                if "éª¤å¢" in desc_part:
                    anomaly_type = "ğŸ“ˆ éª¤å¢"
                else:
                    anomaly_type = "ğŸ“‰ éª¤é™"
                
                # æå–æ•°æ®
                cm2_match = re.search(r'CM2ä¸º(\d+)å•', data_part)
                compare_match = re.search(r'{}åŒæœŸä¸º(\d+)å•'.format(compare_vehicle), data_part)
                change_match = re.search(r'å˜åŒ–å¹…åº¦([+-]?\d+\.\d+)%', data_part)
                
                cm2_orders = cm2_match.group(1) if cm2_match else ""
                compare_orders = compare_match.group(1) if compare_match else ""
                change_rate = change_match.group(1) + "%" if change_match else ""
                
                report_content += f"| {i} | {date} | {cm2_orders} | {compare_vehicle} | {compare_orders} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… åŒå‘¨æœŸå¯¹æ¯”æ­£å¸¸:** CM2ç›¸å¯¹äºå†å²è½¦å‹åŒæœŸè®¢å•é‡å˜åŒ–å‡åœ¨100%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸
    report_content += "\n#### ğŸ“Š ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    if time_series_anomalies_cumulative:
        report_content += "**ğŸš¨ å‘ç°ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æˆªè‡³æ—¥æœŸ | CM2ç´¯è®¡è®¢å•æ•° | å¯¹æ¯”è½¦å‹ | å¯¹æ¯”è½¦å‹ç´¯è®¡è®¢å•æ•° | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|---------------|----------|-------------------|----------|----------|\n"
        for i, anomaly in enumerate(time_series_anomalies_cumulative, 1):
            clean_anomaly = anomaly.replace('[ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            parts = clean_anomaly.split('ï¼š')
            if len(parts) >= 2:
                desc_part = parts[0]
                data_part = parts[1]
                
                # æå–æ—¥æœŸ
                date_match = re.search(r'æˆªè‡³(\d{4}-\d{2}-\d{2})', desc_part)
                date = date_match.group(1) if date_match else ""
                
                # æå–å¯¹æ¯”è½¦å‹
                vehicle_match = re.search(r'ç›¸å¯¹(CM\d|DM\d)åŒæœŸ', desc_part)
                compare_vehicle = vehicle_match.group(1) if vehicle_match else ""
                
                # æå–å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
                if "éª¤å¢" in desc_part:
                    anomaly_type = "ğŸ“ˆ éª¤å¢"
                else:
                    anomaly_type = "ğŸ“‰ éª¤é™"
                
                # æå–æ•°æ®
                cm2_match = re.search(r'CM2ç´¯è®¡(\d+)å•', data_part)
                compare_match = re.search(r'{}åŒæœŸç´¯è®¡(\d+)å•'.format(compare_vehicle), data_part)
                change_match = re.search(r'å˜åŒ–å¹…åº¦([+-]?\d+\.\d+)%', data_part)
                
                cm2_orders = cm2_match.group(1) if cm2_match else ""
                compare_orders = compare_match.group(1) if compare_match else ""
                change_rate = change_match.group(1) + "%" if change_match else ""
                
                report_content += f"| {i} | {date} | {cm2_orders} | {compare_vehicle} | {compare_orders} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”æ­£å¸¸:** CM2ç´¯è®¡è®¢å•é‡ç›¸å¯¹äºå†å²è½¦å‹åŒæœŸå˜åŒ–å‡åœ¨100%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    report_content += "\n## æ£€æŸ¥è¯´æ˜\n\n"
    report_content += "### ğŸ“Š å†å²å¹³å‡å¯¹æ¯”\n"
    report_content += "- **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹(CM0, DM0, CM1, DM1)å¹³å‡å€¼çš„å„åœ°åŒºè®¢å•å æ¯”å˜åŒ–è¶…è¿‡20%çš„æƒ…å†µï¼Œä¸”è¯¥åœ°åŒºå æ¯”è¶…è¿‡1%\n"
    report_content += "- **æ¸ é“ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹å¹³å‡å€¼çš„æ¸ é“é”€é‡å æ¯”å˜åŒ–è¶…è¿‡15%çš„æƒ…å†µï¼Œä¸”è¯¥æ¸ é“å æ¯”è¶…è¿‡1%\n"
    report_content += "- **äººç¾¤ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹å¹³å‡å€¼çš„æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–è¶…è¿‡10%çš„æƒ…å†µ\n\n"
    report_content += "### ğŸ”„ CM1ç›´æ¥å¯¹æ¯”\n"
    report_content += "- **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„å„åœ°åŒºè®¢å•å æ¯”å˜åŒ–è¶…è¿‡20%çš„æƒ…å†µï¼Œä¸”è¯¥åœ°åŒºå æ¯”è¶…è¿‡1%\n"
    report_content += "- **æ¸ é“ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„æ¸ é“é”€é‡å æ¯”å˜åŒ–è¶…è¿‡15%çš„æƒ…å†µï¼Œä¸”è¯¥æ¸ é“å æ¯”è¶…è¿‡1%\n"
    report_content += "- **äººç¾¤ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–è¶…è¿‡10%çš„æƒ…å†µ\n\n"
    report_content += "### ğŸ“ˆ åŒæ¯”/ç¯æ¯”æ£€æµ‹\n"
    report_content += "- **æ—¥ç¯æ¯”å¼‚å¸¸**: æ£€æŸ¥CM2è½¦å‹å†…éƒ¨ç›¸é‚»æ—¥æœŸè®¢å•é‡å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µ\n"
    report_content += "- **åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹(CM0, CM1, DM0, DM1)ç›¸åŒç›¸å¯¹å¤©æ•°çš„è®¢å•é‡å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µ\n"
    report_content += "- **ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸**: æ£€æŸ¥CM2ç´¯è®¡è®¢å•é‡ç›¸å¯¹äºå†å²è½¦å‹åŒæœŸç´¯è®¡è®¢å•é‡å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µ\n"
    
    # æ·»åŠ å‘å¸ƒä¼šåç¬¬Næ—¥é”å•æ•°æ®å¯¹æ¯”åˆ†æ
    report_content += "\n### ğŸ”’ å‘å¸ƒä¼šåç¬¬Næ—¥é”å•æ•°æ®å¯¹æ¯”åˆ†æ\n\n"
    
    if lock_analysis:
        # 1. CM2è½¦å‹å‘å¸ƒä¼šåç¬¬Næ—¥é”å•æ•°æ®å¯¹æ¯”
        if 'cm2_lock_orders' in lock_analysis and lock_analysis['cm2_lock_orders']:
            cm2_lock = lock_analysis['cm2_lock_orders']
            report_content += f"#### ğŸ“Š CM2è½¦å‹å‘å¸ƒä¼šåç¬¬{cm2_lock.get('n_days', 'N')}æ—¥é”å•æ•°æ®å¯¹æ¯”\n\n"
            report_content += f"- **é”å•æ•°**: {cm2_lock.get('lock_orders_count', 0)}å•\n"
            report_content += f"- **æœ€å¤§é”å•æ—¶é—´**: {cm2_lock.get('max_lock_time', 'N/A')}\n\n"
        
        # 2. CM2è½¦å‹å‘å¸ƒä¼šåç¬¬Næ—¥å°è®¢ç•™å­˜é”å•æ•°æ®å¯¹æ¯”
        if 'cm2_small_order_retention' in lock_analysis and lock_analysis['cm2_small_order_retention']:
            cm2_small = lock_analysis['cm2_small_order_retention']
            report_content += f"#### ğŸ“ˆ CM2è½¦å‹å‘å¸ƒä¼šåç¬¬{cm2_small.get('n_days', 'N')}æ—¥å°è®¢ç•™å­˜é”å•æ•°æ®å¯¹æ¯”\n\n"
            report_content += f"- **å°è®¢ç•™å­˜é”å•æ•°**: {cm2_small.get('small_retention_count', 0)}å•\n\n"
        
        # 3. åŒæœŸå…¶ä»–è½¦å‹å‘å¸ƒä¼šåç¬¬Næ—¥é”å•æ•°æ®å¯¹æ¯”
        if 'other_vehicles_lock_orders' in lock_analysis and lock_analysis['other_vehicles_lock_orders']:
            report_content += "#### ğŸš— åŒæœŸå…¶ä»–è½¦å‹å‘å¸ƒä¼šåç¬¬Næ—¥é”å•æ•°æ®å¯¹æ¯”\n\n"
            for vehicle, data in lock_analysis['other_vehicles_lock_orders'].items():
                report_content += f"**{vehicle}è½¦å‹** (ç¬¬{data.get('n_days', 'N')}æ—¥, {data.get('target_date', 'N/A')}):\n"
                report_content += f"- é”å•æ•°: {data.get('lock_orders_count', 0)}å•\n\n"
        
        # 4. åŒæœŸå…¶ä»–è½¦å‹å°è®¢ç•™å­˜é”å•æ•°æ®å¯¹æ¯”
        if 'other_vehicles_small_order_retention' in lock_analysis and lock_analysis['other_vehicles_small_order_retention']:
            report_content += "#### ğŸ“‹ åŒæœŸå…¶ä»–è½¦å‹å‘å¸ƒä¼šåç¬¬Næ—¥å°è®¢ç•™å­˜é”å•æ•°æ®å¯¹æ¯”\n\n"
            for vehicle, data in lock_analysis['other_vehicles_small_order_retention'].items():
                report_content += f"**{vehicle}è½¦å‹** (ç¬¬{data.get('n_days', 'N')}æ—¥):\n"
                report_content += f"- å°è®¢ç•™å­˜é”å•æ•°: {data.get('small_retention_count', 0)}å•\n\n"
        
        # 5. æ‰€æœ‰è½¦å‹å‘å¸ƒä¼šåç¬¬Næ—¥ç´¯è®¡é”å•æ•°æ®å¯¹æ¯”
        if 'cumulative_lock_orders' in lock_analysis and lock_analysis['cumulative_lock_orders']:
            cumulative_lock = lock_analysis['cumulative_lock_orders']
            report_content += "#### ğŸ“Š æ‰€æœ‰è½¦å‹å‘å¸ƒä¼šåç¬¬Næ—¥ç´¯è®¡é”å•æ•°æ®å¯¹æ¯”\n\n"
            report_content += f"- **ç´¯è®¡é”å•æ•°æ€»è®¡**: {cumulative_lock.get('total_count', 0)}å•\n\n"
            
            # æŒ‰è½¦å‹åˆ†åˆ«æ˜¾ç¤º
            if 'by_vehicle' in cumulative_lock and cumulative_lock['by_vehicle']:
                report_content += "**å„è½¦å‹é”å•æ•°è¯¦æƒ…**:\n"
                for vehicle, count in cumulative_lock['by_vehicle'].items():
                    report_content += f"- {vehicle}è½¦å‹: {count}å•\n"
                report_content += "\n"
        
        # 6. æ‰€æœ‰è½¦å‹ç´¯è®¡å°è®¢ç•™å­˜é”å•æ•°æ®å¯¹æ¯”
        if 'cumulative_small_order_retention' in lock_analysis and lock_analysis['cumulative_small_order_retention']:
            cumulative_small = lock_analysis['cumulative_small_order_retention']
            report_content += "#### ğŸ“ˆ æ‰€æœ‰è½¦å‹å‘å¸ƒä¼šåç¬¬Næ—¥ç´¯è®¡å°è®¢ç•™å­˜é”å•æ•°æ®å¯¹æ¯”\n\n"
            report_content += f"- **ç´¯è®¡å°è®¢ç•™å­˜é”å•æ•°æ€»è®¡**: {cumulative_small.get('total_count', 0)}å•\n\n"
            
            # æŒ‰è½¦å‹åˆ†åˆ«æ˜¾ç¤º
            if 'by_vehicle' in cumulative_small and cumulative_small['by_vehicle']:
                report_content += "**å„è½¦å‹å°è®¢ç•™å­˜é”å•æ•°è¯¦æƒ…**:\n"
                for vehicle, count in cumulative_small['by_vehicle'].items():
                    report_content += f"- {vehicle}è½¦å‹: {count}å•\n"
                report_content += "\n"
    else:
        report_content += "**âš ï¸ æ— æ³•ç”Ÿæˆé”å•æ•°æ®å¯¹æ¯”åˆ†æ**: ç¼ºå°‘é”å•åˆ†ææ•°æ®\n\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/structure_check_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"ç»“æ„æ£€æŸ¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“‹ ç»“æ„æ£€æŸ¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # æ›´æ–°çŠ¶æ€
    state["structure_anomalies"] = anomalies
    state["structure_report_path"] = report_path

# åˆ›å»ºå·¥ä½œæµå›¾
def create_workflow():
    """
    åˆ›å»ºLangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ
    """
    workflow = StateGraph(WorkflowState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("anomaly_detection", anomaly_detection_node)
    workflow.add_node("structure_check", structure_check_node)
    workflow.add_node("sales_agent_analysis", sales_agent_analysis_node)
    workflow.add_node("time_interval_analysis", lambda state: analyze_time_intervals(state))
    workflow.add_node("generate_complete_report", lambda state: generate_complete_report(state) or state)
    workflow.add_node("update_readme", update_readme_mermaid_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("anomaly_detection")
    
    # æ·»åŠ è¾¹
    workflow.add_edge("anomaly_detection", "structure_check")
    workflow.add_edge("structure_check", "sales_agent_analysis")
    workflow.add_edge("sales_agent_analysis", "time_interval_analysis")
    workflow.add_edge("time_interval_analysis", "generate_complete_report")
    workflow.add_edge("generate_complete_report", "update_readme")
    workflow.add_edge("update_readme", END)
    
    return workflow.compile()

# ä¸»å‡½æ•°
def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œå·¥ä½œæµ
    """
    logger.info("å¯åŠ¨LangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ...")
    
    # åˆ›å»ºå·¥ä½œæµ
    app = create_workflow()
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "data": None,
        "data_path": "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet",
        "integrity_check_results": {},
        "errors": [],
        "status": "initialized",
        "metadata": {}
    }
    
    # è¿è¡Œå·¥ä½œæµ
    try:
        result = app.invoke(initial_state)
        
        if result["status"] == "failed":
            logger.error("å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            for error in result["errors"]:
                logger.error(f"é”™è¯¯: {error}")
        else:
            logger.info("å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
            logger.info(f"æœ€ç»ˆçŠ¶æ€: {result['status']}")
            
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    main()
