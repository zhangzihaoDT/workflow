#!/usr/bin/env python3
"""
LangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµè„šæœ¬
ç”¨äºå¤„ç†æ•°æ®é›†å¹¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import pandas as pd
import pyarrow.parquet as pq
from typing import Dict, Any, List
from langgraph.graph import StateGraph, END
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å®šä¹‰å·¥ä½œæµçŠ¶æ€ç±»å‹
from typing import TypedDict

class WorkflowState(TypedDict):
    data: pd.DataFrame
    data_path: str
    integrity_check_results: Dict[str, Any]
    errors: List[str]
    status: str
    metadata: Dict[str, Any]

# ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
def generate_anomaly_report(state: WorkflowState, data_completeness: float, duplicate_rows: int, 
                           numeric_columns: list, categorical_columns: list, datetime_columns: list, 
                           missing_values: pd.Series, missing_cells: int) -> None:
    """
    ç”Ÿæˆå¼‚å¸¸æ£€æµ‹MDæŠ¥å‘Š
    """
    report_content = f"""# å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
- **æ•°æ®æ–‡ä»¶**: {state['data_path']}
- **ç”Ÿæˆæ—¶é—´**: {state['metadata']['processing_timestamp']}
- **æ–‡ä»¶å¤§å°**: {state['metadata']['file_size_mb']:.2f} MB

## æ•°æ®åŸºæœ¬ä¿¡æ¯
- **æ•°æ®å½¢çŠ¶**: {state['data'].shape[0]} è¡Œ Ã— {state['data'].shape[1]} åˆ—
- **æ•°æ®å®Œæ•´æ€§**: {data_completeness:.2f}%
- **é‡å¤è¡Œæ•°**: {duplicate_rows}

## æ•°æ®ç±»å‹åˆ†å¸ƒ
- **æ•°å€¼åˆ—**: {len(numeric_columns)} ä¸ª
- **åˆ†ç±»åˆ—**: {len(categorical_columns)} ä¸ª  
- **æ—¥æœŸåˆ—**: {len(datetime_columns)} ä¸ª

## åˆ—ä¿¡æ¯è¯¦æƒ…
### æ•°å€¼åˆ—
{', '.join(numeric_columns) if numeric_columns else 'æ— '}

### åˆ†ç±»åˆ—
{', '.join(categorical_columns) if categorical_columns else 'æ— '}

### æ—¥æœŸåˆ—
{', '.join(datetime_columns) if datetime_columns else 'æ— '}

## å¼‚å¸¸æ£€æµ‹ç»“æœ
"""
    
    if missing_cells > 0:
        report_content += "\n### ç¼ºå¤±å€¼å¼‚å¸¸\n\n| åˆ—å | ç¼ºå¤±æ•°é‡ | ç¼ºå¤±æ¯”ä¾‹ |\n|------|----------|----------|\n"
        for col, missing_count in missing_values.items():
            if missing_count > 0:
                missing_pct = missing_count / len(state["data"]) * 100
                report_content += f"| {col} | {missing_count} | {missing_pct:.2f}% |\n"
    else:
        report_content += "\n### ç¼ºå¤±å€¼æ£€æŸ¥\nâœ… æœªå‘ç°ç¼ºå¤±å€¼\n"
    
    if duplicate_rows > 0:
        report_content += f"\n### é‡å¤æ•°æ®å¼‚å¸¸\nâš ï¸ å‘ç° {duplicate_rows} è¡Œé‡å¤æ•°æ®\n"
    else:
        report_content += "\n### é‡å¤æ•°æ®æ£€æŸ¥\nâœ… æœªå‘ç°é‡å¤æ•°æ®\n"
    
    # æ•°æ®è´¨é‡è¯„çº§
    if data_completeness >= 95:
        quality_grade = "ä¼˜ç§€ âœ…"
    elif data_completeness >= 90:
        quality_grade = "è‰¯å¥½ âš ï¸"
    elif data_completeness >= 80:
        quality_grade = "ä¸€èˆ¬ âš ï¸"
    else:
        quality_grade = "è¾ƒå·® âŒ"
    
    report_content += f"\n## æ•°æ®è´¨é‡è¯„ä¼°\n- **æ•´ä½“è¯„çº§**: {quality_grade}\n- **å®Œæ•´æ€§å¾—åˆ†**: {data_completeness:.2f}%\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/anomaly_detection_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“Š å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

# æ›´æ–°READMEä¸­çš„mermaidå›¾ç¤ºèŠ‚ç‚¹
def update_readme_mermaid_node(state: WorkflowState) -> WorkflowState:
    """
    æ›´æ–°README.mdä¸­çš„mermaidå·¥ä½œæµå›¾ç¤º
    """
    logger.info("å¼€å§‹æ›´æ–°README.mdä¸­çš„mermaidå›¾ç¤º...")
    
    try:
        mermaid_content = """# W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµ

## å·¥ä½œæµæ¶æ„

```mermaid
flowchart TD
    A[å¼€å§‹] --> B[å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹]
    B --> C[è¯»å–æ•°æ®æ–‡ä»¶]
    C --> D[æ•°æ®å®Œæ•´æ€§æ£€æŸ¥]
    D --> E[ç¼ºå¤±å€¼æ£€æµ‹]
    E --> F[é‡å¤æ•°æ®æ£€æµ‹]
    F --> G[æ•°æ®ç±»å‹åˆ†æ]
    G --> H[ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š]
    H --> I[æ›´æ–°READMEå›¾ç¤º]
    I --> J[ç»“æŸ]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style H fill:#e8f5e8
    style I fill:#fff3e0
    style J fill:#ffebee
```

## åŠŸèƒ½è¯´æ˜

### å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹
- æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
- æ•°æ®è¯»å–éªŒè¯
- åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
- ç¼ºå¤±å€¼æ£€æµ‹ä¸åˆ†æ
- é‡å¤æ•°æ®æ£€æµ‹
- æ•°æ®ç±»å‹åˆ†å¸ƒåˆ†æ
- å¼‚å¸¸å€¼è¯†åˆ«

### æŠ¥å‘Šç”Ÿæˆ
- è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„MDæ ¼å¼å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
- åŒ…å«æ•°æ®è´¨é‡è¯„çº§
- æä¾›å¯è§†åŒ–çš„å¼‚å¸¸ç»Ÿè®¡ä¿¡æ¯

### å·¥ä½œæµç‰¹æ€§
- åŸºäºLangGraphæ¡†æ¶æ„å»º
- æ¨¡å—åŒ–èŠ‚ç‚¹è®¾è®¡
- å®Œæ•´çš„é”™è¯¯å¤„ç†æœºåˆ¶
- è¯¦ç»†çš„æ—¥å¿—è®°å½•

## ä½¿ç”¨æ–¹æ³•

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# è¿è¡Œå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ
python main.py
```

## è¾“å‡ºæ–‡ä»¶
- `anomaly_detection_report.md`: è¯¦ç»†çš„å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
- æ§åˆ¶å°æ—¥å¿—: å®æ—¶å¤„ç†çŠ¶æ€ä¿¡æ¯
"""
        
        readme_path = "/Users/zihao_/Documents/github/W35_workflow/README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(mermaid_content)
        
        state["status"] = "readme_updated"
        logger.info("README.mdä¸­çš„mermaidå›¾ç¤ºæ›´æ–°å®Œæˆ")
        print(f"\nğŸ“ README.mdå·²æ›´æ–°: {readme_path}")
        
    except Exception as e:
        error_msg = f"æ›´æ–°README.mdè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹
def anomaly_detection_node(state: WorkflowState) -> WorkflowState:
    """
    å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹ - å·¥ä½œæµçš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
    æ£€æŸ¥æ•°æ®é›†çš„å¼‚å¸¸æƒ…å†µï¼ŒåŒ…æ‹¬ï¼š
    1. æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    2. æ•°æ®æ˜¯å¦å¯ä»¥æ­£å¸¸è¯»å–
    3. æ•°æ®åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    4. ç¼ºå¤±å€¼æ£€æŸ¥
    5. æ•°æ®ç±»å‹æ£€æŸ¥
    6. å¼‚å¸¸å€¼æ£€æµ‹
    """
    logger.info("å¼€å§‹æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
    
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        data_path = Path(state["data_path"])
        if not data_path.exists():
            error_msg = f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {state['data_path']}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["status"] = "failed"
            return state
        
        logger.info(f"æ•°æ®æ–‡ä»¶å­˜åœ¨: {state['data_path']}")
        
        # 2. è¯»å–æ•°æ®
        try:
            state["data"] = pd.read_parquet(state["data_path"])
            logger.info(f"æˆåŠŸè¯»å–æ•°æ®ï¼Œæ•°æ®å½¢çŠ¶: {state['data'].shape}")
        except Exception as e:
            error_msg = f"è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["status"] = "failed"
            return state
        
        # 3. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        state["integrity_check_results"]["shape"] = state["data"].shape
        state["integrity_check_results"]["columns"] = list(state["data"].columns)
        state["integrity_check_results"]["dtypes"] = state["data"].dtypes.to_dict()
        
        # 4. ç¼ºå¤±å€¼æ£€æŸ¥
        missing_values = state["data"].isnull().sum()
        state["integrity_check_results"]["missing_values"] = missing_values.to_dict()
        state["integrity_check_results"]["missing_percentage"] = (missing_values / len(state["data"]) * 100).to_dict()
        
        # 5. æ•°æ®ç±»å‹åˆ†å¸ƒ
        numeric_columns = state["data"].select_dtypes(include=['number']).columns.tolist()
        categorical_columns = state["data"].select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_columns = state["data"].select_dtypes(include=['datetime']).columns.tolist()
        
        state["integrity_check_results"]["numeric_columns"] = numeric_columns
        state["integrity_check_results"]["categorical_columns"] = categorical_columns
        state["integrity_check_results"]["datetime_columns"] = datetime_columns
        
        # 6. æ•°æ®è´¨é‡è¯„ä¼°
        total_cells = state["data"].shape[0] * state["data"].shape[1]
        missing_cells = state["data"].isnull().sum().sum()
        data_completeness = (total_cells - missing_cells) / total_cells * 100
        
        state["integrity_check_results"]["data_completeness_percentage"] = data_completeness
        
        # 7. åŸºæœ¬æè¿°æ€§ç»Ÿè®¡
        if numeric_columns:
            state["integrity_check_results"]["numeric_summary"] = state["data"][numeric_columns].describe().to_dict()
        
        # 8. é‡å¤è¡Œæ£€æŸ¥
        duplicate_rows = state["data"].duplicated().sum()
        state["integrity_check_results"]["duplicate_rows"] = duplicate_rows
        
        # è®°å½•å…ƒæ•°æ®
        state["metadata"]["file_size_mb"] = data_path.stat().st_size / (1024 * 1024)
        state["metadata"]["processing_timestamp"] = pd.Timestamp.now().isoformat()
        
        state["status"] = "anomaly_detection_completed"
        logger.info("å¼‚å¸¸æ£€æµ‹å®Œæˆ")
        
        # ç”ŸæˆMDæŠ¥å‘Š
        generate_anomaly_report(state, data_completeness, duplicate_rows, numeric_columns, categorical_columns, datetime_columns, missing_values, missing_cells)
        
    except Exception as e:
        error_msg = f"å¼‚å¸¸æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# åˆ›å»ºå·¥ä½œæµå›¾
def create_workflow():
    """
    åˆ›å»ºLangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ
    """
    workflow = StateGraph(WorkflowState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("anomaly_detection", anomaly_detection_node)
    workflow.add_node("update_readme", update_readme_mermaid_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("anomaly_detection")
    
    # æ·»åŠ è¾¹
    workflow.add_edge("anomaly_detection", "update_readme")
    workflow.add_edge("update_readme", END)
    
    return workflow.compile()

# ä¸»å‡½æ•°
def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œå·¥ä½œæµ
    """
    logger.info("å¯åŠ¨LangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ...")
    
    # åˆ›å»ºå·¥ä½œæµ
    app = create_workflow()
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "data": None,
        "data_path": "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet",
        "integrity_check_results": {},
        "errors": [],
        "status": "initialized",
        "metadata": {}
    }
    
    # è¿è¡Œå·¥ä½œæµ
    try:
        result = app.invoke(initial_state)
        
        if result["status"] == "failed":
            logger.error("å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            for error in result["errors"]:
                logger.error(f"é”™è¯¯: {error}")
        else:
            logger.info("å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
            logger.info(f"æœ€ç»ˆçŠ¶æ€: {result['status']}")
            
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    main()
