#!/usr/bin/env python3
"""
LangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµè„šæœ¬
ç”¨äºå¤„ç†æ•°æ®é›†å¹¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import pandas as pd
import pyarrow.parquet as pq
from typing import Dict, Any, List
from langgraph.graph import StateGraph, END
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å®šä¹‰å·¥ä½œæµçŠ¶æ€ç±»å‹
from typing import TypedDict

class WorkflowState(TypedDict):
    data: pd.DataFrame
    data_path: str
    integrity_check_results: Dict[str, Any]
    errors: List[str]
    status: str
    metadata: Dict[str, Any]

# ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
def generate_anomaly_report(state: WorkflowState, data_completeness: float, duplicate_rows: int, 
                           numeric_columns: list, categorical_columns: list, datetime_columns: list, 
                           missing_values: pd.Series, missing_cells: int) -> None:
    """
    ç”Ÿæˆå¼‚å¸¸æ£€æµ‹MDæŠ¥å‘Š
    """
    report_content = f"""# å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
- **æ•°æ®æ–‡ä»¶**: {state['data_path']}
- **ç”Ÿæˆæ—¶é—´**: {state['metadata']['processing_timestamp']}
- **æ–‡ä»¶å¤§å°**: {state['metadata']['file_size_mb']:.2f} MB

## æ•°æ®åŸºæœ¬ä¿¡æ¯
- **æ•°æ®å½¢çŠ¶**: {state['data'].shape[0]} è¡Œ Ã— {state['data'].shape[1]} åˆ—
- **æ•°æ®å®Œæ•´æ€§**: {data_completeness:.2f}%
- **é‡å¤è¡Œæ•°**: {duplicate_rows}

## æ•°æ®ç±»å‹åˆ†å¸ƒ
- **æ•°å€¼åˆ—**: {len(numeric_columns)} ä¸ª
- **åˆ†ç±»åˆ—**: {len(categorical_columns)} ä¸ª  
- **æ—¥æœŸåˆ—**: {len(datetime_columns)} ä¸ª

## åˆ—ä¿¡æ¯è¯¦æƒ…
### æ•°å€¼åˆ—
{', '.join(numeric_columns) if numeric_columns else 'æ— '}

### åˆ†ç±»åˆ—
{', '.join(categorical_columns) if categorical_columns else 'æ— '}

### æ—¥æœŸåˆ—
{', '.join(datetime_columns) if datetime_columns else 'æ— '}

## å¼‚å¸¸æ£€æµ‹ç»“æœ
"""
    
    if missing_cells > 0:
        report_content += "\n### ç¼ºå¤±å€¼å¼‚å¸¸\n\n| åˆ—å | ç¼ºå¤±æ•°é‡ | ç¼ºå¤±æ¯”ä¾‹ |\n|------|----------|----------|\n"
        for col, missing_count in missing_values.items():
            if missing_count > 0:
                missing_pct = missing_count / len(state["data"]) * 100
                report_content += f"| {col} | {missing_count} | {missing_pct:.2f}% |\n"
    else:
        report_content += "\n### ç¼ºå¤±å€¼æ£€æŸ¥\nâœ… æœªå‘ç°ç¼ºå¤±å€¼\n"
    
    if duplicate_rows > 0:
        report_content += f"\n### é‡å¤æ•°æ®å¼‚å¸¸\nâš ï¸ å‘ç° {duplicate_rows} è¡Œé‡å¤æ•°æ®\n"
    else:
        report_content += "\n### é‡å¤æ•°æ®æ£€æŸ¥\nâœ… æœªå‘ç°é‡å¤æ•°æ®\n"
    
    # æ•°æ®è´¨é‡è¯„çº§
    if data_completeness >= 95:
        quality_grade = "ä¼˜ç§€ âœ…"
    elif data_completeness >= 90:
        quality_grade = "è‰¯å¥½ âš ï¸"
    elif data_completeness >= 80:
        quality_grade = "ä¸€èˆ¬ âš ï¸"
    else:
        quality_grade = "è¾ƒå·® âŒ"
    
    report_content += f"\n## æ•°æ®è´¨é‡è¯„ä¼°\n- **æ•´ä½“è¯„çº§**: {quality_grade}\n- **å®Œæ•´æ€§å¾—åˆ†**: {data_completeness:.2f}%\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/anomaly_detection_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“Š å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

def generate_complete_report(state: WorkflowState) -> None:
    """
    ç”Ÿæˆå®Œæ•´çš„ç»¼åˆæŠ¥å‘Šï¼Œæ•´åˆå¼‚å¸¸æ£€æµ‹å’Œç»“æ„æ£€æŸ¥ç»“æœ
    """
    from datetime import datetime
    import os
    
    try:
        # è¯»å–å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
        anomaly_report_path = "/Users/zihao_/Documents/github/W35_workflow/anomaly_detection_report.md"
        structure_report_path = "/Users/zihao_/Documents/github/W35_workflow/structure_check_report.md"
        
        anomaly_content = ""
        structure_content = ""
        
        if os.path.exists(anomaly_report_path):
            with open(anomaly_report_path, 'r', encoding='utf-8') as f:
                anomaly_content = f.read()
        
        if os.path.exists(structure_report_path):
            with open(structure_report_path, 'r', encoding='utf-8') as f:
                structure_content = f.read()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        complete_report = f"""# W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµ - ç»¼åˆåˆ†ææŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}
- **å·¥ä½œæµç‰ˆæœ¬**: W35 Anomaly Detection Workflow
- **åˆ†æèŒƒå›´**: æ•°æ®è´¨é‡æ£€æµ‹ + ç»“æ„å¼‚å¸¸åˆ†æ

---

{anomaly_content}

---

{structure_content}

---

## ç»¼åˆç»“è®º

### æ•°æ®è´¨é‡çŠ¶å†µ
ä»æ•°æ®è´¨é‡æ£€æµ‹ç»“æœæ¥çœ‹ï¼Œæ•°æ®çš„åŸºæœ¬å®Œæ•´æ€§å’Œä¸€è‡´æ€§æƒ…å†µã€‚

### ç»“æ„å¼‚å¸¸çŠ¶å†µ
ä»ç»“æ„æ£€æŸ¥ç»“æœæ¥çœ‹ï¼ŒCM2è½¦å‹ç›¸å¯¹äºå†å²è½¦å‹åœ¨åœ°åŒºåˆ†å¸ƒã€æ¸ é“ç»“æ„ã€äººç¾¤ç»“æ„æ–¹é¢çš„å˜åŒ–æƒ…å†µã€‚

### å»ºè®®æªæ–½
1. **æ•°æ®è´¨é‡æ–¹é¢**: æ ¹æ®å¼‚å¸¸æ£€æµ‹ç»“æœï¼Œå¯¹å‘ç°çš„æ•°æ®è´¨é‡é—®é¢˜è¿›è¡Œç›¸åº”å¤„ç†
2. **ç»“æ„å¼‚å¸¸æ–¹é¢**: å¯¹å‘ç°çš„ç»“æ„å¼‚å¸¸è¿›è¡Œæ·±å…¥åˆ†æï¼Œç¡®å®šæ˜¯å¦ä¸ºæ­£å¸¸çš„ä¸šåŠ¡å˜åŒ–æˆ–éœ€è¦å…³æ³¨çš„å¼‚å¸¸æƒ…å†µ
3. **æŒç»­ç›‘æ§**: å»ºè®®å®šæœŸè¿è¡Œæ­¤å·¥ä½œæµï¼ŒæŒç»­ç›‘æ§æ•°æ®è´¨é‡å’Œç»“æ„å˜åŒ–

---

*æœ¬æŠ¥å‘Šç”± W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        complete_report_path = "/Users/zihao_/Documents/github/W35_workflow/complete_analysis_report.md"
        with open(complete_report_path, 'w', encoding='utf-8') as f:
            f.write(complete_report)
        
        logger.info(f"ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {complete_report_path}")
        print(f"\nğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {complete_report_path}")
        
        # æ›´æ–°çŠ¶æ€
        state["complete_report_path"] = complete_report_path
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆç»¼åˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        print(f"âŒ {error_msg}")

# æ›´æ–°READMEä¸­çš„mermaidå›¾ç¤ºèŠ‚ç‚¹
def update_readme_mermaid_node(state: WorkflowState) -> WorkflowState:
    """
    æ›´æ–°README.mdä¸­çš„mermaidå·¥ä½œæµå›¾ç¤º
    """
    logger.info("å¼€å§‹æ›´æ–°README.mdä¸­çš„mermaidå›¾ç¤º...")
    
    try:
        mermaid_content = """# W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµ

## å·¥ä½œæµæ¶æ„

```mermaid
flowchart TD
    A[å¼€å§‹] --> B[å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹]
    B --> C[ç»“æ„æ£€æŸ¥èŠ‚ç‚¹]
    C --> D[ç”Ÿæˆç»¼åˆæŠ¥å‘Š]
    D --> E[æ›´æ–°READMEå›¾ç¤º]
    E --> F[ç»“æŸ]
    
    B --> B1[æ•°æ®è´¨é‡æ£€æŸ¥]
    B --> B2[ç¼ºå¤±å€¼æ£€æµ‹]
    B --> B3[é‡å¤æ•°æ®æ£€æµ‹]
    B --> B4[ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š]
    
    C --> C1[åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æŸ¥]
    C --> C2[æ¸ é“ç»“æ„å¼‚å¸¸æ£€æŸ¥]
    C --> C3[äººç¾¤ç»“æ„å¼‚å¸¸æ£€æŸ¥]
    C --> C4[ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š]
    
    D --> D1[æ•´åˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š]
    D --> D2[æ•´åˆç»“æ„æ£€æŸ¥æŠ¥å‘Š]
    D --> D3[ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#ffebee
    style F fill:#f1f8e9
```

## åŠŸèƒ½è¯´æ˜

### å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹
- æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
- æ•°æ®è¯»å–éªŒè¯
- åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
- ç¼ºå¤±å€¼æ£€æµ‹ä¸åˆ†æ
- é‡å¤æ•°æ®æ£€æµ‹
- æ•°æ®ç±»å‹åˆ†å¸ƒåˆ†æ
- å¼‚å¸¸å€¼è¯†åˆ«

### ç»“æ„æ£€æŸ¥èŠ‚ç‚¹
- åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹ï¼šå¯¹æ¯”CM2è½¦å‹ä¸å†å²è½¦å‹çš„åœ°åŒºè®¢å•åˆ†å¸ƒå˜åŒ–
- æ¸ é“ç»“æ„å¼‚å¸¸æ£€æµ‹ï¼šåˆ†æå„æ¸ é“é”€é‡å æ¯”çš„çªå˜æƒ…å†µ
- äººç¾¤ç»“æ„å¼‚å¸¸æ£€æµ‹ï¼šæ£€æµ‹æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„çš„å¤§å¹…å˜åŒ–
- åŸºäºä¸šåŠ¡å®šä¹‰çš„æ—¶é—´èŒƒå›´è¿›è¡Œæ•°æ®ç­›é€‰

### ç»¼åˆæŠ¥å‘Šç”Ÿæˆ
- æ•´åˆå¼‚å¸¸æ£€æµ‹å’Œç»“æ„æ£€æŸ¥ç»“æœ
- æä¾›æ•°æ®è´¨é‡å’Œç»“æ„å¼‚å¸¸çš„ç»¼åˆè¯„ä¼°
- åŸºäºæ£€æµ‹ç»“æœæä¾›ç›¸åº”çš„å¤„ç†å»ºè®®

### æŠ¥å‘Šç”Ÿæˆ
- è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„MDæ ¼å¼å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
- ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
- ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
- åŒ…å«æ•°æ®è´¨é‡è¯„çº§
- æä¾›å¯è§†åŒ–çš„å¼‚å¸¸ç»Ÿè®¡ä¿¡æ¯

### å·¥ä½œæµç‰¹æ€§
- åŸºäºLangGraphæ¡†æ¶æ„å»º
- æ¨¡å—åŒ–èŠ‚ç‚¹è®¾è®¡
- å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹ï¼ˆæ•°æ®è´¨é‡+ä¸šåŠ¡ç»“æ„ï¼‰
- å®Œæ•´çš„é”™è¯¯å¤„ç†æœºåˆ¶
- è¯¦ç»†çš„æ—¥å¿—è®°å½•

## ä½¿ç”¨æ–¹æ³•

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# è¿è¡Œå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ
python main.py
```

## è¾“å‡ºæ–‡ä»¶
- `anomaly_detection_report.md`: è¯¦ç»†çš„å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
- `structure_check_report.md`: ç»“æ„æ£€æŸ¥è¯¦ç»†æŠ¥å‘Š
- `complete_analysis_report.md`: ç»¼åˆåˆ†ææŠ¥å‘Š
- æ§åˆ¶å°æ—¥å¿—: å®æ—¶å¤„ç†çŠ¶æ€ä¿¡æ¯

## é…ç½®æ–‡ä»¶
- `business_definition.json`: ä¸šåŠ¡æ—¶é—´èŒƒå›´å®šä¹‰ï¼Œç”¨äºç»“æ„æ£€æŸ¥çš„æ•°æ®ç­›é€‰
"""
        
        readme_path = "/Users/zihao_/Documents/github/W35_workflow/README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(mermaid_content)
        
        state["status"] = "readme_updated"
        logger.info("README.mdä¸­çš„mermaidå›¾ç¤ºæ›´æ–°å®Œæˆ")
        print(f"\nğŸ“ README.mdå·²æ›´æ–°: {readme_path}")
        
    except Exception as e:
        error_msg = f"æ›´æ–°README.mdè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹
def anomaly_detection_node(state: WorkflowState) -> WorkflowState:
    """
    å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹ - å·¥ä½œæµçš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
    æ£€æŸ¥æ•°æ®é›†çš„å¼‚å¸¸æƒ…å†µï¼ŒåŒ…æ‹¬ï¼š
    1. æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    2. æ•°æ®æ˜¯å¦å¯ä»¥æ­£å¸¸è¯»å–
    3. æ•°æ®åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    4. ç¼ºå¤±å€¼æ£€æŸ¥
    5. æ•°æ®ç±»å‹æ£€æŸ¥
    6. å¼‚å¸¸å€¼æ£€æµ‹
    """
    logger.info("å¼€å§‹æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
    
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        data_path = Path(state["data_path"])
        if not data_path.exists():
            error_msg = f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {state['data_path']}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["status"] = "failed"
            return state
        
        logger.info(f"æ•°æ®æ–‡ä»¶å­˜åœ¨: {state['data_path']}")
        
        # 2. è¯»å–æ•°æ®
        try:
            state["data"] = pd.read_parquet(state["data_path"])
            logger.info(f"æˆåŠŸè¯»å–æ•°æ®ï¼Œæ•°æ®å½¢çŠ¶: {state['data'].shape}")
        except Exception as e:
            error_msg = f"è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["status"] = "failed"
            return state
        
        # 3. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        state["integrity_check_results"]["shape"] = state["data"].shape
        state["integrity_check_results"]["columns"] = list(state["data"].columns)
        state["integrity_check_results"]["dtypes"] = state["data"].dtypes.to_dict()
        
        # 4. ç¼ºå¤±å€¼æ£€æŸ¥
        missing_values = state["data"].isnull().sum()
        state["integrity_check_results"]["missing_values"] = missing_values.to_dict()
        state["integrity_check_results"]["missing_percentage"] = (missing_values / len(state["data"]) * 100).to_dict()
        
        # 5. æ•°æ®ç±»å‹åˆ†å¸ƒ
        numeric_columns = state["data"].select_dtypes(include=['number']).columns.tolist()
        categorical_columns = state["data"].select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_columns = state["data"].select_dtypes(include=['datetime']).columns.tolist()
        
        state["integrity_check_results"]["numeric_columns"] = numeric_columns
        state["integrity_check_results"]["categorical_columns"] = categorical_columns
        state["integrity_check_results"]["datetime_columns"] = datetime_columns
        
        # 6. æ•°æ®è´¨é‡è¯„ä¼°
        total_cells = state["data"].shape[0] * state["data"].shape[1]
        missing_cells = state["data"].isnull().sum().sum()
        data_completeness = (total_cells - missing_cells) / total_cells * 100
        
        state["integrity_check_results"]["data_completeness_percentage"] = data_completeness
        
        # 7. åŸºæœ¬æè¿°æ€§ç»Ÿè®¡
        if numeric_columns:
            state["integrity_check_results"]["numeric_summary"] = state["data"][numeric_columns].describe().to_dict()
        
        # 8. é‡å¤è¡Œæ£€æŸ¥
        duplicate_rows = state["data"].duplicated().sum()
        state["integrity_check_results"]["duplicate_rows"] = duplicate_rows
        
        # è®°å½•å…ƒæ•°æ®
        state["metadata"]["file_size_mb"] = data_path.stat().st_size / (1024 * 1024)
        state["metadata"]["processing_timestamp"] = pd.Timestamp.now().isoformat()
        
        state["status"] = "anomaly_detection_completed"
        logger.info("å¼‚å¸¸æ£€æµ‹å®Œæˆ")
        
        # ç”ŸæˆMDæŠ¥å‘Š
        generate_anomaly_report(state, data_completeness, duplicate_rows, numeric_columns, categorical_columns, datetime_columns, missing_values, missing_cells)
        
    except Exception as e:
        error_msg = f"å¼‚å¸¸æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# ç»“æ„æ£€æŸ¥èŠ‚ç‚¹
def structure_check_node(state: WorkflowState) -> WorkflowState:
    """
    ç»“æ„æ£€æŸ¥èŠ‚ç‚¹ï¼šåˆ†æCM2è½¦å‹ç›¸å¯¹äºå†å²è½¦å‹çš„ç»“æ„å¼‚å¸¸
    """
    logger.info("å¼€å§‹æ‰§è¡Œç»“æ„æ£€æŸ¥...")
    
    try:
        # è¯»å–ä¸šåŠ¡å®šä¹‰æ–‡ä»¶
        business_def_path = "/Users/zihao_/Documents/github/W35_workflow/business_definition.json"
        with open(business_def_path, 'r', encoding='utf-8') as f:
            import json
            presale_periods = json.load(f)
        
        # è¯»å–æ•°æ®
        data_path = "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet"
        df = pd.read_parquet(data_path)
        
        # è½¬æ¢æ—¶é—´åˆ—
        df['Intention_Payment_Time'] = pd.to_datetime(df['Intention_Payment_Time'])
        
        # ç­›é€‰å„è½¦å‹æ•°æ®
        vehicle_data = {}
        for vehicle, period in presale_periods.items():
            start_date = pd.to_datetime(period['start'])
            end_date = pd.to_datetime(period['end'])
            mask = (df['Intention_Payment_Time'] >= start_date) & (df['Intention_Payment_Time'] <= end_date)
            vehicle_data[vehicle] = df[mask].copy()
        
        # åˆ†æç»“æœ
        anomalies = []
        
        # 1. åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æŸ¥
        region_anomalies = analyze_region_distribution(vehicle_data)
        if region_anomalies:
            anomalies.extend(region_anomalies)
        
        # 2. æ¸ é“ç»“æ„å¼‚å¸¸æ£€æŸ¥
        channel_anomalies = analyze_channel_structure(vehicle_data)
        if channel_anomalies:
            anomalies.extend(channel_anomalies)
        
        # 3. äººç¾¤ç»“æ„å¼‚å¸¸æ£€æŸ¥
        demographic_anomalies = analyze_demographic_structure(vehicle_data)
        if demographic_anomalies:
            anomalies.extend(demographic_anomalies)
        
        # ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
        generate_structure_report(state, vehicle_data, anomalies)
        
        state["status"] = "structure_check_completed"
        logger.info("ç»“æ„æ£€æŸ¥å®Œæˆ")
        
    except Exception as e:
        error_msg = f"ç»“æ„æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# åœ°åŒºåˆ†å¸ƒå¼‚å¸¸åˆ†æ
def analyze_region_distribution(vehicle_data):
    """
    åˆ†æåœ°åŒºåˆ†å¸ƒå¼‚å¸¸
    """
    anomalies = []
    
    # è·å–CM2æ•°æ®
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåœ°åŒºåˆ†å¸ƒåˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    for region_col in ['Parent Region Name', 'License Province', 'license_city_level', 'License City']:
        if region_col not in cm2_data.columns:
            continue
            
        # CM2åœ°åŒºåˆ†å¸ƒ
        cm2_region_dist = cm2_data[region_col].value_counts(normalize=True)
        
        # å†å²è½¦å‹å¹³å‡åˆ†å¸ƒ
        historical_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                hist_dist = vehicle_data[vehicle][region_col].value_counts(normalize=True)
                historical_dists.append(hist_dist)
        
        if historical_dists:
            # è®¡ç®—å†å²å¹³å‡åˆ†å¸ƒ
            all_regions = set()
            for dist in historical_dists:
                all_regions.update(dist.index)
            all_regions.update(cm2_region_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_regions))
            for dist in historical_dists:
                for region in all_regions:
                    avg_historical[region] += dist.get(region, 0) / len(historical_dists)
            
            # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡20%ï¼‰
            for region in all_regions:
                cm2_ratio = cm2_region_dist.get(region, 0)
                hist_ratio = avg_historical.get(region, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.2:  # 20%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºè®¢å•å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
                elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„åœ°åŒºï¼Œå æ¯”è¶…è¿‡1%
                    anomalies.append(f"[å†å²å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºä¸ºæ–°å‡ºç°åŒºåŸŸï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼Œå†å²æ— æ•°æ®")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0:
        for region_col in ['Parent Region Name', 'License Province', 'license_city_level', 'License City']:
            if region_col not in cm2_data.columns or region_col not in cm1_data.columns:
                continue
                
            # CM2å’ŒCM1åœ°åŒºåˆ†å¸ƒ
            cm2_region_dist = cm2_data[region_col].value_counts(normalize=True)
            cm1_region_dist = cm1_data[region_col].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰åœ°åŒº
            all_regions = set(cm2_region_dist.index) | set(cm1_region_dist.index)
            
            # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡20%ï¼‰
            for region in all_regions:
                cm2_ratio = cm2_region_dist.get(region, 0)
                cm1_ratio = cm1_region_dist.get(region, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.2:  # 20%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºè®¢å•å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
                elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„åœ°åŒºï¼Œå æ¯”è¶…è¿‡1%
                    anomalies.append(f"[CM1å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºä¸ºæ–°å‡ºç°åŒºåŸŸï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼ŒCM1æ— æ•°æ®")
    
    return anomalies

# æ¸ é“ç»“æ„å¼‚å¸¸åˆ†æ
def analyze_channel_structure(vehicle_data):
    """
    åˆ†ææ¸ é“ç»“æ„å¼‚å¸¸
    """
    anomalies = []
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ¸ é“ç»“æ„åˆ†æ"]
    
    channel_col = 'first_middle_channel_name'
    
    if channel_col not in cm2_data.columns:
        return [f"ç¼ºå°‘{channel_col}åˆ—ï¼Œæ— æ³•è¿›è¡Œæ¸ é“åˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    # CM2æ¸ é“åˆ†å¸ƒ
    cm2_channel_dist = cm2_data[channel_col].value_counts(normalize=True)
    
    # å†å²æ¸ é“åˆ†å¸ƒ
    historical_dists = []
    for vehicle in historical_vehicles:
        if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
            hist_dist = vehicle_data[vehicle][channel_col].value_counts(normalize=True)
            historical_dists.append(hist_dist)
    
    if historical_dists:
        # è®¡ç®—å†å²å¹³å‡åˆ†å¸ƒ
        all_channels = set()
        for dist in historical_dists:
            all_channels.update(dist.index)
        all_channels.update(cm2_channel_dist.index)
        
        avg_historical = pd.Series(0.0, index=list(all_channels))
        for dist in historical_dists:
            for channel in all_channels:
                avg_historical[channel] += dist.get(channel, 0) / len(historical_dists)
        
        # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
        for channel in all_channels:
            cm2_ratio = cm2_channel_dist.get(channel, 0)
            hist_ratio = avg_historical.get(channel, 0)
            
            # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
            if hist_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                if change_rate > 0.15:  # 15%å˜åŒ–å¹…åº¦é˜ˆå€¼
                    change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                    anomalies.append(f"[å†å²å¯¹æ¯”]æ¸ é“{channel}é”€é‡å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
            elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„æ¸ é“ï¼Œå æ¯”è¶…è¿‡1%
                anomalies.append(f"[å†å²å¯¹æ¯”]æ¸ é“{channel}ä¸ºæ–°å‡ºç°æ¸ é“ï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼Œå†å²æ— æ•°æ®")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0 and channel_col in cm1_data.columns:
        # CM2å’ŒCM1æ¸ é“åˆ†å¸ƒ
        cm2_channel_dist = cm2_data[channel_col].value_counts(normalize=True)
        cm1_channel_dist = cm1_data[channel_col].value_counts(normalize=True)
        
        # è·å–æ‰€æœ‰æ¸ é“
        all_channels = set(cm2_channel_dist.index) | set(cm1_channel_dist.index)
        
        # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
        for channel in all_channels:
            cm2_ratio = cm2_channel_dist.get(channel, 0)
            cm1_ratio = cm1_channel_dist.get(channel, 0)
            
            # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
            if cm1_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                if change_rate > 0.15:  # 15%å˜åŒ–å¹…åº¦é˜ˆå€¼
                    change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                    anomalies.append(f"[CM1å¯¹æ¯”]æ¸ é“{channel}é”€é‡å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
            elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„æ¸ é“ï¼Œå æ¯”è¶…è¿‡1%
                anomalies.append(f"[CM1å¯¹æ¯”]æ¸ é“{channel}ä¸ºæ–°å‡ºç°æ¸ é“ï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼ŒCM1æ— æ•°æ®")
    
    return anomalies

# äººç¾¤ç»“æ„å¼‚å¸¸åˆ†æ
def analyze_demographic_structure(vehicle_data):
    """
    åˆ†æäººç¾¤ç»“æ„å¼‚å¸¸
    """
    anomalies = []
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œäººç¾¤ç»“æ„åˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    # 1.1 æ€§åˆ«æ¯”ä¾‹åˆ†æ
    if 'order_gender' in cm2_data.columns:
        cm2_gender_dist = cm2_data['order_gender'].value_counts(normalize=True)
        
        historical_gender_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                hist_dist = vehicle_data[vehicle]['order_gender'].value_counts(normalize=True)
                historical_gender_dists.append(hist_dist)
        
        if historical_gender_dists:
            all_genders = set()
            for dist in historical_gender_dists:
                all_genders.update(dist.index)
            all_genders.update(cm2_gender_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_genders))
            for dist in historical_gender_dists:
                for gender in all_genders:
                    avg_historical[gender] += dist.get(gender, 0) / len(historical_gender_dists)
            
            # æ£€æŸ¥æ€§åˆ«æ¯”ä¾‹å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for gender in all_genders:
                cm2_ratio = cm2_gender_dist.get(gender, 0)
                hist_ratio = avg_historical.get(gender, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0:
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]æ€§åˆ«{gender}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    # 1.2 å¹´é¾„æ®µç»“æ„åˆ†æ
    if 'buyer_age' in cm2_data.columns:
        # å®šä¹‰å¹´é¾„æ®µ
        def age_group(age):
            if pd.isna(age):
                return 'æœªçŸ¥'
            elif age < 25:
                return '25å²ä»¥ä¸‹'
            elif age < 35:
                return '25-34å²'
            elif age < 45:
                return '35-44å²'
            elif age < 55:
                return '45-54å²'
            else:
                return '55å²ä»¥ä¸Š'
        
        cm2_data_copy = cm2_data.copy()
        cm2_data_copy['age_group'] = cm2_data_copy['buyer_age'].apply(age_group)
        cm2_age_dist = cm2_data_copy['age_group'].value_counts(normalize=True)
        
        historical_age_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                vehicle_data_copy = vehicle_data[vehicle].copy()
                vehicle_data_copy['age_group'] = vehicle_data_copy['buyer_age'].apply(age_group)
                hist_dist = vehicle_data_copy['age_group'].value_counts(normalize=True)
                historical_age_dists.append(hist_dist)
        
        if historical_age_dists:
            all_age_groups = set()
            for dist in historical_age_dists:
                all_age_groups.update(dist.index)
            all_age_groups.update(cm2_age_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_age_groups))
            for dist in historical_age_dists:
                for age_group_name in all_age_groups:
                    avg_historical[age_group_name] += dist.get(age_group_name, 0) / len(historical_age_dists)
            
            # æ£€æŸ¥å¹´é¾„æ®µå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for age_group_name in all_age_groups:
                cm2_ratio = cm2_age_dist.get(age_group_name, 0)
                hist_ratio = avg_historical.get(age_group_name, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0:
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]å¹´é¾„æ®µ{age_group_name}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0:
        # 2.1 æ€§åˆ«æ¯”ä¾‹åˆ†æ
        if 'order_gender' in cm2_data.columns and 'order_gender' in cm1_data.columns:
            cm2_gender_dist = cm2_data['order_gender'].value_counts(normalize=True)
            cm1_gender_dist = cm1_data['order_gender'].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰æ€§åˆ«
            all_genders = set(cm2_gender_dist.index) | set(cm1_gender_dist.index)
            
            # æ£€æŸ¥æ€§åˆ«æ¯”ä¾‹å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for gender in all_genders:
                cm2_ratio = cm2_gender_dist.get(gender, 0)
                cm1_ratio = cm1_gender_dist.get(gender, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0:
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]æ€§åˆ«{gender}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
        
        # 2.2 å¹´é¾„æ®µç»“æ„åˆ†æ
        if 'buyer_age' in cm2_data.columns and 'buyer_age' in cm1_data.columns:
            # å®šä¹‰å¹´é¾„æ®µ
            def age_group(age):
                if pd.isna(age):
                    return 'æœªçŸ¥'
                elif age < 25:
                    return '25å²ä»¥ä¸‹'
                elif age < 35:
                    return '25-34å²'
                elif age < 45:
                    return '35-44å²'
                elif age < 55:
                    return '45-54å²'
                else:
                    return '55å²ä»¥ä¸Š'
            
            cm2_data_copy = cm2_data.copy()
            cm2_data_copy['age_group'] = cm2_data_copy['buyer_age'].apply(age_group)
            cm2_age_dist = cm2_data_copy['age_group'].value_counts(normalize=True)
            
            cm1_data_copy = cm1_data.copy()
            cm1_data_copy['age_group'] = cm1_data_copy['buyer_age'].apply(age_group)
            cm1_age_dist = cm1_data_copy['age_group'].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰å¹´é¾„æ®µ
            all_age_groups = set(cm2_age_dist.index) | set(cm1_age_dist.index)
            
            # æ£€æŸ¥å¹´é¾„æ®µå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for age_group_name in all_age_groups:
                cm2_ratio = cm2_age_dist.get(age_group_name, 0)
                cm1_ratio = cm1_age_dist.get(age_group_name, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0:
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]å¹´é¾„æ®µ{age_group_name}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    return anomalies

# ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
def generate_structure_report(state, vehicle_data, anomalies):
    """
    ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
    """
    from datetime import datetime
    
    report_content = f"""# ç»“æ„æ£€æŸ¥æŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}
- **åˆ†æè½¦å‹**: CM2 vs å†å²è½¦å‹(CM0, DM0, CM1) + CM2 vs CM1ç›´æ¥å¯¹æ¯”
- **æ£€æŸ¥ç»´åº¦**: åœ°åŒºåˆ†å¸ƒã€æ¸ é“ç»“æ„ã€äººç¾¤ç»“æ„

## æ•°æ®æ¦‚å†µ
"""
    
    # æ·»åŠ å„è½¦å‹æ•°æ®é‡ç»Ÿè®¡
    for vehicle, data in vehicle_data.items():
        report_content += f"- **{vehicle}è½¦å‹**: {len(data)} æ¡è®¢å•æ•°æ®\n"
    
    report_content += "\n## ç»“æ„å¼‚å¸¸æ£€æµ‹ç»“æœ\n\n"
    
    # åˆ†ç±»å¼‚å¸¸ - åŒºåˆ†å†å²å¯¹æ¯”å’ŒCM1å¯¹æ¯”
    region_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and any(region_type in a for region_type in ['Parent Region Name', 'License Province', 'license_city_level', 'License City'])]
    region_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and any(region_type in a for region_type in ['Parent Region Name', 'License Province', 'license_city_level', 'License City'])]
    
    channel_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and 'æ¸ é“' in a]
    channel_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and 'æ¸ é“' in a]
    
    demographic_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and any(demo_type in a for demo_type in ['æ€§åˆ«', 'å¹´é¾„æ®µ'])]
    demographic_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and any(demo_type in a for demo_type in ['æ€§åˆ«', 'å¹´é¾„æ®µ'])]
    
    # åœ°åŒºåˆ†å¸ƒå¼‚å¸¸
    report_content += "### ğŸŒ åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if region_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°åœ°åŒºåˆ†å¸ƒå¼‚å¸¸:**\n\n"
        for i, anomaly in enumerate(region_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            report_content += f"{i}. {clean_anomaly}\n"
    else:
        report_content += "**âœ… åœ°åŒºåˆ†å¸ƒæ­£å¸¸:** æ‰€æœ‰åœ°åŒºè®¢å•å æ¯”å˜åŒ–å‡åœ¨20%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if region_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°åœ°åŒºåˆ†å¸ƒå¼‚å¸¸:**\n\n"
        for i, anomaly in enumerate(region_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            report_content += f"{i}. {clean_anomaly}\n"
    else:
        report_content += "**âœ… åœ°åŒºåˆ†å¸ƒæ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰åœ°åŒºè®¢å•å æ¯”å˜åŒ–å‡åœ¨20%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # æ¸ é“ç»“æ„å¼‚å¸¸
    report_content += "\n### ğŸ›’ æ¸ é“ç»“æ„å¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if channel_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°æ¸ é“ç»“æ„å¼‚å¸¸:**\n\n"
        for i, anomaly in enumerate(channel_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            report_content += f"{i}. {clean_anomaly}\n"
    else:
        report_content += "**âœ… æ¸ é“ç»“æ„æ­£å¸¸:** æ‰€æœ‰æ¸ é“é”€é‡å æ¯”å˜åŒ–å‡åœ¨15%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if channel_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°æ¸ é“ç»“æ„å¼‚å¸¸:**\n\n"
        for i, anomaly in enumerate(channel_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            report_content += f"{i}. {clean_anomaly}\n"
    else:
        report_content += "**âœ… æ¸ é“ç»“æ„æ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰æ¸ é“é”€é‡å æ¯”å˜åŒ–å‡åœ¨15%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # äººç¾¤ç»“æ„å¼‚å¸¸
    report_content += "\n### ğŸ‘¥ äººç¾¤ç»“æ„å¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if demographic_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°äººç¾¤ç»“æ„å¼‚å¸¸:**\n\n"
        for i, anomaly in enumerate(demographic_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            report_content += f"{i}. {clean_anomaly}\n"
    else:
        report_content += "**âœ… äººç¾¤ç»“æ„æ­£å¸¸:** æ‰€æœ‰æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–å‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if demographic_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°äººç¾¤ç»“æ„å¼‚å¸¸:**\n\n"
        for i, anomaly in enumerate(demographic_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            report_content += f"{i}. {clean_anomaly}\n"
    else:
        report_content += "**âœ… äººç¾¤ç»“æ„æ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–å‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    report_content += "\n## æ£€æŸ¥è¯´æ˜\n\n"
    report_content += "### ğŸ“Š å†å²å¹³å‡å¯¹æ¯”\n"
    report_content += "- **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹(CM0, DM0, CM1, DM1)å¹³å‡å€¼çš„å„åœ°åŒºè®¢å•å æ¯”å˜åŒ–è¶…è¿‡20%çš„æƒ…å†µï¼Œä¸”è¯¥åœ°åŒºå æ¯”è¶…è¿‡1%\n"
    report_content += "- **æ¸ é“ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹å¹³å‡å€¼çš„æ¸ é“é”€é‡å æ¯”å˜åŒ–è¶…è¿‡15%çš„æƒ…å†µï¼Œä¸”è¯¥æ¸ é“å æ¯”è¶…è¿‡1%\n"
    report_content += "- **äººç¾¤ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹å¹³å‡å€¼çš„æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–è¶…è¿‡10%çš„æƒ…å†µ\n\n"
    report_content += "### ğŸ”„ CM1ç›´æ¥å¯¹æ¯”\n"
    report_content += "- **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„å„åœ°åŒºè®¢å•å æ¯”å˜åŒ–è¶…è¿‡20%çš„æƒ…å†µï¼Œä¸”è¯¥åœ°åŒºå æ¯”è¶…è¿‡1%\n"
    report_content += "- **æ¸ é“ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„æ¸ é“é”€é‡å æ¯”å˜åŒ–è¶…è¿‡15%çš„æƒ…å†µï¼Œä¸”è¯¥æ¸ é“å æ¯”è¶…è¿‡1%\n"
    report_content += "- **äººç¾¤ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–è¶…è¿‡10%çš„æƒ…å†µ\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/structure_check_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"ç»“æ„æ£€æŸ¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“‹ ç»“æ„æ£€æŸ¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # æ›´æ–°çŠ¶æ€
    state["structure_anomalies"] = anomalies
    state["structure_report_path"] = report_path

# åˆ›å»ºå·¥ä½œæµå›¾
def create_workflow():
    """
    åˆ›å»ºLangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ
    """
    workflow = StateGraph(WorkflowState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("anomaly_detection", anomaly_detection_node)
    workflow.add_node("structure_check", structure_check_node)
    workflow.add_node("generate_complete_report", lambda state: generate_complete_report(state) or state)
    workflow.add_node("update_readme", update_readme_mermaid_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("anomaly_detection")
    
    # æ·»åŠ è¾¹
    workflow.add_edge("anomaly_detection", "structure_check")
    workflow.add_edge("structure_check", "generate_complete_report")
    workflow.add_edge("generate_complete_report", "update_readme")
    workflow.add_edge("update_readme", END)
    
    return workflow.compile()

# ä¸»å‡½æ•°
def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œå·¥ä½œæµ
    """
    logger.info("å¯åŠ¨LangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ...")
    
    # åˆ›å»ºå·¥ä½œæµ
    app = create_workflow()
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "data": None,
        "data_path": "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet",
        "integrity_check_results": {},
        "errors": [],
        "status": "initialized",
        "metadata": {}
    }
    
    # è¿è¡Œå·¥ä½œæµ
    try:
        result = app.invoke(initial_state)
        
        if result["status"] == "failed":
            logger.error("å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            for error in result["errors"]:
                logger.error(f"é”™è¯¯: {error}")
        else:
            logger.info("å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
            logger.info(f"æœ€ç»ˆçŠ¶æ€: {result['status']}")
            
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    main()
