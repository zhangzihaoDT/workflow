#!/usr/bin/env python3
"""
LangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµè„šæœ¬
ç”¨äºå¤„ç†æ•°æ®é›†å¹¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import pandas as pd
import pyarrow.parquet as pq
from typing import Dict, Any, List
from langgraph.graph import StateGraph, END
import logging
from pathlib import Path
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å®šä¹‰å·¥ä½œæµçŠ¶æ€ç±»å‹
from typing import TypedDict

class WorkflowState(TypedDict):
    data: pd.DataFrame
    data_path: str
    integrity_check_results: Dict[str, Any]
    errors: List[str]
    status: str
    metadata: Dict[str, Any]

# ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
def generate_anomaly_report(state: WorkflowState, data_completeness: float, duplicate_rows: int, 
                           numeric_columns: list, categorical_columns: list, datetime_columns: list, 
                           missing_values: pd.Series, missing_cells: int) -> None:
    """
    ç”Ÿæˆå¼‚å¸¸æ£€æµ‹MDæŠ¥å‘Š
    """
    report_content = f"""# å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
- **æ•°æ®æ–‡ä»¶**: {state['data_path']}
- **ç”Ÿæˆæ—¶é—´**: {state['metadata']['processing_timestamp']}
- **æ–‡ä»¶å¤§å°**: {state['metadata']['file_size_mb']:.2f} MB

## æ•°æ®åŸºæœ¬ä¿¡æ¯
- **æ•°æ®å½¢çŠ¶**: {state['data'].shape[0]} è¡Œ Ã— {state['data'].shape[1]} åˆ—
- **æ•°æ®å®Œæ•´æ€§**: {data_completeness:.2f}%
- **é‡å¤è¡Œæ•°**: {duplicate_rows}

## æ•°æ®ç±»å‹åˆ†å¸ƒ
- **æ•°å€¼åˆ—**: {len(numeric_columns)} ä¸ª
- **åˆ†ç±»åˆ—**: {len(categorical_columns)} ä¸ª  
- **æ—¥æœŸåˆ—**: {len(datetime_columns)} ä¸ª

## åˆ—ä¿¡æ¯è¯¦æƒ…
### æ•°å€¼åˆ—
{', '.join(numeric_columns) if numeric_columns else 'æ— '}

### åˆ†ç±»åˆ—
{', '.join(categorical_columns) if categorical_columns else 'æ— '}

### æ—¥æœŸåˆ—
{', '.join(datetime_columns) if datetime_columns else 'æ— '}

## å¼‚å¸¸æ£€æµ‹ç»“æœ
"""
    
    if missing_cells > 0:
        report_content += "\n### ç¼ºå¤±å€¼å¼‚å¸¸\n\n| åˆ—å | ç¼ºå¤±æ•°é‡ | ç¼ºå¤±æ¯”ä¾‹ |\n|------|----------|----------|\n"
        for col, missing_count in missing_values.items():
            if missing_count > 0:
                missing_pct = missing_count / len(state["data"]) * 100
                report_content += f"| {col} | {missing_count} | {missing_pct:.2f}% |\n"
    else:
        report_content += "\n### ç¼ºå¤±å€¼æ£€æŸ¥\nâœ… æœªå‘ç°ç¼ºå¤±å€¼\n"
    
    if duplicate_rows > 0:
        report_content += f"\n### é‡å¤æ•°æ®å¼‚å¸¸\nâš ï¸ å‘ç° {duplicate_rows} è¡Œé‡å¤æ•°æ®\n"
    else:
        report_content += "\n### é‡å¤æ•°æ®æ£€æŸ¥\nâœ… æœªå‘ç°é‡å¤æ•°æ®\n"
    
    # æ•°æ®è´¨é‡è¯„çº§
    if data_completeness >= 95:
        quality_grade = "ä¼˜ç§€ âœ…"
    elif data_completeness >= 90:
        quality_grade = "è‰¯å¥½ âš ï¸"
    elif data_completeness >= 80:
        quality_grade = "ä¸€èˆ¬ âš ï¸"
    else:
        quality_grade = "è¾ƒå·® âŒ"
    
    report_content += f"\n## æ•°æ®è´¨é‡è¯„ä¼°\n- **æ•´ä½“è¯„çº§**: {quality_grade}\n- **å®Œæ•´æ€§å¾—åˆ†**: {data_completeness:.2f}%\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/anomaly_detection_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“Š å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

def generate_complete_report(state: WorkflowState) -> None:
    """
    ç”Ÿæˆå®Œæ•´çš„ç»¼åˆæŠ¥å‘Šï¼Œæ•´åˆå¼‚å¸¸æ£€æµ‹å’Œç»“æ„æ£€æŸ¥ç»“æœ
    """
    from datetime import datetime
    import os
    
    try:
        # è¯»å–å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
        anomaly_report_path = "/Users/zihao_/Documents/github/W35_workflow/anomaly_detection_report.md"
        structure_report_path = "/Users/zihao_/Documents/github/W35_workflow/structure_check_report.md"
        
        anomaly_content = ""
        structure_content = ""
        
        if os.path.exists(anomaly_report_path):
            with open(anomaly_report_path, 'r', encoding='utf-8') as f:
                anomaly_content = f.read()
        
        if os.path.exists(structure_report_path):
            with open(structure_report_path, 'r', encoding='utf-8') as f:
                structure_content = f.read()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        complete_report = f"""# W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµ - ç»¼åˆåˆ†ææŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}
- **å·¥ä½œæµç‰ˆæœ¬**: W35 Anomaly Detection Workflow
- **åˆ†æèŒƒå›´**: æ•°æ®è´¨é‡æ£€æµ‹ + ç»“æ„å¼‚å¸¸åˆ†æ

---

{anomaly_content}

---

{structure_content}

---

## ç»¼åˆç»“è®º

### æ•°æ®è´¨é‡çŠ¶å†µ
ä»æ•°æ®è´¨é‡æ£€æµ‹ç»“æœæ¥çœ‹ï¼Œæ•°æ®çš„åŸºæœ¬å®Œæ•´æ€§å’Œä¸€è‡´æ€§æƒ…å†µã€‚

### ç»“æ„å¼‚å¸¸çŠ¶å†µ
ä»ç»“æ„æ£€æŸ¥ç»“æœæ¥çœ‹ï¼ŒCM2è½¦å‹ç›¸å¯¹äºå†å²è½¦å‹åœ¨åœ°åŒºåˆ†å¸ƒã€æ¸ é“ç»“æ„ã€äººç¾¤ç»“æ„æ–¹é¢çš„å˜åŒ–æƒ…å†µã€‚

### å»ºè®®æªæ–½
1. **æ•°æ®è´¨é‡æ–¹é¢**: æ ¹æ®å¼‚å¸¸æ£€æµ‹ç»“æœï¼Œå¯¹å‘ç°çš„æ•°æ®è´¨é‡é—®é¢˜è¿›è¡Œç›¸åº”å¤„ç†
2. **ç»“æ„å¼‚å¸¸æ–¹é¢**: å¯¹å‘ç°çš„ç»“æ„å¼‚å¸¸è¿›è¡Œæ·±å…¥åˆ†æï¼Œç¡®å®šæ˜¯å¦ä¸ºæ­£å¸¸çš„ä¸šåŠ¡å˜åŒ–æˆ–éœ€è¦å…³æ³¨çš„å¼‚å¸¸æƒ…å†µ
3. **æŒç»­ç›‘æ§**: å»ºè®®å®šæœŸè¿è¡Œæ­¤å·¥ä½œæµï¼ŒæŒç»­ç›‘æ§æ•°æ®è´¨é‡å’Œç»“æ„å˜åŒ–

---

*æœ¬æŠ¥å‘Šç”± W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        complete_report_path = "/Users/zihao_/Documents/github/W35_workflow/complete_analysis_report.md"
        with open(complete_report_path, 'w', encoding='utf-8') as f:
            f.write(complete_report)
        
        logger.info(f"ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {complete_report_path}")
        print(f"\nğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {complete_report_path}")
        
        # æ›´æ–°çŠ¶æ€
        state["complete_report_path"] = complete_report_path
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆç»¼åˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        print(f"âŒ {error_msg}")

# æ›´æ–°READMEä¸­çš„mermaidå›¾ç¤ºèŠ‚ç‚¹
def update_readme_mermaid_node(state: WorkflowState) -> WorkflowState:
    """
    æ›´æ–°README.mdä¸­çš„mermaidå·¥ä½œæµå›¾ç¤º
    """
    logger.info("å¼€å§‹æ›´æ–°README.mdä¸­çš„mermaidå›¾ç¤º...")
    
    try:
        mermaid_content = """# W35 å¼‚å¸¸æ£€æµ‹å·¥ä½œæµ

## å·¥ä½œæµæ¶æ„

```mermaid
flowchart TD
    A[å¼€å§‹] --> B[å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹]
    B --> C[ç»“æ„æ£€æŸ¥èŠ‚ç‚¹]
    C --> D[ç”Ÿæˆç»¼åˆæŠ¥å‘Š]
    D --> E[æ›´æ–°READMEå›¾ç¤º]
    E --> F[ç»“æŸ]
    
    B --> B1[æ•°æ®è´¨é‡æ£€æŸ¥]
    B --> B2[ç¼ºå¤±å€¼æ£€æµ‹]
    B --> B3[é‡å¤æ•°æ®æ£€æµ‹]
    B --> B4[ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š]
    
    C --> C1[åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æŸ¥]
    C --> C2[æ¸ é“ç»“æ„å¼‚å¸¸æ£€æŸ¥]
    C --> C3[äººç¾¤ç»“æ„å¼‚å¸¸æ£€æŸ¥]
    C --> C4[ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š]
    
    D --> D1[æ•´åˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š]
    D --> D2[æ•´åˆç»“æ„æ£€æŸ¥æŠ¥å‘Š]
    D --> D3[ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#ffebee
    style F fill:#f1f8e9
```

## åŠŸèƒ½è¯´æ˜

### å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹
- æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
- æ•°æ®è¯»å–éªŒè¯
- åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
- ç¼ºå¤±å€¼æ£€æµ‹ä¸åˆ†æ
- é‡å¤æ•°æ®æ£€æµ‹
- æ•°æ®ç±»å‹åˆ†å¸ƒåˆ†æ
- å¼‚å¸¸å€¼è¯†åˆ«

### ç»“æ„æ£€æŸ¥èŠ‚ç‚¹
- åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹ï¼šå¯¹æ¯”CM2è½¦å‹ä¸å†å²è½¦å‹çš„åœ°åŒºè®¢å•åˆ†å¸ƒå˜åŒ–
- æ¸ é“ç»“æ„å¼‚å¸¸æ£€æµ‹ï¼šåˆ†æå„æ¸ é“é”€é‡å æ¯”çš„çªå˜æƒ…å†µ
- äººç¾¤ç»“æ„å¼‚å¸¸æ£€æµ‹ï¼šæ£€æµ‹æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„çš„å¤§å¹…å˜åŒ–
- åŸºäºä¸šåŠ¡å®šä¹‰çš„æ—¶é—´èŒƒå›´è¿›è¡Œæ•°æ®ç­›é€‰

### ç»¼åˆæŠ¥å‘Šç”Ÿæˆ
- æ•´åˆå¼‚å¸¸æ£€æµ‹å’Œç»“æ„æ£€æŸ¥ç»“æœ
- æä¾›æ•°æ®è´¨é‡å’Œç»“æ„å¼‚å¸¸çš„ç»¼åˆè¯„ä¼°
- åŸºäºæ£€æµ‹ç»“æœæä¾›ç›¸åº”çš„å¤„ç†å»ºè®®

### æŠ¥å‘Šç”Ÿæˆ
- è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„MDæ ¼å¼å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
- ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
- ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
- åŒ…å«æ•°æ®è´¨é‡è¯„çº§
- æä¾›å¯è§†åŒ–çš„å¼‚å¸¸ç»Ÿè®¡ä¿¡æ¯

### å·¥ä½œæµç‰¹æ€§
- åŸºäºLangGraphæ¡†æ¶æ„å»º
- æ¨¡å—åŒ–èŠ‚ç‚¹è®¾è®¡
- å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹ï¼ˆæ•°æ®è´¨é‡+ä¸šåŠ¡ç»“æ„ï¼‰
- å®Œæ•´çš„é”™è¯¯å¤„ç†æœºåˆ¶
- è¯¦ç»†çš„æ—¥å¿—è®°å½•

## ä½¿ç”¨æ–¹æ³•

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# è¿è¡Œå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ
python main.py
```

## è¾“å‡ºæ–‡ä»¶
- `anomaly_detection_report.md`: è¯¦ç»†çš„å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
- `structure_check_report.md`: ç»“æ„æ£€æŸ¥è¯¦ç»†æŠ¥å‘Š
- `complete_analysis_report.md`: ç»¼åˆåˆ†ææŠ¥å‘Š
- æ§åˆ¶å°æ—¥å¿—: å®æ—¶å¤„ç†çŠ¶æ€ä¿¡æ¯

## é…ç½®æ–‡ä»¶
- `business_definition.json`: ä¸šåŠ¡æ—¶é—´èŒƒå›´å®šä¹‰ï¼Œç”¨äºç»“æ„æ£€æŸ¥çš„æ•°æ®ç­›é€‰
"""
        
        readme_path = "/Users/zihao_/Documents/github/W35_workflow/README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(mermaid_content)
        
        state["status"] = "readme_updated"
        logger.info("README.mdä¸­çš„mermaidå›¾ç¤ºæ›´æ–°å®Œæˆ")
        print(f"\nğŸ“ README.mdå·²æ›´æ–°: {readme_path}")
        
    except Exception as e:
        error_msg = f"æ›´æ–°README.mdè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹
def anomaly_detection_node(state: WorkflowState) -> WorkflowState:
    """
    å¼‚å¸¸æ£€æµ‹èŠ‚ç‚¹ - å·¥ä½œæµçš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
    æ£€æŸ¥æ•°æ®é›†çš„å¼‚å¸¸æƒ…å†µï¼ŒåŒ…æ‹¬ï¼š
    1. æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    2. æ•°æ®æ˜¯å¦å¯ä»¥æ­£å¸¸è¯»å–
    3. æ•°æ®åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    4. ç¼ºå¤±å€¼æ£€æŸ¥
    5. æ•°æ®ç±»å‹æ£€æŸ¥
    6. å¼‚å¸¸å€¼æ£€æµ‹
    """
    logger.info("å¼€å§‹æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
    
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        data_path = Path(state["data_path"])
        if not data_path.exists():
            error_msg = f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {state['data_path']}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["status"] = "failed"
            return state
        
        logger.info(f"æ•°æ®æ–‡ä»¶å­˜åœ¨: {state['data_path']}")
        
        # 2. è¯»å–æ•°æ®
        try:
            state["data"] = pd.read_parquet(state["data_path"])
            logger.info(f"æˆåŠŸè¯»å–æ•°æ®ï¼Œæ•°æ®å½¢çŠ¶: {state['data'].shape}")
        except Exception as e:
            error_msg = f"è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            state["errors"].append(error_msg)
            state["status"] = "failed"
            return state
        
        # 3. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        state["integrity_check_results"]["shape"] = state["data"].shape
        state["integrity_check_results"]["columns"] = list(state["data"].columns)
        state["integrity_check_results"]["dtypes"] = state["data"].dtypes.to_dict()
        
        # 4. ç¼ºå¤±å€¼æ£€æŸ¥
        missing_values = state["data"].isnull().sum()
        state["integrity_check_results"]["missing_values"] = missing_values.to_dict()
        state["integrity_check_results"]["missing_percentage"] = (missing_values / len(state["data"]) * 100).to_dict()
        
        # 5. æ•°æ®ç±»å‹åˆ†å¸ƒ
        numeric_columns = state["data"].select_dtypes(include=['number']).columns.tolist()
        categorical_columns = state["data"].select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_columns = state["data"].select_dtypes(include=['datetime']).columns.tolist()
        
        state["integrity_check_results"]["numeric_columns"] = numeric_columns
        state["integrity_check_results"]["categorical_columns"] = categorical_columns
        state["integrity_check_results"]["datetime_columns"] = datetime_columns
        
        # 6. æ•°æ®è´¨é‡è¯„ä¼°
        total_cells = state["data"].shape[0] * state["data"].shape[1]
        missing_cells = state["data"].isnull().sum().sum()
        data_completeness = (total_cells - missing_cells) / total_cells * 100
        
        state["integrity_check_results"]["data_completeness_percentage"] = data_completeness
        
        # 7. åŸºæœ¬æè¿°æ€§ç»Ÿè®¡
        if numeric_columns:
            state["integrity_check_results"]["numeric_summary"] = state["data"][numeric_columns].describe().to_dict()
        
        # 8. é‡å¤è¡Œæ£€æŸ¥
        duplicate_rows = state["data"].duplicated().sum()
        state["integrity_check_results"]["duplicate_rows"] = duplicate_rows
        
        # è®°å½•å…ƒæ•°æ®
        state["metadata"]["file_size_mb"] = data_path.stat().st_size / (1024 * 1024)
        state["metadata"]["processing_timestamp"] = pd.Timestamp.now().isoformat()
        
        state["status"] = "anomaly_detection_completed"
        logger.info("å¼‚å¸¸æ£€æµ‹å®Œæˆ")
        
        # ç”ŸæˆMDæŠ¥å‘Š
        generate_anomaly_report(state, data_completeness, duplicate_rows, numeric_columns, categorical_columns, datetime_columns, missing_values, missing_cells)
        
    except Exception as e:
        error_msg = f"å¼‚å¸¸æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# ç»“æ„æ£€æŸ¥èŠ‚ç‚¹
def structure_check_node(state: WorkflowState) -> WorkflowState:
    """
    ç»“æ„æ£€æŸ¥èŠ‚ç‚¹ï¼šåˆ†æCM2è½¦å‹ç›¸å¯¹äºå†å²è½¦å‹çš„ç»“æ„å¼‚å¸¸
    """
    logger.info("å¼€å§‹æ‰§è¡Œç»“æ„æ£€æŸ¥...")
    
    try:
        # è¯»å–ä¸šåŠ¡å®šä¹‰æ–‡ä»¶
        business_def_path = "/Users/zihao_/Documents/github/W35_workflow/business_definition.json"
        with open(business_def_path, 'r', encoding='utf-8') as f:
            import json
            presale_periods = json.load(f)
        
        # è¯»å–æ•°æ®
        data_path = "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet"
        df = pd.read_parquet(data_path)
        
        # è½¬æ¢æ—¶é—´åˆ—
        df['Intention_Payment_Time'] = pd.to_datetime(df['Intention_Payment_Time'])
        
        # ç­›é€‰å„è½¦å‹æ•°æ®
        vehicle_data = {}
        for vehicle, period in presale_periods.items():
            start_date = pd.to_datetime(period['start'])
            end_date = pd.to_datetime(period['end'])
            mask = (df['Intention_Payment_Time'] >= start_date) & (df['Intention_Payment_Time'] <= end_date)
            vehicle_data[vehicle] = df[mask].copy()
        
        # åˆ†æç»“æœ
        anomalies = []
        
        # 1. åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æŸ¥
        region_anomalies = analyze_region_distribution(vehicle_data)
        if region_anomalies:
            anomalies.extend(region_anomalies)
        
        # 2. æ¸ é“ç»“æ„å¼‚å¸¸æ£€æŸ¥
        channel_anomalies = analyze_channel_structure(vehicle_data)
        if channel_anomalies:
            anomalies.extend(channel_anomalies)
        
        # 3. äººç¾¤ç»“æ„å¼‚å¸¸æ£€æŸ¥
        demographic_anomalies = analyze_demographic_structure(vehicle_data)
        if demographic_anomalies:
            anomalies.extend(demographic_anomalies)
        
        # 4. åŒæ¯”/ç¯æ¯”å¼‚å¸¸æ£€æŸ¥
        time_series_anomalies = analyze_time_series_anomalies(vehicle_data, presale_periods)
        if time_series_anomalies:
            anomalies.extend(time_series_anomalies)
        
        # å°†presale_periodsæ·»åŠ åˆ°stateä¸­
        state['presale_periods'] = presale_periods
        
        # ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
        try:
            generate_structure_report(state, vehicle_data, anomalies)
        except Exception as report_error:
    
            import traceback
            traceback.print_exc()
            raise report_error
        
        state["status"] = "structure_check_completed"
        logger.info("ç»“æ„æ£€æŸ¥å®Œæˆ")
        
    except Exception as e:
        error_msg = f"ç»“æ„æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state

# åœ°åŒºåˆ†å¸ƒå¼‚å¸¸åˆ†æ
def analyze_region_distribution(vehicle_data):
    """
    åˆ†æåœ°åŒºåˆ†å¸ƒå¼‚å¸¸
    """
    anomalies = []
    
    # è·å–CM2æ•°æ®
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåœ°åŒºåˆ†å¸ƒåˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    for region_col in ['Parent Region Name', 'License Province', 'license_city_level', 'License City']:
        if region_col not in cm2_data.columns:
            continue
            
        # CM2åœ°åŒºåˆ†å¸ƒ
        cm2_region_dist = cm2_data[region_col].value_counts(normalize=True)
        
        # å†å²è½¦å‹å¹³å‡åˆ†å¸ƒ
        historical_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                hist_dist = vehicle_data[vehicle][region_col].value_counts(normalize=True)
                historical_dists.append(hist_dist)
        
        if historical_dists:
            # è®¡ç®—å†å²å¹³å‡åˆ†å¸ƒ
            all_regions = set()
            for dist in historical_dists:
                all_regions.update(dist.index)
            all_regions.update(cm2_region_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_regions))
            for dist in historical_dists:
                for region in all_regions:
                    avg_historical[region] += dist.get(region, 0) / len(historical_dists)
            
            # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡20%ï¼‰
            for region in all_regions:
                cm2_ratio = cm2_region_dist.get(region, 0)
                hist_ratio = avg_historical.get(region, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.2:  # 20%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºè®¢å•å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
                elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„åœ°åŒºï¼Œå æ¯”è¶…è¿‡1%
                    anomalies.append(f"[å†å²å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºä¸ºæ–°å‡ºç°åŒºåŸŸï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼Œå†å²æ— æ•°æ®")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0:
        for region_col in ['Parent Region Name', 'License Province', 'license_city_level', 'License City']:
            if region_col not in cm2_data.columns or region_col not in cm1_data.columns:
                continue
                
            # CM2å’ŒCM1åœ°åŒºåˆ†å¸ƒ
            cm2_region_dist = cm2_data[region_col].value_counts(normalize=True)
            cm1_region_dist = cm1_data[region_col].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰åœ°åŒº
            all_regions = set(cm2_region_dist.index) | set(cm1_region_dist.index)
            
            # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡20%ï¼‰
            for region in all_regions:
                cm2_ratio = cm2_region_dist.get(region, 0)
                cm1_ratio = cm1_region_dist.get(region, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.2:  # 20%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºè®¢å•å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
                elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„åœ°åŒºï¼Œå æ¯”è¶…è¿‡1%
                    anomalies.append(f"[CM1å¯¹æ¯”]{region_col}ä¸­{region}åœ°åŒºä¸ºæ–°å‡ºç°åŒºåŸŸï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼ŒCM1æ— æ•°æ®")
    
    return anomalies

# æ¸ é“ç»“æ„å¼‚å¸¸åˆ†æ
def analyze_channel_structure(vehicle_data):
    """
    åˆ†ææ¸ é“ç»“æ„å¼‚å¸¸
    """
    anomalies = []
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ¸ é“ç»“æ„åˆ†æ"]
    
    channel_col = 'first_middle_channel_name'
    
    if channel_col not in cm2_data.columns:
        return [f"ç¼ºå°‘{channel_col}åˆ—ï¼Œæ— æ³•è¿›è¡Œæ¸ é“åˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    # CM2æ¸ é“åˆ†å¸ƒ
    cm2_channel_dist = cm2_data[channel_col].value_counts(normalize=True)
    
    # å†å²æ¸ é“åˆ†å¸ƒ
    historical_dists = []
    for vehicle in historical_vehicles:
        if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
            hist_dist = vehicle_data[vehicle][channel_col].value_counts(normalize=True)
            historical_dists.append(hist_dist)
    
    if historical_dists:
        # è®¡ç®—å†å²å¹³å‡åˆ†å¸ƒ
        all_channels = set()
        for dist in historical_dists:
            all_channels.update(dist.index)
        all_channels.update(cm2_channel_dist.index)
        
        avg_historical = pd.Series(0.0, index=list(all_channels))
        for dist in historical_dists:
            for channel in all_channels:
                avg_historical[channel] += dist.get(channel, 0) / len(historical_dists)
        
        # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
        for channel in all_channels:
            cm2_ratio = cm2_channel_dist.get(channel, 0)
            hist_ratio = avg_historical.get(channel, 0)
            
            # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
            if hist_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                if change_rate > 0.15:  # 15%å˜åŒ–å¹…åº¦é˜ˆå€¼
                    change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                    anomalies.append(f"[å†å²å¯¹æ¯”]æ¸ é“{channel}é”€é‡å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
            elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„æ¸ é“ï¼Œå æ¯”è¶…è¿‡1%
                anomalies.append(f"[å†å²å¯¹æ¯”]æ¸ é“{channel}ä¸ºæ–°å‡ºç°æ¸ é“ï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼Œå†å²æ— æ•°æ®")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0 and channel_col in cm1_data.columns:
        # CM2å’ŒCM1æ¸ é“åˆ†å¸ƒ
        cm2_channel_dist = cm2_data[channel_col].value_counts(normalize=True)
        cm1_channel_dist = cm1_data[channel_col].value_counts(normalize=True)
        
        # è·å–æ‰€æœ‰æ¸ é“
        all_channels = set(cm2_channel_dist.index) | set(cm1_channel_dist.index)
        
        # æ£€æŸ¥å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
        for channel in all_channels:
            cm2_ratio = cm2_channel_dist.get(channel, 0)
            cm1_ratio = cm1_channel_dist.get(channel, 0)
            
            # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
            if cm1_ratio > 0 and cm2_ratio > 0.01:  # å¢åŠ å æ¯”è¶…è¿‡1%çš„æ¡ä»¶
                change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                if change_rate > 0.15:  # 15%å˜åŒ–å¹…åº¦é˜ˆå€¼
                    change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                    anomalies.append(f"[CM1å¯¹æ¯”]æ¸ é“{channel}é”€é‡å æ¯”å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
            elif cm2_ratio > 0.01:  # æ–°å‡ºç°çš„æ¸ é“ï¼Œå æ¯”è¶…è¿‡1%
                anomalies.append(f"[CM1å¯¹æ¯”]æ¸ é“{channel}ä¸ºæ–°å‡ºç°æ¸ é“ï¼šCM2å æ¯”{cm2_ratio:.2%}ï¼ŒCM1æ— æ•°æ®")
    
    return anomalies

# äººç¾¤ç»“æ„å¼‚å¸¸åˆ†æ
def analyze_demographic_structure(vehicle_data):
    """
    åˆ†æäººç¾¤ç»“æ„å¼‚å¸¸
    """
    anomalies = []
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œäººç¾¤ç»“æ„åˆ†æ"]
    
    # 1. CM2 vs å†å²å¹³å‡å¼‚å¸¸æ£€æµ‹
    historical_vehicles = ['CM0', 'DM0', 'CM1', 'DM1']
    
    # 1.1 æ€§åˆ«æ¯”ä¾‹åˆ†æ
    if 'order_gender' in cm2_data.columns:
        cm2_gender_dist = cm2_data['order_gender'].value_counts(normalize=True)
        
        historical_gender_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                hist_dist = vehicle_data[vehicle]['order_gender'].value_counts(normalize=True)
                historical_gender_dists.append(hist_dist)
        
        if historical_gender_dists:
            all_genders = set()
            for dist in historical_gender_dists:
                all_genders.update(dist.index)
            all_genders.update(cm2_gender_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_genders))
            for dist in historical_gender_dists:
                for gender in all_genders:
                    avg_historical[gender] += dist.get(gender, 0) / len(historical_gender_dists)
            
            # æ£€æŸ¥æ€§åˆ«æ¯”ä¾‹å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for gender in all_genders:
                cm2_ratio = cm2_gender_dist.get(gender, 0)
                hist_ratio = avg_historical.get(gender, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0:
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]æ€§åˆ«{gender}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    # 1.2 å¹´é¾„æ®µç»“æ„åˆ†æ
    if 'buyer_age' in cm2_data.columns:
        # å®šä¹‰å¹´é¾„æ®µ
        def age_group(age):
            if pd.isna(age):
                return 'æœªçŸ¥'
            elif age < 25:
                return '25å²ä»¥ä¸‹'
            elif age < 35:
                return '25-34å²'
            elif age < 45:
                return '35-44å²'
            elif age < 55:
                return '45-54å²'
            else:
                return '55å²ä»¥ä¸Š'
        
        cm2_data_copy = cm2_data.copy()
        cm2_data_copy['age_group'] = cm2_data_copy['buyer_age'].apply(age_group)
        cm2_age_dist = cm2_data_copy['age_group'].value_counts(normalize=True)
        
        historical_age_dists = []
        for vehicle in historical_vehicles:
            if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                vehicle_data_copy = vehicle_data[vehicle].copy()
                vehicle_data_copy['age_group'] = vehicle_data_copy['buyer_age'].apply(age_group)
                hist_dist = vehicle_data_copy['age_group'].value_counts(normalize=True)
                historical_age_dists.append(hist_dist)
        
        if historical_age_dists:
            all_age_groups = set()
            for dist in historical_age_dists:
                all_age_groups.update(dist.index)
            all_age_groups.update(cm2_age_dist.index)
            
            avg_historical = pd.Series(0.0, index=list(all_age_groups))
            for dist in historical_age_dists:
                for age_group_name in all_age_groups:
                    avg_historical[age_group_name] += dist.get(age_group_name, 0) / len(historical_age_dists)
            
            # æ£€æŸ¥å¹´é¾„æ®µå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for age_group_name in all_age_groups:
                cm2_ratio = cm2_age_dist.get(age_group_name, 0)
                hist_ratio = avg_historical.get(age_group_name, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if hist_ratio > 0:
                    change_rate = abs(cm2_ratio - hist_ratio) / hist_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > hist_ratio else "ä¸‹é™"
                        anomalies.append(f"[å†å²å¯¹æ¯”]å¹´é¾„æ®µ{age_group_name}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼Œå†å²å¹³å‡ä¸º{hist_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    # 2. CM2 vs CM1ç›´æ¥å¯¹æ¯”å¼‚å¸¸æ£€æµ‹
    cm1_data = vehicle_data.get('CM1')
    if cm1_data is not None and len(cm1_data) > 0:
        # 2.1 æ€§åˆ«æ¯”ä¾‹åˆ†æ
        if 'order_gender' in cm2_data.columns and 'order_gender' in cm1_data.columns:
            cm2_gender_dist = cm2_data['order_gender'].value_counts(normalize=True)
            cm1_gender_dist = cm1_data['order_gender'].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰æ€§åˆ«
            all_genders = set(cm2_gender_dist.index) | set(cm1_gender_dist.index)
            
            # æ£€æŸ¥æ€§åˆ«æ¯”ä¾‹å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for gender in all_genders:
                cm2_ratio = cm2_gender_dist.get(gender, 0)
                cm1_ratio = cm1_gender_dist.get(gender, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0:
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]æ€§åˆ«{gender}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
        
        # 2.2 å¹´é¾„æ®µç»“æ„åˆ†æ
        if 'buyer_age' in cm2_data.columns and 'buyer_age' in cm1_data.columns:
            # å®šä¹‰å¹´é¾„æ®µ
            def age_group(age):
                if pd.isna(age):
                    return 'æœªçŸ¥'
                elif age < 25:
                    return '25å²ä»¥ä¸‹'
                elif age < 35:
                    return '25-34å²'
                elif age < 45:
                    return '35-44å²'
                elif age < 55:
                    return '45-54å²'
                else:
                    return '55å²ä»¥ä¸Š'
            
            cm2_data_copy = cm2_data.copy()
            cm2_data_copy['age_group'] = cm2_data_copy['buyer_age'].apply(age_group)
            cm2_age_dist = cm2_data_copy['age_group'].value_counts(normalize=True)
            
            cm1_data_copy = cm1_data.copy()
            cm1_data_copy['age_group'] = cm1_data_copy['buyer_age'].apply(age_group)
            cm1_age_dist = cm1_data_copy['age_group'].value_counts(normalize=True)
            
            # è·å–æ‰€æœ‰å¹´é¾„æ®µ
            all_age_groups = set(cm2_age_dist.index) | set(cm1_age_dist.index)
            
            # æ£€æŸ¥å¹´é¾„æ®µå¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
            for age_group_name in all_age_groups:
                cm2_ratio = cm2_age_dist.get(age_group_name, 0)
                cm1_ratio = cm1_age_dist.get(age_group_name, 0)
                
                # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆç›¸å¯¹å˜åŒ–ç‡ï¼‰
                if cm1_ratio > 0:
                    change_rate = abs(cm2_ratio - cm1_ratio) / cm1_ratio
                    if change_rate > 0.1:  # 10%å˜åŒ–å¹…åº¦é˜ˆå€¼
                        change_direction = "å¢é•¿" if cm2_ratio > cm1_ratio else "ä¸‹é™"
                        anomalies.append(f"[CM1å¯¹æ¯”]å¹´é¾„æ®µ{age_group_name}æ¯”ä¾‹å¼‚å¸¸{change_direction}ï¼šCM2ä¸º{cm2_ratio:.2%}ï¼ŒCM1ä¸º{cm1_ratio:.2%}ï¼Œå˜åŒ–å¹…åº¦{change_rate:.1%}")
    
    return anomalies

# åŒæ¯”/ç¯æ¯”å¼‚å¸¸åˆ†æ
def analyze_time_series_anomalies(vehicle_data, presale_periods):
    """
    åˆ†æCM2è½¦å‹çš„åŒæ¯”/ç¯æ¯”å¼‚å¸¸
    åŒ…æ‹¬ï¼šæ—¥ç¯æ¯”ã€åŒå‘¨æœŸå¯¹æ¯”ã€ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”
    """
    anomalies = []
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return ["CM2è½¦å‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒæ¯”/ç¯æ¯”åˆ†æ"]
    
    # è·å–CM2çš„èµ·å§‹æ—¥æœŸ
    cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
    
    # å‡†å¤‡CM2æ¯æ—¥è®¢å•æ•°æ®
    cm2_data_copy = cm2_data.copy()
    cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
    cm2_daily = cm2_data_copy.groupby('date').size().reset_index(name='orders')
    cm2_daily['date'] = pd.to_datetime(cm2_daily['date'])
    cm2_daily = cm2_daily.sort_values('date')
    
    # 1. æ—¥ç¯æ¯”å¼‚å¸¸æ£€æŸ¥ï¼ˆCM2å†…éƒ¨å¯¹æ¯”ï¼‰
    for i in range(1, len(cm2_daily)):
        current_orders = cm2_daily.iloc[i]['orders']
        previous_orders = cm2_daily.iloc[i-1]['orders']
        current_date = cm2_daily.iloc[i]['date']
        
        if previous_orders > 0:
            change_rate = (current_orders - previous_orders) / previous_orders
            if abs(change_rate) > 0.5:  # 50%é˜ˆå€¼
                change_type = "éª¤å¢" if change_rate > 0 else "éª¤é™"
                anomalies.append(f"[æ—¥ç¯æ¯”]{current_date.strftime('%Y-%m-%d')}è®¢å•é‡å¼‚å¸¸{change_type}ï¼šå½“æ—¥{current_orders}å•ï¼Œå‰æ—¥{previous_orders}å•ï¼Œå˜åŒ–å¹…åº¦{change_rate*100:.1f}%")
    
    # 2. åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸æ£€æŸ¥ï¼ˆCM2 vs CM0, CM1, DM0, DM1ï¼‰
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        
        # å¯¹æ¯”ç›¸åŒç›¸å¯¹å¤©æ•°çš„è®¢å•é‡
        for _, cm2_row in cm2_daily.iterrows():
            cm2_date = cm2_row['date']
            cm2_orders = cm2_row['orders']
            
            # è®¡ç®—ç›¸å¯¹äºèµ·å§‹æ—¥çš„å¤©æ•°
            days_from_start = (cm2_date - cm2_start).days
            
            # æ‰¾åˆ°å¯¹åº”çš„å†å²è½¦å‹æ—¥æœŸ
            target_date = vehicle_start + pd.Timedelta(days=days_from_start)
            
            # æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„è®¢å•é‡
            vehicle_orders_on_date = vehicle_daily[vehicle_daily['date'] == target_date]
            
            if not vehicle_orders_on_date.empty:
                vehicle_orders = vehicle_orders_on_date.iloc[0]['orders']
                
                if vehicle_orders > 0:
                    change_rate = (cm2_orders - vehicle_orders) / vehicle_orders
                    if abs(change_rate) > 1.0:  # 100%é˜ˆå€¼
                        change_type = "éª¤å¢" if change_rate > 0 else "éª¤é™"
                        anomalies.append(f"[åŒå‘¨æœŸå¯¹æ¯”]CM2åœ¨{cm2_date.strftime('%Y-%m-%d')}ç›¸å¯¹{vehicle}åŒæœŸå¼‚å¸¸{change_type}ï¼šCM2ä¸º{cm2_orders}å•ï¼Œ{vehicle}åŒæœŸä¸º{vehicle_orders}å•ï¼Œå˜åŒ–å¹…åº¦{change_rate*100:.1f}%")
    
    # 3. ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸æ£€æŸ¥
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        
        # è®¡ç®—ç´¯è®¡è®¢å•é‡å¯¹æ¯”
        for i, cm2_row in cm2_daily.iterrows():
            cm2_date = cm2_row['date']
            
            # è®¡ç®—ç›¸å¯¹äºèµ·å§‹æ—¥çš„å¤©æ•°
            days_from_start = (cm2_date - cm2_start).days
            
            # CM2ç´¯è®¡è®¢å•é‡ï¼ˆä»èµ·å§‹æ—¥åˆ°å½“å‰æ—¥ï¼‰
            cm2_cumulative = cm2_daily[cm2_daily['date'] <= cm2_date]['orders'].sum()
            
            # å†å²è½¦å‹å¯¹åº”æœŸé—´çš„ç´¯è®¡è®¢å•é‡
            target_end_date = vehicle_start + pd.Timedelta(days=days_from_start)
            vehicle_cumulative_data = vehicle_daily[
                (vehicle_daily['date'] >= vehicle_start) & 
                (vehicle_daily['date'] <= target_end_date)
            ]
            
            if not vehicle_cumulative_data.empty:
                vehicle_cumulative = vehicle_cumulative_data['orders'].sum()
                
                if vehicle_cumulative > 0:
                    change_rate = (cm2_cumulative - vehicle_cumulative) / vehicle_cumulative
                    if abs(change_rate) > 1.0:  # 100%é˜ˆå€¼
                        change_type = "éª¤å¢" if change_rate > 0 else "éª¤é™"
                        anomalies.append(f"[ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”]CM2æˆªè‡³{cm2_date.strftime('%Y-%m-%d')}ç´¯è®¡è®¢å•ç›¸å¯¹{vehicle}åŒæœŸå¼‚å¸¸{change_type}ï¼šCM2ç´¯è®¡{cm2_cumulative}å•ï¼Œ{vehicle}åŒæœŸç´¯è®¡{vehicle_cumulative}å•ï¼Œå˜åŒ–å¹…åº¦{change_rate*100:.1f}%")
    
    return anomalies

# ç”Ÿæˆæ—¥ç¯æ¯”æè¿°æ•°æ®
def generate_time_series_description(vehicle_data, presale_periods):
    """
    ç”ŸæˆCM2è½¦å‹çš„æ—¥ç¯æ¯”æè¿°æ•°æ®
    åŒ…æ‹¬ï¼šCM2æ—¥è®¢å•æ•°ã€åŒæœŸå…¶ä»–è½¦å‹æ—¥è®¢å•æ•°ã€ç´¯è®¡è®¢å•æ•°å¯¹æ¯”
    """
    description_data = {
        'cm2_daily': [],
        'cm2_cumulative': [],
        'comparison_daily': {},
        'comparison_cumulative': {}
    }
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return description_data
    
    # æ£€æŸ¥presale_periodsæ˜¯å¦æœ‰æ•ˆ
    if not presale_periods or 'CM2' not in presale_periods:
        return description_data
    
    # è·å–CM2çš„èµ·å§‹æ—¥æœŸ
    cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
    
    # å‡†å¤‡CM2æ¯æ—¥è®¢å•æ•°æ®
    cm2_data_copy = cm2_data.copy()
    cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
    cm2_daily = cm2_data_copy.groupby('date').size().reset_index(name='orders')
    cm2_daily['date'] = pd.to_datetime(cm2_daily['date'])
    cm2_daily = cm2_daily.sort_values('date')
    
    # è®¡ç®—CM2ç´¯è®¡è®¢å•æ•°
    cm2_daily['cumulative'] = cm2_daily['orders'].cumsum()
    
    # å­˜å‚¨CM2æ•°æ®ï¼ˆä»…ä¿ç•™æœ€åä¸€å¤©ï¼‰
    if not cm2_daily.empty:
        last_row = cm2_daily.iloc[-1]
        days_from_start = (last_row['date'] - cm2_start).days + 1
        description_data['cm2_daily'].append({
            'date': last_row['date'].strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'orders': last_row['orders']
        })
        description_data['cm2_cumulative'].append({
            'date': last_row['date'].strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'cumulative_orders': last_row['cumulative']
        })
    
    # å¤„ç†å…¶ä»–è½¦å‹çš„åŒæœŸæ•°æ®
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
        
        # æ£€æŸ¥è¯¥è½¦å‹çš„presale_periodsæ˜¯å¦å­˜åœ¨
        if vehicle not in presale_periods:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        vehicle_daily['cumulative'] = vehicle_daily['orders'].cumsum()
        
        description_data['comparison_daily'][vehicle] = []
        description_data['comparison_cumulative'][vehicle] = []
        
        # å¯¹æ¯”ç›¸åŒç›¸å¯¹å¤©æ•°çš„æ•°æ®ï¼ˆä»…ä¿ç•™æœ€åä¸€å¤©ï¼‰
        if not cm2_daily.empty:
            cm2_row = cm2_daily.iloc[-1]
            cm2_date = cm2_row['date']
            days_from_start = (cm2_date - cm2_start).days
            
            # æ‰¾åˆ°å¯¹åº”çš„å†å²è½¦å‹æ—¥æœŸ
            target_date = vehicle_start + pd.Timedelta(days=days_from_start)
            
            # æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„è®¢å•é‡
            vehicle_orders_on_date = vehicle_daily[vehicle_daily['date'] == target_date]
            
            if not vehicle_orders_on_date.empty:
                vehicle_orders = vehicle_orders_on_date.iloc[0]['orders']
                vehicle_cumulative = vehicle_daily[vehicle_daily['date'] <= target_date]['orders'].sum()
                
                description_data['comparison_daily'][vehicle].append({
                    'cm2_date': cm2_date.strftime('%Y-%m-%d'),
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start + 1,
                    'orders': vehicle_orders
                })
                description_data['comparison_cumulative'][vehicle].append({
                    'cm2_date': cm2_date.strftime('%Y-%m-%d'),
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start + 1,
                    'cumulative_orders': vehicle_cumulative
                })
    
    return description_data

# ç”Ÿæˆé€€è®¢æè¿°æ•°æ®
def generate_refund_description(vehicle_data, presale_periods):
    """
    ç”ŸæˆCM2è½¦å‹çš„é€€è®¢æè¿°æ•°æ®
    åŒ…æ‹¬ï¼šCM2æ—¥é€€è®¢æ•°ã€åŒæœŸå…¶ä»–è½¦å‹æ—¥é€€è®¢æ•°ã€ç´¯è®¡é€€è®¢æ•°å¯¹æ¯”
    """

    
    refund_data = {
        'cm2_daily': [],
        'cm2_cumulative': [],
        'comparison_daily': {},
        'comparison_cumulative': {}
    }
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return refund_data
    
    # æ£€æŸ¥presale_periodsæ˜¯å¦æœ‰æ•ˆ
    if not presale_periods or 'CM2' not in presale_periods:
        return refund_data
    
    # è¿‡æ»¤æœ‰é€€è®¢æ—¶é—´çš„æ•°æ®
    cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
    if len(cm2_refund_data) == 0:
        return refund_data
    
    # è·å–CM2çš„èµ·å§‹æ—¥æœŸ
    cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
    
    # å‡†å¤‡CM2æ¯æ—¥é€€è®¢æ•°æ®
    cm2_refund_data['refund_date'] = cm2_refund_data['intention_refund_time'].dt.date
    cm2_daily_refund = cm2_refund_data.groupby('refund_date').size().reset_index(name='refunds')
    cm2_daily_refund['refund_date'] = pd.to_datetime(cm2_daily_refund['refund_date'])
    cm2_daily_refund = cm2_daily_refund.sort_values('refund_date')
    
    # è®¡ç®—CM2ç´¯è®¡é€€è®¢æ•°
    cm2_daily_refund['cumulative'] = cm2_daily_refund['refunds'].cumsum()
    
    # å­˜å‚¨CM2æ•°æ®ï¼ˆä»…ä¿ç•™æœ€åä¸€å¤©ï¼‰
    if not cm2_daily_refund.empty:
        last_row = cm2_daily_refund.iloc[-1]
        days_from_start = (last_row['refund_date'] - cm2_start).days + 1
        refund_data['cm2_daily'].append({
            'date': last_row['refund_date'].strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'refunds': last_row['refunds']
        })
        refund_data['cm2_cumulative'].append({
            'date': last_row['refund_date'].strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'cumulative_refunds': last_row['cumulative']
        })
    
    # å¤„ç†å…¶ä»–è½¦å‹çš„åŒæœŸé€€è®¢æ•°æ®
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
        
        # æ£€æŸ¥è¯¥è½¦å‹çš„presale_periodsæ˜¯å¦å­˜åœ¨
        if vehicle not in presale_periods:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
        
        if len(vehicle_refund_data) == 0:
            continue
            
        vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
        vehicle_daily_refund = vehicle_refund_data.groupby('refund_date').size().reset_index(name='refunds')
        vehicle_daily_refund['refund_date'] = pd.to_datetime(vehicle_daily_refund['refund_date'])
        vehicle_daily_refund = vehicle_daily_refund.sort_values('refund_date')
        vehicle_daily_refund['cumulative'] = vehicle_daily_refund['refunds'].cumsum()
        
        refund_data['comparison_daily'][vehicle] = []
        refund_data['comparison_cumulative'][vehicle] = []
        
        # å¯¹æ¯”ç›¸åŒç›¸å¯¹å¤©æ•°çš„æ•°æ®ï¼ˆä»…ä¿ç•™æœ€åä¸€å¤©ï¼‰
        if not cm2_daily_refund.empty:
            cm2_row = cm2_daily_refund.iloc[-1]
            cm2_refund_date = cm2_row['refund_date']
            days_from_start = (cm2_refund_date - cm2_start).days
            
            # æ‰¾åˆ°å¯¹åº”çš„å†å²è½¦å‹æ—¥æœŸ
            target_date = vehicle_start + pd.Timedelta(days=days_from_start)
            
            # æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„é€€è®¢é‡
            vehicle_refunds_on_date = vehicle_daily_refund[vehicle_daily_refund['refund_date'] == target_date]
            
            if not vehicle_refunds_on_date.empty:
                vehicle_refunds = vehicle_refunds_on_date.iloc[0]['refunds']
                vehicle_cumulative = vehicle_daily_refund[vehicle_daily_refund['refund_date'] <= target_date]['refunds'].sum()
                
                refund_data['comparison_daily'][vehicle].append({
                    'cm2_date': cm2_refund_date.strftime('%Y-%m-%d'),
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start + 1,
                    'refunds': vehicle_refunds
                })
                refund_data['comparison_cumulative'][vehicle].append({
                    'cm2_date': cm2_refund_date.strftime('%Y-%m-%d'),
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start + 1,
                    'cumulative_refunds': vehicle_cumulative
                })
    
    return refund_data

def generate_previous_day_data(vehicle_data, presale_periods):
    """
    ç”ŸæˆCM2è½¦å‹å‰ä¸€å¤©çš„è®¢å•å’Œé€€è®¢æ•°æ®
    åŒ…æ‹¬ï¼šCM2å‰ä¸€å¤©è®¢å•æ•°ã€é€€è®¢æ•°ï¼ŒåŒæœŸå…¶ä»–è½¦å‹å¯¹æ¯”ï¼Œä»¥åŠç´¯è®¡æ•°æ®
    """
    previous_day_data = {
        'cm2_previous_day': None,
        'comparison_previous_day': {},
        'cm2_cumulative_previous': None,
        'comparison_cumulative_previous': {}
    }
    
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is None or len(cm2_data) == 0:
        return previous_day_data
    
    # æ£€æŸ¥presale_periodsæ˜¯å¦æœ‰æ•ˆ
    if not presale_periods or 'CM2' not in presale_periods:
        return previous_day_data
    
    # è·å–CM2çš„èµ·å§‹æ—¥æœŸ
    cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
    
    # å‡†å¤‡CM2æ¯æ—¥è®¢å•æ•°æ®
    cm2_data_copy = cm2_data.copy()
    cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
    cm2_daily = cm2_data_copy.groupby('date').size().reset_index(name='orders')
    cm2_daily['date'] = pd.to_datetime(cm2_daily['date'])
    cm2_daily = cm2_daily.sort_values('date')
    cm2_daily['cumulative'] = cm2_daily['orders'].cumsum()
    
    # å‡†å¤‡CM2é€€è®¢æ•°æ®
    cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
    if len(cm2_refund_data) > 0:
        cm2_refund_data['refund_date'] = cm2_refund_data['intention_refund_time'].dt.date
        cm2_daily_refund = cm2_refund_data.groupby('refund_date').size().reset_index(name='refunds')
        cm2_daily_refund['refund_date'] = pd.to_datetime(cm2_daily_refund['refund_date'])
        cm2_daily_refund = cm2_daily_refund.sort_values('refund_date')
        cm2_daily_refund['cumulative_refunds'] = cm2_daily_refund['refunds'].cumsum()
    else:
        cm2_daily_refund = pd.DataFrame(columns=['refund_date', 'refunds', 'cumulative_refunds'])
    
    # è·å–å‰ä¸€å¤©æ•°æ®ï¼ˆå€’æ•°ç¬¬äºŒå¤©ï¼‰
    if len(cm2_daily) >= 2:
        previous_day_row = cm2_daily.iloc[-2]  # å€’æ•°ç¬¬äºŒå¤©
        previous_day_date = previous_day_row['date']
        days_from_start = (previous_day_date - cm2_start).days + 1
        
        # è·å–å‰ä¸€å¤©çš„é€€è®¢æ•°
        previous_day_refunds = 0
        if len(cm2_daily_refund) > 0:
            refund_on_date = cm2_daily_refund[cm2_daily_refund['refund_date'] == previous_day_date]
            if not refund_on_date.empty:
                previous_day_refunds = refund_on_date.iloc[0]['refunds']
        
        previous_day_data['cm2_previous_day'] = {
            'date': previous_day_date.strftime('%Y-%m-%d'),
            'day_n': days_from_start,
            'orders': previous_day_row['orders'],
            'refunds': previous_day_refunds
        }
        
        # è·å–å‰ä¸€å¤©çš„ç´¯è®¡æ•°æ®
        cumulative_orders = previous_day_row['cumulative']
        cumulative_refunds = 0
        if len(cm2_daily_refund) > 0:
            cumulative_refunds = cm2_daily_refund[cm2_daily_refund['refund_date'] <= previous_day_date]['refunds'].sum()
        
        previous_day_data['cm2_cumulative_previous'] = {
            'day_n': days_from_start,
            'cumulative_orders': cumulative_orders,
            'cumulative_refunds': cumulative_refunds
        }
        
        # å¤„ç†å…¶ä»–è½¦å‹çš„åŒæœŸæ•°æ®
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
                continue
            
            # æ£€æŸ¥è¯¥è½¦å‹çš„presale_periodsæ˜¯å¦å­˜åœ¨
            if vehicle not in presale_periods:
                continue
                
            vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
            
            # å‡†å¤‡è½¦å‹è®¢å•æ•°æ®
            vehicle_data_copy = vehicle_data[vehicle].copy()
            vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
            vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
            vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
            vehicle_daily = vehicle_daily.sort_values('date')
            vehicle_daily['cumulative'] = vehicle_daily['orders'].cumsum()
            
            # å‡†å¤‡è½¦å‹é€€è®¢æ•°æ®
            vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
            if len(vehicle_refund_data) > 0:
                vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
                vehicle_daily_refund = vehicle_refund_data.groupby('refund_date').size().reset_index(name='refunds')
                vehicle_daily_refund['refund_date'] = pd.to_datetime(vehicle_daily_refund['refund_date'])
                vehicle_daily_refund = vehicle_daily_refund.sort_values('refund_date')
            else:
                vehicle_daily_refund = pd.DataFrame(columns=['refund_date', 'refunds'])
            
            # æ‰¾åˆ°å¯¹åº”çš„å†å²è½¦å‹æ—¥æœŸï¼ˆå‰ä¸€å¤©ï¼‰
            target_date = vehicle_start + pd.Timedelta(days=days_from_start-1)
            
            # æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„è®¢å•é‡å’Œé€€è®¢é‡
            vehicle_orders_on_date = vehicle_daily[vehicle_daily['date'] == target_date]
            vehicle_refunds_on_date = 0
            if len(vehicle_daily_refund) > 0:
                refund_on_date = vehicle_daily_refund[vehicle_daily_refund['refund_date'] == target_date]
                if not refund_on_date.empty:
                    vehicle_refunds_on_date = refund_on_date.iloc[0]['refunds']
            
            if not vehicle_orders_on_date.empty:
                vehicle_orders = vehicle_orders_on_date.iloc[0]['orders']
                vehicle_cumulative_orders = vehicle_daily[vehicle_daily['date'] <= target_date]['orders'].sum()
                vehicle_cumulative_refunds = 0
                if len(vehicle_daily_refund) > 0:
                    vehicle_cumulative_refunds = vehicle_daily_refund[vehicle_daily_refund['refund_date'] <= target_date]['refunds'].sum()
                
                previous_day_data['comparison_previous_day'][vehicle] = {
                    'vehicle_date': target_date.strftime('%Y-%m-%d'),
                    'day_n': days_from_start,
                    'orders': vehicle_orders,
                    'refunds': vehicle_refunds_on_date
                }
                
                previous_day_data['comparison_cumulative_previous'][vehicle] = {
                    'day_n': days_from_start,
                    'cumulative_orders': vehicle_cumulative_orders,
                    'cumulative_refunds': vehicle_cumulative_refunds
                }
    
    return previous_day_data

# ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
def generate_structure_report(state, vehicle_data, anomalies):
    """
    ç”Ÿæˆç»“æ„æ£€æŸ¥æŠ¥å‘Š
    """
    from datetime import datetime
    
    report_content = f"""# ç»“æ„æ£€æŸ¥æŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}
- **åˆ†æè½¦å‹**: CM2 vs å†å²è½¦å‹(CM0, DM0, CM1) + CM2 vs CM1ç›´æ¥å¯¹æ¯”
- **æ£€æŸ¥ç»´åº¦**: åœ°åŒºåˆ†å¸ƒã€æ¸ é“ç»“æ„ã€äººç¾¤ç»“æ„

## æ•°æ®æ¦‚å†µ
"""
    
    # æ·»åŠ å„è½¦å‹æ•°æ®é‡ç»Ÿè®¡
    for vehicle, data in vehicle_data.items():
        report_content += f"- **{vehicle}è½¦å‹**: {len(data)} æ¡è®¢å•æ•°æ®\n"
    
    report_content += "\n## ç»“æ„å¼‚å¸¸æ£€æµ‹ç»“æœ\n\n"
    
    # åˆ†ç±»å¼‚å¸¸ - åŒºåˆ†å†å²å¯¹æ¯”å’ŒCM1å¯¹æ¯”
    region_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and any(region_type in a for region_type in ['Parent Region Name', 'License Province', 'license_city_level', 'License City'])]
    region_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and any(region_type in a for region_type in ['Parent Region Name', 'License Province', 'license_city_level', 'License City'])]
    
    channel_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and 'æ¸ é“' in a]
    channel_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and 'æ¸ é“' in a]
    
    demographic_anomalies_hist = [a for a in anomalies if '[å†å²å¯¹æ¯”]' in a and any(demo_type in a for demo_type in ['æ€§åˆ«', 'å¹´é¾„æ®µ'])]
    demographic_anomalies_cm1 = [a for a in anomalies if '[CM1å¯¹æ¯”]' in a and any(demo_type in a for demo_type in ['æ€§åˆ«', 'å¹´é¾„æ®µ'])]
    
    # åŒæ¯”/ç¯æ¯”å¼‚å¸¸
    time_series_anomalies_daily = [a for a in anomalies if '[æ—¥ç¯æ¯”]' in a]
    time_series_anomalies_period = [a for a in anomalies if '[åŒå‘¨æœŸå¯¹æ¯”]' in a]
    time_series_anomalies_cumulative = [a for a in anomalies if '[ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”]' in a]
    
    # åœ°åŒºåˆ†å¸ƒå¼‚å¸¸
    report_content += "### ğŸŒ åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if region_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°åœ°åŒºåˆ†å¸ƒå¼‚å¸¸:**\n\n"
        report_content += "| åºå· | åœ°åŒºç±»å‹ | åœ°åŒºåç§° | CM2å æ¯” | å†å²å¹³å‡å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|---------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(region_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            region_type_match = re.search(r'(Parent Region Name|License Province|license_city_level|License City)ä¸­(.+?)åœ°åŒº', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            hist_match = re.search(r'å†å²å¹³å‡ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            region_type = region_type_match.group(1) if region_type_match else ""
            region_name = region_type_match.group(2) if region_type_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            hist_ratio = hist_match.group(1) if hist_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {region_type} | {region_name} | {cm2_ratio} | {hist_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… åœ°åŒºåˆ†å¸ƒæ­£å¸¸:** æ‰€æœ‰åœ°åŒºè®¢å•å æ¯”å˜åŒ–å‡åœ¨20%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if region_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°åœ°åŒºåˆ†å¸ƒå¼‚å¸¸:**\n\n"
        report_content += "| åºå· | åœ°åŒºç±»å‹ | åœ°åŒºåç§° | CM2å æ¯” | CM1å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|---------|---------|----------|----------|\n"
        
        for i, anomaly in enumerate(region_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            region_type_match = re.search(r'(Parent Region Name|License Province|license_city_level|License City)ä¸­(.+?)åœ°åŒº', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            cm1_match = re.search(r'CM1ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            region_type = region_type_match.group(1) if region_type_match else ""
            region_name = region_type_match.group(2) if region_type_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            cm1_ratio = cm1_match.group(1) if cm1_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {region_type} | {region_name} | {cm2_ratio} | {cm1_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… åœ°åŒºåˆ†å¸ƒæ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰åœ°åŒºè®¢å•å æ¯”å˜åŒ–å‡åœ¨20%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # æ¸ é“ç»“æ„å¼‚å¸¸
    report_content += "\n### ğŸ›’ æ¸ é“ç»“æ„å¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if channel_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°æ¸ é“ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æ¸ é“åç§° | CM2å æ¯” | å†å²å¹³å‡å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|---------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(channel_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            channel_match = re.search(r'æ¸ é“(.+?)é”€é‡å æ¯”', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            hist_match = re.search(r'å†å²å¹³å‡ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            channel_name = channel_match.group(1) if channel_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            hist_ratio = hist_match.group(1) if hist_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {channel_name} | {cm2_ratio} | {hist_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… æ¸ é“ç»“æ„æ­£å¸¸:** æ‰€æœ‰æ¸ é“é”€é‡å æ¯”å˜åŒ–å‡åœ¨15%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if channel_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°æ¸ é“ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æ¸ é“åç§° | CM2å æ¯” | CM1å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|---------|---------|----------|----------|\n"
        
        for i, anomaly in enumerate(channel_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            channel_match = re.search(r'æ¸ é“(.+?)é”€é‡å æ¯”', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            cm1_match = re.search(r'CM1ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            channel_name = channel_match.group(1) if channel_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            cm1_ratio = cm1_match.group(1) if cm1_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {channel_name} | {cm2_ratio} | {cm1_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… æ¸ é“ç»“æ„æ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰æ¸ é“é”€é‡å æ¯”å˜åŒ–å‡åœ¨15%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # äººç¾¤ç»“æ„å¼‚å¸¸
    report_content += "\n### ğŸ‘¥ äººç¾¤ç»“æ„å¼‚å¸¸æ£€æµ‹\n\n"
    
    # å†å²å¯¹æ¯”ç»“æœ
    report_content += "#### ğŸ“Š CM2 vs å†å²å¹³å‡å¯¹æ¯”\n\n"
    if demographic_anomalies_hist:
        report_content += "**ğŸš¨ å‘ç°äººç¾¤ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | äººç¾¤ç±»å‹ | äººç¾¤åç§° | CM2å æ¯” | å†å²å¹³å‡å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|---------|-------------|----------|----------|\n"
        
        for i, anomaly in enumerate(demographic_anomalies_hist, 1):
            clean_anomaly = anomaly.replace('[å†å²å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            demo_match = re.search(r'(æ€§åˆ«|å¹´é¾„æ®µ)(.+?)æ¯”ä¾‹å¼‚å¸¸', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            hist_match = re.search(r'å†å²å¹³å‡ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            demo_type = demo_match.group(1) if demo_match else ""
            demo_name = demo_match.group(2) if demo_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            hist_ratio = hist_match.group(1) if hist_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {demo_type} | {demo_name} | {cm2_ratio} | {hist_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… äººç¾¤ç»“æ„æ­£å¸¸:** æ‰€æœ‰æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–å‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # CM1å¯¹æ¯”ç»“æœ
    report_content += "\n#### ğŸ”„ CM2 vs CM1ç›´æ¥å¯¹æ¯”\n\n"
    if demographic_anomalies_cm1:
        report_content += "**ğŸš¨ å‘ç°äººç¾¤ç»“æ„å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | äººç¾¤ç±»å‹ | äººç¾¤åç§° | CM2å æ¯” | CM1å æ¯” | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|----------|---------|---------|----------|----------|\n"
        
        for i, anomaly in enumerate(demographic_anomalies_cm1, 1):
            clean_anomaly = anomaly.replace('[CM1å¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            demo_match = re.search(r'(æ€§åˆ«|å¹´é¾„æ®µ)(.+?)æ¯”ä¾‹å¼‚å¸¸', clean_anomaly)
            cm2_match = re.search(r'CM2ä¸º([\d.]+%)', clean_anomaly)
            cm1_match = re.search(r'CM1ä¸º([\d.]+%)', clean_anomaly)
            change_match = re.search(r'å˜åŒ–å¹…åº¦([\d.]+%)', clean_anomaly)
            
            demo_type = demo_match.group(1) if demo_match else ""
            demo_name = demo_match.group(2) if demo_match else ""
            cm2_ratio = cm2_match.group(1) if cm2_match else ""
            cm1_ratio = cm1_match.group(1) if cm1_match else ""
            change_rate = change_match.group(1) if change_match else ""
            
            # åˆ¤æ–­å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
            if "å¼‚å¸¸å¢é•¿" in clean_anomaly:
                anomaly_type = "ğŸ“ˆ å¢é•¿"
            elif "å¼‚å¸¸ä¸‹é™" in clean_anomaly:
                anomaly_type = "ğŸ“‰ ä¸‹é™"
            else:
                anomaly_type = "å¼‚å¸¸"
            
            report_content += f"| {i} | {demo_type} | {demo_name} | {cm2_ratio} | {cm1_ratio} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… äººç¾¤ç»“æ„æ­£å¸¸:** ç›¸æ¯”CM1ï¼Œæ‰€æœ‰æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–å‡åœ¨10%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # åŒæ¯”/ç¯æ¯”å¼‚å¸¸
    report_content += "\n### ğŸ“ˆ åŒæ¯”/ç¯æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    
    # ç”Ÿæˆæ—¥ç¯æ¯”æè¿°æ•°æ®
    time_series_desc = generate_time_series_description(vehicle_data, state.get('presale_periods', {}))
    
    # å°†æ—¶é—´åºåˆ—æ•°æ®å­˜å‚¨åˆ°stateä¸­ï¼Œä¾›é€€è®¢ç‡æ—¥ç¯æ¯”å¼‚å¸¸æ£€æµ‹ä½¿ç”¨
    if 'time_series_data' not in state:
        state['time_series_data'] = {}
    
    # å‡†å¤‡CM2æ¯æ—¥æ•°æ®ç”¨äºé€€è®¢ç‡è®¡ç®—
    cm2_data = vehicle_data.get('CM2')
    if cm2_data is not None and len(cm2_data) > 0:
        presale_periods = state.get('presale_periods', {})
        if presale_periods and 'CM2' in presale_periods:
            cm2_start = pd.to_datetime(presale_periods['CM2']['start'])
            
            # å‡†å¤‡CM2æ¯æ—¥è®¢å•æ•°æ®
            cm2_data_copy = cm2_data.copy()
            cm2_data_copy['date'] = cm2_data_copy['Intention_Payment_Time'].dt.date
            cm2_daily = cm2_data_copy.groupby('date').size().reset_index(name='orders')
            cm2_daily['date'] = pd.to_datetime(cm2_daily['date'])
            cm2_daily = cm2_daily.sort_values('date')
            cm2_daily['cumulative_orders'] = cm2_daily['orders'].cumsum()
            
            # å‡†å¤‡CM2é€€è®¢æ•°æ®
            cm2_refund_data = cm2_data[cm2_data['intention_refund_time'].notna()].copy()
            if len(cm2_refund_data) > 0:
                cm2_refund_data['refund_date'] = cm2_refund_data['intention_refund_time'].dt.date
                cm2_daily_refund = cm2_refund_data.groupby('refund_date').size().reset_index(name='refunds')
                cm2_daily_refund['refund_date'] = pd.to_datetime(cm2_daily_refund['refund_date'])
                cm2_daily_refund = cm2_daily_refund.sort_values('refund_date')
                cm2_daily_refund['cumulative_refunds'] = cm2_daily_refund['refunds'].cumsum()
                
                # åˆå¹¶è®¢å•å’Œé€€è®¢æ•°æ®
                cm2_daily = cm2_daily.merge(cm2_daily_refund[['refund_date', 'cumulative_refunds']], 
                                          left_on='date', right_on='refund_date', how='left')
                cm2_daily['cumulative_refunds'] = cm2_daily['cumulative_refunds'].fillna(method='ffill').fillna(0)
            else:
                cm2_daily['cumulative_refunds'] = 0
            
            state['time_series_data']['cm2_daily_data'] = cm2_daily
    
    # æ—¥ç¯æ¯”æè¿°
    report_content += "#### ğŸ“Š æ—¥ç¯æ¯”æè¿°\n\n"
    
    # CM2æ—¥è®¢å•æ•°æè¿°
    if time_series_desc['cm2_daily']:
        report_content += "**CM2è½¦å‹æ—¥è®¢å•æ•°:**\n\n"
        for data in time_series_desc['cm2_daily']:  # æ˜¾ç¤ºæ‰€æœ‰å¤©æ•°
            report_content += f"- ç¬¬{data['day_n']}æ—¥ ({data['date']}): {data['orders']}å•\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹æ—¥è®¢å•æ•°
    report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹æ—¥è®¢å•æ•°å¯¹æ¯”:**\n\n"
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle in time_series_desc['comparison_daily'] and time_series_desc['comparison_daily'][vehicle]:
            report_content += f"- **{vehicle}è½¦å‹**: "
            for data in time_series_desc['comparison_daily'][vehicle]:  # æ˜¾ç¤ºæ‰€æœ‰å¤©æ•°
                report_content += f"ç¬¬{data['day_n']}æ—¥({data['vehicle_date']}):{data['orders']}å•; "
            report_content += "\n"
    
    # CM2ç´¯è®¡è®¢å•æ•°æè¿°
    if time_series_desc['cm2_cumulative']:
        report_content += "\n**CM2è½¦å‹ç´¯è®¡è®¢å•æ•°:**\n\n"
        for data in time_series_desc['cm2_cumulative']:  # æ˜¾ç¤ºæ‰€æœ‰å¤©æ•°
            report_content += f"- ç¬¬{data['day_n']}æ—¥ ({data['date']}): ç´¯è®¡{data['cumulative_orders']}å•\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹ç´¯è®¡è®¢å•æ•°
    report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹ç´¯è®¡è®¢å•æ•°å¯¹æ¯”:**\n\n"
    for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle in time_series_desc['comparison_cumulative'] and time_series_desc['comparison_cumulative'][vehicle]:
            report_content += f"- **{vehicle}è½¦å‹**: "
            for data in time_series_desc['comparison_cumulative'][vehicle]:  # æ˜¾ç¤ºæ‰€æœ‰å¤©æ•°
                report_content += f"ç¬¬{data['day_n']}æ—¥({data['vehicle_date']}):ç´¯è®¡{data['cumulative_orders']}å•; "
            report_content += "\n"
    
    # é€€è®¢æè¿°
    report_content += "\n#### ğŸ”„ é€€è®¢æƒ…å†µæè¿°\n\n"
    
    # ç”Ÿæˆé€€è®¢æè¿°æ•°æ®
    refund_desc = generate_refund_description(vehicle_data, state.get('presale_periods', {}))

    
    # CM2æ—¥é€€è®¢æ•°æè¿°
    if refund_desc['cm2_daily']:
        report_content += "**CM2è½¦å‹æ—¥é€€è®¢æ•°:**\n\n"
        for data in refund_desc['cm2_daily']:
            report_content += f"- ç¬¬{data['day_n']}æ—¥ ({data['date']}): {data['refunds']}å•\n"
    else:
        report_content += "**CM2è½¦å‹æ—¥é€€è®¢æ•°:** æš‚æ— é€€è®¢æ•°æ®\n\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹æ—¥é€€è®¢æ•°å¯¹æ¯”
    if refund_desc['comparison_daily']:
        report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹æ—¥é€€è®¢æ•°å¯¹æ¯”:**\n\n"
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in refund_desc['comparison_daily'] and refund_desc['comparison_daily'][vehicle]:
                report_content += f"- **{vehicle}è½¦å‹**: "
                for data in refund_desc['comparison_daily'][vehicle]:
                    report_content += f"ç¬¬{data['day_n']}æ—¥({data['vehicle_date']}):{data['refunds']}å•; "
                report_content += "\n"
    
    # CM2ç´¯è®¡é€€è®¢æ•°æè¿°
    if refund_desc['cm2_cumulative']:
        report_content += "\n**CM2è½¦å‹ç´¯è®¡é€€è®¢æ•°:**\n\n"
        for data in refund_desc['cm2_cumulative']:
            report_content += f"- ç¬¬{data['day_n']}æ—¥ ({data['date']}): ç´¯è®¡{data['cumulative_refunds']}å•\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹ç´¯è®¡é€€è®¢æ•°å¯¹æ¯”
    if refund_desc['comparison_cumulative']:
        report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹ç´¯è®¡é€€è®¢æ•°å¯¹æ¯”:**\n\n"
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in refund_desc['comparison_cumulative'] and refund_desc['comparison_cumulative'][vehicle]:
                report_content += f"- **{vehicle}è½¦å‹**: "
                for data in refund_desc['comparison_cumulative'][vehicle]:
                    report_content += f"ç¬¬{data['day_n']}æ—¥({data['vehicle_date']}):ç´¯è®¡{data['cumulative_refunds']}å•; "
                report_content += "\n"
    
    # é€€è®¢ç‡è®¡ç®—å’Œæè¿°
    report_content += "\n**é€€è®¢ç‡åˆ†æ:**\n\n"
    
    # CM2è½¦å‹é€€è®¢ç‡
    if refund_desc['cm2_cumulative'] and time_series_desc['cm2_cumulative']:
        cm2_refund_data = refund_desc['cm2_cumulative'][0]
        cm2_order_data = time_series_desc['cm2_cumulative'][0]
        
        cm2_refund_rate = (cm2_refund_data['cumulative_refunds'] / cm2_order_data['cumulative_orders']) * 100
        report_content += f"- **CM2è½¦å‹**: ç¬¬{cm2_refund_data['day_n']}æ—¥ ({cm2_refund_data['date']}) é€€è®¢ç‡: {cm2_refund_rate:.2f}% (ç´¯è®¡é€€è®¢{cm2_refund_data['cumulative_refunds']}å•/ç´¯è®¡è®¢å•{cm2_order_data['cumulative_orders']}å•)\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹é€€è®¢ç‡å¯¹æ¯”
    if refund_desc['comparison_cumulative'] and time_series_desc['comparison_cumulative']:
        report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹é€€è®¢ç‡å¯¹æ¯”:**\n\n"
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if (vehicle in refund_desc['comparison_cumulative'] and 
                refund_desc['comparison_cumulative'][vehicle] and
                vehicle in time_series_desc['comparison_cumulative'] and 
                time_series_desc['comparison_cumulative'][vehicle]):
                
                refund_data = refund_desc['comparison_cumulative'][vehicle][0]
                order_data = time_series_desc['comparison_cumulative'][vehicle][0]
                
                if order_data['cumulative_orders'] > 0:
                    refund_rate = (refund_data['cumulative_refunds'] / order_data['cumulative_orders']) * 100
                    report_content += f"- **{vehicle}è½¦å‹**: ç¬¬{refund_data['day_n']}æ—¥({refund_data['vehicle_date']}) é€€è®¢ç‡: {refund_rate:.2f}% (ç´¯è®¡é€€è®¢{refund_data['cumulative_refunds']}å•/ç´¯è®¡è®¢å•{order_data['cumulative_orders']}å•)\n"
    
    # å‰ä¸€å¤©æ•°æ®æè¿°
    report_content += "\n#### ğŸ“… å‰ä¸€å¤©æ•°æ®åˆ†æ\n\n"
    
    # ç”Ÿæˆå‰ä¸€å¤©æ•°æ®
    previous_day_data = generate_previous_day_data(vehicle_data, state.get('presale_periods', {}))
    
    # CM2å‰ä¸€å¤©è®¢å•æ•°å’Œé€€è®¢æ•°
    if previous_day_data['cm2_previous_day']:
        cm2_prev = previous_day_data['cm2_previous_day']
        report_content += f"**CM2è½¦å‹å‰ä¸€å¤©æ•°æ® (ç¬¬{cm2_prev['day_n']}æ—¥, {cm2_prev['date']}):**\n\n"
        report_content += f"- è®¢å•æ•°: {cm2_prev['orders']}å•\n"
        report_content += f"- é€€è®¢æ•°: {cm2_prev['refunds']}å•\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹å‰ä¸€å¤©æ•°æ®å¯¹æ¯”
    if previous_day_data['comparison_previous_day']:
        report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹å‰ä¸€å¤©æ•°æ®å¯¹æ¯”:**\n\n"
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in previous_day_data['comparison_previous_day']:
                data = previous_day_data['comparison_previous_day'][vehicle]
                report_content += f"- **{vehicle}è½¦å‹** (ç¬¬{data['day_n']}æ—¥, {data['vehicle_date']}): è®¢å•{data['orders']}å•, é€€è®¢{data['refunds']}å•\n"
    
    # CM2å‰N-1æ—¥ç´¯è®¡æ•°æ®
    if previous_day_data['cm2_cumulative_previous']:
        cm2_cum_prev = previous_day_data['cm2_cumulative_previous']
        report_content += f"\n**CM2è½¦å‹å‰{cm2_cum_prev['day_n']}æ—¥ç´¯è®¡æ•°æ®:**\n\n"
        report_content += f"- ç´¯è®¡è®¢å•æ•°: {cm2_cum_prev['cumulative_orders']}å•\n"
        report_content += f"- ç´¯è®¡é€€è®¢æ•°: {cm2_cum_prev['cumulative_refunds']}å•\n"
        
        # è®¡ç®—å‰N-1æ—¥é€€è®¢ç‡
        if cm2_cum_prev['cumulative_orders'] > 0:
            previous_refund_rate = (cm2_cum_prev['cumulative_refunds'] / cm2_cum_prev['cumulative_orders']) * 100
            report_content += f"- é€€è®¢ç‡: {previous_refund_rate:.2f}%\n"
    
    # åŒæœŸå…¶ä»–è½¦å‹å‰N-1æ—¥ç´¯è®¡æ•°æ®å¯¹æ¯”
    if previous_day_data['comparison_cumulative_previous']:
        report_content += "\n**åŒæœŸå…¶ä»–è½¦å‹å‰N-1æ—¥ç´¯è®¡æ•°æ®å¯¹æ¯”:**\n\n"
        for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in previous_day_data['comparison_cumulative_previous']:
                data = previous_day_data['comparison_cumulative_previous'][vehicle]
                vehicle_refund_rate = 0
                if data['cumulative_orders'] > 0:
                    vehicle_refund_rate = (data['cumulative_refunds'] / data['cumulative_orders']) * 100
                report_content += f"- **{vehicle}è½¦å‹** (ç¬¬{data['day_n']}æ—¥): ç´¯è®¡è®¢å•{data['cumulative_orders']}å•, ç´¯è®¡é€€è®¢{data['cumulative_refunds']}å•, é€€è®¢ç‡{vehicle_refund_rate:.2f}%\n"

    # æ—¥ç¯æ¯”å¼‚å¸¸
    report_content += "\n#### ğŸ“… æ—¥ç¯æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    if time_series_anomalies_daily:
        report_content += "**ğŸš¨ å‘ç°æ—¥ç¯æ¯”å¼‚å¸¸:**\n\n"
        for i, anomaly in enumerate(time_series_anomalies_daily, 1):
            clean_anomaly = anomaly.replace('[æ—¥ç¯æ¯”]', '')
            report_content += f"{i}. {clean_anomaly}\n"
    else:
        report_content += "**âœ… æ—¥ç¯æ¯”æ­£å¸¸:** CM2è½¦å‹æ—¥è®¢å•é‡å˜åŒ–å‡åœ¨50%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # é€€è®¢ç‡æ—¥ç¯æ¯”å¼‚å¸¸æ£€æµ‹
    report_content += "\n#### ğŸ“Š é€€è®¢ç‡æ—¥ç¯æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    
    # è·å–å½“å‰æ—¥æœŸ(ç¬¬Næ—¥)çš„é€€è®¢ç‡æ•°æ®
    time_series_data = state.get('time_series_data', {})
    if time_series_data and 'cm2_daily_data' in time_series_data:
        cm2_daily_data = time_series_data['cm2_daily_data']
        if len(cm2_daily_data) > 0:
            # è·å–æœ€åä¸€å¤©çš„é€€è®¢ç‡
            last_day_data = cm2_daily_data.iloc[-1]
            current_refund_rate = 0
            if last_day_data['cumulative_orders'] > 0:
                current_refund_rate = (last_day_data['cumulative_refunds'] / last_day_data['cumulative_orders']) * 100
            
            # è·å–å‰ä¸€å¤©çš„é€€è®¢ç‡
            previous_refund_rate = 0
            if previous_day_data['cm2_cumulative_previous'] and previous_day_data['cm2_cumulative_previous']['cumulative_orders'] > 0:
                previous_refund_rate = (previous_day_data['cm2_cumulative_previous']['cumulative_refunds'] / previous_day_data['cm2_cumulative_previous']['cumulative_orders']) * 100
            
            # è®¡ç®—CM2é€€è®¢ç‡å˜åŒ–å¹…åº¦
            cm2_refund_rate_change = 0
            if previous_refund_rate > 0:
                cm2_refund_rate_change = ((current_refund_rate - previous_refund_rate) / previous_refund_rate) * 100
            
            # è®¡ç®—å†å²è½¦å‹çš„é€€è®¢ç‡æ—¥ç¯æ¯”å˜åŒ–å¹…åº¦ä½œä¸ºåŸºå‡†
            historical_changes = []
            for vehicle in ['CM0', 'CM1', 'DM0', 'DM1']:
                if vehicle in previous_day_data['comparison_cumulative_previous']:
                    # è·å–å†å²è½¦å‹å½“å‰æ—¥æœŸçš„é€€è®¢ç‡ï¼ˆæ¨¡æ‹Ÿç¬¬Næ—¥ï¼‰
                    vehicle_current_data = previous_day_data['comparison_cumulative_previous'][vehicle]
                    vehicle_current_refund_rate = 0
                    if vehicle_current_data['cumulative_orders'] > 0:
                        vehicle_current_refund_rate = (vehicle_current_data['cumulative_refunds'] / vehicle_current_data['cumulative_orders']) * 100
                    
                    # è·å–å†å²è½¦å‹å‰ä¸€å¤©çš„é€€è®¢ç‡ï¼ˆæ¨¡æ‹Ÿç¬¬N-1æ—¥ï¼‰
                    # è¿™é‡Œéœ€è¦è®¡ç®—ç¬¬N-1æ—¥çš„ç´¯è®¡æ•°æ®
                    if vehicle in vehicle_data and len(vehicle_data[vehicle]) > 0:
                        vehicle_start = pd.to_datetime(state.get('presale_periods', {}).get(vehicle, {}).get('start'))
                        if vehicle_start is not None:
                            target_date_n_minus_1 = vehicle_start + pd.Timedelta(days=previous_day_data['cm2_cumulative_previous']['day_n']-2)
                            
                            # è®¡ç®—ç¬¬N-1æ—¥çš„ç´¯è®¡è®¢å•å’Œé€€è®¢
                            vehicle_data_copy = vehicle_data[vehicle].copy()
                            vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
                            vehicle_orders_n_minus_1 = vehicle_data_copy[pd.to_datetime(vehicle_data_copy['date']) <= target_date_n_minus_1].shape[0]
                            
                            vehicle_refunds_n_minus_1 = 0
                            vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
                            if len(vehicle_refund_data) > 0:
                                vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
                                vehicle_refunds_n_minus_1 = vehicle_refund_data[pd.to_datetime(vehicle_refund_data['refund_date']) <= target_date_n_minus_1].shape[0]
                            
                            vehicle_previous_refund_rate = 0
                            if vehicle_orders_n_minus_1 > 0:
                                vehicle_previous_refund_rate = (vehicle_refunds_n_minus_1 / vehicle_orders_n_minus_1) * 100
                            
                            # è®¡ç®—å†å²è½¦å‹çš„é€€è®¢ç‡å˜åŒ–å¹…åº¦
                            if vehicle_previous_refund_rate > 0:
                                vehicle_change = ((vehicle_current_refund_rate - vehicle_previous_refund_rate) / vehicle_previous_refund_rate) * 100
                                historical_changes.append(abs(vehicle_change))
            
            # è®¡ç®—å†å²è½¦å‹å˜åŒ–å¹…åº¦çš„å¹³å‡å€¼ä½œä¸ºåŸºå‡†
            if historical_changes:
                avg_historical_change = sum(historical_changes) / len(historical_changes)
                threshold = max(20, avg_historical_change * 1.5)  # é˜ˆå€¼ä¸º20%æˆ–å†å²å¹³å‡å˜åŒ–çš„1.5å€ï¼Œå–è¾ƒå¤§å€¼
                
                # æ£€æµ‹CM2å¼‚å¸¸
                if abs(cm2_refund_rate_change) > threshold:
                    if cm2_refund_rate_change > 0:
                        report_content += f"**ğŸš¨ å‘ç°é€€è®¢ç‡å¼‚å¸¸éª¤å¢:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                        report_content += f"**ğŸ“Š å†å²åŸºå‡†:** å†å²è½¦å‹å¹³å‡å˜åŒ–å¹…åº¦{avg_historical_change:.1f}%ï¼Œå¼‚å¸¸é˜ˆå€¼{threshold:.1f}%\n"
                    else:
                        report_content += f"**ğŸš¨ å‘ç°é€€è®¢ç‡å¼‚å¸¸éª¤é™:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                        report_content += f"**ğŸ“Š å†å²åŸºå‡†:** å†å²è½¦å‹å¹³å‡å˜åŒ–å¹…åº¦{avg_historical_change:.1f}%ï¼Œå¼‚å¸¸é˜ˆå€¼{threshold:.1f}%\n"
                else:
                    report_content += f"**âœ… é€€è®¢ç‡æ—¥ç¯æ¯”æ­£å¸¸:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                    report_content += f"**ğŸ“Š å†å²åŸºå‡†:** å†å²è½¦å‹å¹³å‡å˜åŒ–å¹…åº¦{avg_historical_change:.1f}%ï¼Œå¼‚å¸¸é˜ˆå€¼{threshold:.1f}%\n"
            else:
                # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨å›ºå®š20%é˜ˆå€¼
                if abs(cm2_refund_rate_change) > 20:
                    if cm2_refund_rate_change > 0:
                        report_content += f"**ğŸš¨ å‘ç°é€€è®¢ç‡å¼‚å¸¸éª¤å¢:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                    else:
                        report_content += f"**ğŸš¨ å‘ç°é€€è®¢ç‡å¼‚å¸¸éª¤é™:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                else:
                    report_content += f"**âœ… é€€è®¢ç‡æ—¥ç¯æ¯”æ­£å¸¸:** CM2å½“å‰é€€è®¢ç‡{current_refund_rate:.2f}%ï¼Œå‰æ—¥é€€è®¢ç‡{previous_refund_rate:.2f}%ï¼Œå˜åŒ–å¹…åº¦{cm2_refund_rate_change:+.1f}%\n"
                report_content += f"**ğŸ“Š ä½¿ç”¨å›ºå®šé˜ˆå€¼:** 20%ï¼ˆç¼ºå°‘å†å²è½¦å‹æ•°æ®ï¼‰\n"
        else:
            report_content += "**âš ï¸ æ— æ³•è¿›è¡Œé€€è®¢ç‡æ—¥ç¯æ¯”æ£€æµ‹:** ç¼ºå°‘æ—¶é—´åºåˆ—æ•°æ®\n"
    else:
        report_content += "**âš ï¸ æ— æ³•è¿›è¡Œé€€è®¢ç‡æ—¥ç¯æ¯”æ£€æµ‹:** ç¼ºå°‘æ—¶é—´åºåˆ—æ•°æ®\n"
    
    # æ·»åŠ å„è½¦å‹æ¯æ—¥é€€è®¢ç‡å¯¹æ¯”è¡¨æ ¼
    report_content += "\n#### ğŸ“ˆ å„è½¦å‹æ¯æ—¥é€€è®¢ç‡å¯¹æ¯”è¡¨\n\n"
    
    # æ„å»ºé€€è®¢ç‡å¯¹æ¯”è¡¨æ ¼
    refund_rate_table_data = {}
    max_days = 0
    
    # å¤„ç†æ‰€æœ‰è½¦å‹çš„é€€è®¢ç‡æ•°æ®
    for vehicle in ['CM2', 'CM0', 'CM1', 'DM0', 'DM1']:
        if vehicle not in vehicle_data or len(vehicle_data[vehicle]) == 0:
            continue
            
        presale_periods = state.get('presale_periods', {})
        if vehicle not in presale_periods:
            continue
            
        vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
        
        # å‡†å¤‡è½¦å‹è®¢å•æ•°æ®
        vehicle_data_copy = vehicle_data[vehicle].copy()
        vehicle_data_copy['date'] = vehicle_data_copy['Intention_Payment_Time'].dt.date
        vehicle_daily = vehicle_data_copy.groupby('date').size().reset_index(name='orders')
        vehicle_daily['date'] = pd.to_datetime(vehicle_daily['date'])
        vehicle_daily = vehicle_daily.sort_values('date')
        vehicle_daily['cumulative_orders'] = vehicle_daily['orders'].cumsum()
        
        # å‡†å¤‡è½¦å‹é€€è®¢æ•°æ®
        vehicle_refund_data = vehicle_data[vehicle][vehicle_data[vehicle]['intention_refund_time'].notna()].copy()
        if len(vehicle_refund_data) > 0:
            vehicle_refund_data['refund_date'] = vehicle_refund_data['intention_refund_time'].dt.date
            vehicle_daily_refund = vehicle_refund_data.groupby('refund_date').size().reset_index(name='refunds')
            vehicle_daily_refund['refund_date'] = pd.to_datetime(vehicle_daily_refund['refund_date'])
            vehicle_daily_refund = vehicle_daily_refund.sort_values('refund_date')
            vehicle_daily_refund['cumulative_refunds'] = vehicle_daily_refund['refunds'].cumsum()
            
            # åˆå¹¶è®¢å•å’Œé€€è®¢æ•°æ®
            vehicle_daily = vehicle_daily.merge(vehicle_daily_refund[['refund_date', 'cumulative_refunds']], 
                                              left_on='date', right_on='refund_date', how='left')
            vehicle_daily['cumulative_refunds'] = vehicle_daily['cumulative_refunds'].ffill().fillna(0)
        else:
            vehicle_daily['cumulative_refunds'] = 0
        
        # è®¡ç®—æ¯æ—¥é€€è®¢ç‡
        vehicle_daily['refund_rate'] = 0.0
        vehicle_daily.loc[vehicle_daily['cumulative_orders'] > 0, 'refund_rate'] = (
            vehicle_daily['cumulative_refunds'] / vehicle_daily['cumulative_orders'] * 100
        )
        
        # è®¡ç®—ä»é¢„å”®å¼€å§‹çš„å¤©æ•°
        vehicle_daily['days_from_start'] = (vehicle_daily['date'] - vehicle_start).dt.days
        
        # å­˜å‚¨æ•°æ®
        refund_rate_table_data[vehicle] = {}
        for _, row in vehicle_daily.iterrows():
            day_num = row['days_from_start']
            if day_num >= 0:  # åªåŒ…å«é¢„å”®å¼€å§‹åçš„æ•°æ®
                refund_rate_table_data[vehicle][day_num] = row['refund_rate']
                max_days = max(max_days, day_num)
    
    # ç”Ÿæˆè¡¨æ ¼ï¼ˆè½¦å‹ä½œä¸ºåˆ—ï¼Œæ—¥æœŸä½œä¸ºè¡Œï¼‰
    if refund_rate_table_data and max_days > 0:
        # è¡¨å¤´
        header = "| æ—¥æœŸ |"
        separator = "|------|"
        vehicles_with_data = []
        for vehicle in ['CM2', 'CM0', 'CM1', 'DM0', 'DM1']:
            if vehicle in refund_rate_table_data:
                vehicles_with_data.append(vehicle)
                header += f" **{vehicle}** |"
                separator += "-------|"
        
        report_content += header + "\n"
        report_content += separator + "\n"
        
        # è¡¨æ ¼å†…å®¹ï¼ˆæŒ‰æ—¥æœŸè¡Œå±•ç¤ºï¼‰
        for day in range(max_days + 1):  # å®Œæ•´å±•ç¤ºæ‰€æœ‰å¤©æ•°
            row = f"| ç¬¬{day}æ—¥ |"
            for vehicle in vehicles_with_data:
                if day in refund_rate_table_data[vehicle]:
                    rate = refund_rate_table_data[vehicle][day]
                    row += f" {rate:.2f}% |"
                else:
                    row += " - |"
            report_content += row + "\n"
        
        report_content += "\n*æ³¨ï¼šè¡¨æ ¼æ˜¾ç¤ºå„è½¦å‹ä»é¢„å”®å¼€å§‹æ¯æ—¥çš„ç´¯è®¡é€€è®¢ç‡ï¼Œ'-' è¡¨ç¤ºè¯¥æ—¥æ— æ•°æ®*\n\n"
        
        # ç”Ÿæˆå½’ä¸€åŒ–æ—¶é—´å¯¹æ¯”è¡¨æ ¼
        report_content += "#### ğŸ“ˆ å„è½¦å‹å½’ä¸€åŒ–é¢„å”®å‘¨æœŸé€€è®¢ç‡å¯¹æ¯”è¡¨\n\n"
        
        # è®¡ç®—æ¯ä¸ªè½¦å‹çš„é¢„å”®å‘¨æœŸé•¿åº¦å’Œå½’ä¸€åŒ–æ•°æ®
        normalized_data = {}
        presale_periods = state.get('presale_periods', {})
        
        for vehicle in vehicles_with_data:
            vehicle_days = list(refund_rate_table_data[vehicle].keys())
            if vehicle_days and vehicle in presale_periods:
                # ä½¿ç”¨é¢„å”®å‘¨æœŸå®šä¹‰è®¡ç®—æ€»å¤©æ•°
                vehicle_start = pd.to_datetime(presale_periods[vehicle]['start'])
                vehicle_end = pd.to_datetime(presale_periods[vehicle]['end'])
                total_presale_days = (vehicle_end - vehicle_start).days
                
                normalized_data[vehicle] = {}
                
                # ä¸ºæ¯ä¸ªç™¾åˆ†æ¯”ç‚¹è®¡ç®—å¯¹åº”çš„å¤©æ•°å’Œé€€è®¢ç‡
                max_available_day = max(vehicle_days) if vehicle_days else 0
                max_available_pct = int((max_available_day / total_presale_days) * 100) if total_presale_days > 0 else 0
                
                for pct in range(0, 101, 10):  # 0%, 10%, 20%, ..., 100%
                    target_day = int(total_presale_days * pct / 100)
                    
                    # å¦‚æœç›®æ ‡ç™¾åˆ†æ¯”è¶…å‡ºå½“å‰å¯ç”¨æ•°æ®èŒƒå›´ï¼Œè·³è¿‡
                    if pct > max_available_pct and vehicle == 'CM2':
                        continue  # ä¸æ·»åŠ æ•°æ®ï¼Œåç»­ä¼šæ˜¾ç¤ºä¸º "-"
                    
                    # æ‰¾åˆ°æœ€æ¥è¿‘çš„æœ‰æ•°æ®çš„å¤©æ•°
                    closest_day = None
                    min_diff = float('inf')
                    for day in vehicle_days:
                        if day <= target_day:  # åªè€ƒè™‘ä¸è¶…è¿‡ç›®æ ‡å¤©æ•°çš„æ•°æ®
                            diff = abs(day - target_day)
                            if diff < min_diff:
                                min_diff = diff
                                closest_day = day
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸è¶…è¿‡ç›®æ ‡å¤©æ•°çš„æ•°æ®ï¼Œå–æœ€å°çš„å¤©æ•°
                    if closest_day is None and vehicle_days:
                        closest_day = min([d for d in vehicle_days if d >= 0])
                    
                    if closest_day is not None:
                        normalized_data[vehicle][pct] = refund_rate_table_data[vehicle][closest_day]
        
        # ç”Ÿæˆå½’ä¸€åŒ–è¡¨æ ¼
        if normalized_data:
            # è¡¨å¤´
            norm_header = "| é¢„å”®è¿›åº¦ |"
            norm_separator = "|-------|"
            for vehicle in vehicles_with_data:
                if vehicle in normalized_data:
                    norm_header += f" **{vehicle}** |"
                    norm_separator += "-------|"
            
            report_content += norm_header + "\n"
            report_content += norm_separator + "\n"
            
            # è¡¨æ ¼å†…å®¹
            for pct in range(0, 101, 10):
                row = f"| {pct}% |"
                for vehicle in vehicles_with_data:
                    if vehicle in normalized_data and pct in normalized_data[vehicle]:
                        rate = normalized_data[vehicle][pct]
                        row += f" {rate:.2f}% |"
                    else:
                        row += " - |"
                report_content += row + "\n"
            
            report_content += "\n*æ³¨ï¼šè¡¨æ ¼æ˜¾ç¤ºå„è½¦å‹åœ¨é¢„å”®å‘¨æœŸä¸åŒè¿›åº¦ä¸‹çš„ç´¯è®¡é€€è®¢ç‡ï¼Œä¾¿äºè·¨è½¦å‹å¯¹æ¯”*\n"
        else:
            report_content += "**âš ï¸ æ— æ³•ç”Ÿæˆå½’ä¸€åŒ–å¯¹æ¯”è¡¨:** ç¼ºå°‘å½’ä¸€åŒ–æ•°æ®\n"
    else:
        report_content += "**âš ï¸ æ— æ³•ç”Ÿæˆé€€è®¢ç‡å¯¹æ¯”è¡¨:** ç¼ºå°‘è½¦å‹æ•°æ®\n"
    
    # åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸
    report_content += "\n#### ğŸ”„ åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    if time_series_anomalies_period:
        report_content += "**ğŸš¨ å‘ç°åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æ—¥æœŸ | CM2è®¢å•æ•° | å¯¹æ¯”è½¦å‹ | å¯¹æ¯”è½¦å‹è®¢å•æ•° | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|------|----------|----------|----------------|----------|----------|\n"
        for i, anomaly in enumerate(time_series_anomalies_period, 1):
            clean_anomaly = anomaly.replace('[åŒå‘¨æœŸå¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            parts = clean_anomaly.split('ï¼š')
            if len(parts) >= 2:
                desc_part = parts[0]
                data_part = parts[1]
                
                # æå–æ—¥æœŸ
                date_match = re.search(r'(\d{4}-\d{2}-\d{2})', desc_part)
                date = date_match.group(1) if date_match else ""
                
                # æå–å¯¹æ¯”è½¦å‹
                vehicle_match = re.search(r'ç›¸å¯¹(CM\d|DM\d)åŒæœŸ', desc_part)
                compare_vehicle = vehicle_match.group(1) if vehicle_match else ""
                
                # æå–å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
                if "éª¤å¢" in desc_part:
                    anomaly_type = "ğŸ“ˆ éª¤å¢"
                else:
                    anomaly_type = "ğŸ“‰ éª¤é™"
                
                # æå–æ•°æ®
                cm2_match = re.search(r'CM2ä¸º(\d+)å•', data_part)
                compare_match = re.search(r'{}åŒæœŸä¸º(\d+)å•'.format(compare_vehicle), data_part)
                change_match = re.search(r'å˜åŒ–å¹…åº¦([+-]?\d+\.\d+)%', data_part)
                
                cm2_orders = cm2_match.group(1) if cm2_match else ""
                compare_orders = compare_match.group(1) if compare_match else ""
                change_rate = change_match.group(1) + "%" if change_match else ""
                
                report_content += f"| {i} | {date} | {cm2_orders} | {compare_vehicle} | {compare_orders} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… åŒå‘¨æœŸå¯¹æ¯”æ­£å¸¸:** CM2ç›¸å¯¹äºå†å²è½¦å‹åŒæœŸè®¢å•é‡å˜åŒ–å‡åœ¨100%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    # ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸
    report_content += "\n#### ğŸ“Š ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸æ£€æµ‹\n\n"
    if time_series_anomalies_cumulative:
        report_content += "**ğŸš¨ å‘ç°ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸:**\n\n"
        report_content += "| åºå· | æˆªè‡³æ—¥æœŸ | CM2ç´¯è®¡è®¢å•æ•° | å¯¹æ¯”è½¦å‹ | å¯¹æ¯”è½¦å‹ç´¯è®¡è®¢å•æ•° | å˜åŒ–å¹…åº¦ | å¼‚å¸¸ç±»å‹ |\n"
        report_content += "|------|----------|---------------|----------|-------------------|----------|----------|\n"
        for i, anomaly in enumerate(time_series_anomalies_cumulative, 1):
            clean_anomaly = anomaly.replace('[ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”]', '')
            
            # è§£æå¼‚å¸¸ä¿¡æ¯
            parts = clean_anomaly.split('ï¼š')
            if len(parts) >= 2:
                desc_part = parts[0]
                data_part = parts[1]
                
                # æå–æ—¥æœŸ
                date_match = re.search(r'æˆªè‡³(\d{4}-\d{2}-\d{2})', desc_part)
                date = date_match.group(1) if date_match else ""
                
                # æå–å¯¹æ¯”è½¦å‹
                vehicle_match = re.search(r'ç›¸å¯¹(CM\d|DM\d)åŒæœŸ', desc_part)
                compare_vehicle = vehicle_match.group(1) if vehicle_match else ""
                
                # æå–å¼‚å¸¸ç±»å‹å¹¶æ·»åŠ emoji
                if "éª¤å¢" in desc_part:
                    anomaly_type = "ğŸ“ˆ éª¤å¢"
                else:
                    anomaly_type = "ğŸ“‰ éª¤é™"
                
                # æå–æ•°æ®
                cm2_match = re.search(r'CM2ç´¯è®¡(\d+)å•', data_part)
                compare_match = re.search(r'{}åŒæœŸç´¯è®¡(\d+)å•'.format(compare_vehicle), data_part)
                change_match = re.search(r'å˜åŒ–å¹…åº¦([+-]?\d+\.\d+)%', data_part)
                
                cm2_orders = cm2_match.group(1) if cm2_match else ""
                compare_orders = compare_match.group(1) if compare_match else ""
                change_rate = change_match.group(1) + "%" if change_match else ""
                
                report_content += f"| {i} | {date} | {cm2_orders} | {compare_vehicle} | {compare_orders} | {change_rate} | {anomaly_type} |\n"
    else:
        report_content += "**âœ… ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”æ­£å¸¸:** CM2ç´¯è®¡è®¢å•é‡ç›¸å¯¹äºå†å²è½¦å‹åŒæœŸå˜åŒ–å‡åœ¨100%é˜ˆå€¼èŒƒå›´å†…ã€‚\n"
    
    report_content += "\n## æ£€æŸ¥è¯´æ˜\n\n"
    report_content += "### ğŸ“Š å†å²å¹³å‡å¯¹æ¯”\n"
    report_content += "- **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹(CM0, DM0, CM1, DM1)å¹³å‡å€¼çš„å„åœ°åŒºè®¢å•å æ¯”å˜åŒ–è¶…è¿‡20%çš„æƒ…å†µï¼Œä¸”è¯¥åœ°åŒºå æ¯”è¶…è¿‡1%\n"
    report_content += "- **æ¸ é“ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹å¹³å‡å€¼çš„æ¸ é“é”€é‡å æ¯”å˜åŒ–è¶…è¿‡15%çš„æƒ…å†µï¼Œä¸”è¯¥æ¸ é“å æ¯”è¶…è¿‡1%\n"
    report_content += "- **äººç¾¤ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹å¹³å‡å€¼çš„æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–è¶…è¿‡10%çš„æƒ…å†µ\n\n"
    report_content += "### ğŸ”„ CM1ç›´æ¥å¯¹æ¯”\n"
    report_content += "- **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„å„åœ°åŒºè®¢å•å æ¯”å˜åŒ–è¶…è¿‡20%çš„æƒ…å†µï¼Œä¸”è¯¥åœ°åŒºå æ¯”è¶…è¿‡1%\n"
    report_content += "- **æ¸ é“ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„æ¸ é“é”€é‡å æ¯”å˜åŒ–è¶…è¿‡15%çš„æƒ…å†µï¼Œä¸”è¯¥æ¸ é“å æ¯”è¶…è¿‡1%\n"
    report_content += "- **äººç¾¤ç»“æ„å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºCM1è½¦å‹çš„æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„å˜åŒ–è¶…è¿‡10%çš„æƒ…å†µ\n\n"
    report_content += "### ğŸ“ˆ åŒæ¯”/ç¯æ¯”æ£€æµ‹\n"
    report_content += "- **æ—¥ç¯æ¯”å¼‚å¸¸**: æ£€æŸ¥CM2è½¦å‹å†…éƒ¨ç›¸é‚»æ—¥æœŸè®¢å•é‡å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µ\n"
    report_content += "- **åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸**: æ£€æŸ¥CM2ç›¸å¯¹äºå†å²è½¦å‹(CM0, CM1, DM0, DM1)ç›¸åŒç›¸å¯¹å¤©æ•°çš„è®¢å•é‡å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µ\n"
    report_content += "- **ç´¯è®¡åŒå‘¨æœŸå¯¹æ¯”å¼‚å¸¸**: æ£€æŸ¥CM2ç´¯è®¡è®¢å•é‡ç›¸å¯¹äºå†å²è½¦å‹åŒæœŸç´¯è®¡è®¢å•é‡å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µ\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/structure_check_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"ç»“æ„æ£€æŸ¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“‹ ç»“æ„æ£€æŸ¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # æ›´æ–°çŠ¶æ€
    state["structure_anomalies"] = anomalies
    state["structure_report_path"] = report_path

# åˆ›å»ºå·¥ä½œæµå›¾
def create_workflow():
    """
    åˆ›å»ºLangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ
    """
    workflow = StateGraph(WorkflowState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("anomaly_detection", anomaly_detection_node)
    workflow.add_node("structure_check", structure_check_node)
    workflow.add_node("generate_complete_report", lambda state: generate_complete_report(state) or state)
    workflow.add_node("update_readme", update_readme_mermaid_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("anomaly_detection")
    
    # æ·»åŠ è¾¹
    workflow.add_edge("anomaly_detection", "structure_check")
    workflow.add_edge("structure_check", "generate_complete_report")
    workflow.add_edge("generate_complete_report", "update_readme")
    workflow.add_edge("update_readme", END)
    
    return workflow.compile()

# ä¸»å‡½æ•°
def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œå·¥ä½œæµ
    """
    logger.info("å¯åŠ¨LangGraphå¼‚å¸¸æ£€æµ‹å·¥ä½œæµ...")
    
    # åˆ›å»ºå·¥ä½œæµ
    app = create_workflow()
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "data": None,
        "data_path": "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet",
        "integrity_check_results": {},
        "errors": [],
        "status": "initialized",
        "metadata": {}
    }
    
    # è¿è¡Œå·¥ä½œæµ
    try:
        result = app.invoke(initial_state)
        
        if result["status"] == "failed":
            logger.error("å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            for error in result["errors"]:
                logger.error(f"é”™è¯¯: {error}")
        else:
            logger.info("å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
            logger.info(f"æœ€ç»ˆçŠ¶æ€: {result['status']}")
            
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    main()
