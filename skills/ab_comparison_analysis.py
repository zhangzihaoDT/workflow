#!/usr/bin/env python3
"""
ABå¯¹æ¯”åˆ†æè„šæœ¬
ç”¨äºå¯¹æ¯”ä¸¤ä¸ªæ ·æœ¬ä¹‹é—´çš„å·®å¼‚ï¼Œå‘ç°å¼‚å¸¸
ä½¿ç”¨Gradioä½œä¸ºå‰ç«¯ç•Œé¢
"""

import pandas as pd
import gradio as gr
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any
import logging
from pathlib import Path
# å…¼å®¹ç›´æ¥è¿è¡Œä¸åŒ…è¿è¡Œä¸¤ç§æ–¹å¼çš„å¯¼å…¥
try:
    from id_card_validator import validate_id_card
except Exception:
    try:
        from skills.id_card_validator import validate_id_card
    except Exception:
        from .id_card_validator import validate_id_card

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_PATH = "/Users/zihao_/Documents/coding/dataset/formatted/intention_order_analysis.parquet"

class ABComparisonAnalyzer:
    def __init__(self):
        self.df = None
        self.load_data()
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        try:
            self.df = pd.read_parquet(DATA_PATH)
            self.df['Intention_Payment_Time'] = pd.to_datetime(self.df['Intention_Payment_Time'])
            if 'intention_refund_time' in self.df.columns:
                self.df['intention_refund_time'] = pd.to_datetime(self.df['intention_refund_time'])
            logger.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå…±{len(self.df)}æ¡è®°å½•")
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise e
    
    def get_vehicle_types(self) -> List[str]:
        """è·å–è½¦å‹åˆ—è¡¨"""
        if 'è½¦å‹åˆ†ç»„' in self.df.columns:
            return sorted(self.df['è½¦å‹åˆ†ç»„'].unique().tolist())
        return []
    
    def get_pre_vehicle_model_types(self) -> List[str]:
        """è·å–äº§å“åˆ†ç±»åˆ—è¡¨ï¼šåŸºäºProduct Nameåˆ†ä¸ºå¢ç¨‹å’Œçº¯ç”µ"""
        return ["å¢ç¨‹", "çº¯ç”µ"]
    
    def get_battery_types(self) -> List[str]:
        """è·å–ç”µæ± ç±»å‹åˆ†ç±»åˆ—è¡¨ï¼šåŸºäºbusiness_definition.jsonä¸­çš„battery_typeså®šä¹‰"""
        try:
            with open("/Users/zihao_/Documents/github/W35_workflow/business_definition.json", "r", encoding="utf-8") as f:
                business_def = json.load(f)
                
            if "battery_types" in business_def:
                # è¿”å›ç”µæ± ç±»å‹åˆ†ç±»ï¼ˆA_LFP, B_NCM, C_EXCLUDEï¼‰
                return list(business_def["battery_types"].keys())
            return []
        except Exception as e:
            logger.error(f"è·å–ç”µæ± ç±»å‹åˆ†ç±»å¤±è´¥: {str(e)}")
            return []
            
    def get_battery_type_mapping(self) -> Dict[str, str]:
        """è·å–è½¦å‹åˆ°ç”µæ± ç±»å‹çš„æ˜ å°„"""
        try:
            with open("/Users/zihao_/Documents/github/W35_workflow/business_definition.json", "r", encoding="utf-8") as f:
                business_def = json.load(f)
            
            if "battery_types" not in business_def:
                return {}
                
            # åˆ›å»ºè½¦å‹åˆ°ç”µæ± ç±»å‹çš„æ˜ å°„
            model_to_battery_type = {}
            for battery_type, models in business_def["battery_types"].items():
                for model in models:
                    model_to_battery_type[model] = battery_type
                    
            return model_to_battery_type
        except Exception as e:
            logger.error(f"è·å–ç”µæ± ç±»å‹æ˜ å°„å¤±è´¥: {str(e)}")
            return {}
    
    def get_parent_regions(self) -> List[str]:
        """è·å–Parent Region Nameåˆ—è¡¨"""
        if 'Parent Region Name' in self.df.columns:
            return sorted(self.df['Parent Region Name'].dropna().unique().tolist())
        return []
    
    def get_date_range(self) -> Tuple[str, str]:
        """è·å–è®¢å•åˆ›å»ºæ—¶é—´çš„æ—¥æœŸèŒƒå›´"""
        if 'Order_Create_Time' in self.df.columns:
            create_data = self.df['Order_Create_Time'].dropna()
            if not create_data.empty:
                min_date = create_data.min().strftime('%Y-%m-%d')
                max_date = create_data.max().strftime('%Y-%m-%d')
                return min_date, max_date
        # å¦‚æœOrder_Create_Timeä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œä½¿ç”¨Intention_Payment_Timeä½œä¸ºå¤‡é€‰
        min_date = self.df['Intention_Payment_Time'].min().strftime('%Y-%m-%d')
        max_date = self.df['Intention_Payment_Time'].max().strftime('%Y-%m-%d')
        return min_date, max_date
    
    def get_refund_date_range(self) -> Tuple[str, str]:
        """è·å–é€€è®¢æ—¶é—´èŒƒå›´"""
        if 'intention_refund_time' in self.df.columns:
            refund_data = self.df['intention_refund_time'].dropna()
            if not refund_data.empty:
                min_date = refund_data.min().strftime('%Y-%m-%d')
                max_date = refund_data.max().strftime('%Y-%m-%d')
                return min_date, max_date
        return '', ''
    
    def get_order_create_date_range(self) -> Tuple[str, str]:
        """è·å–è®¢å•åˆ›å»ºæ—¶é—´èŒƒå›´"""
        if 'Order_Create_Time' in self.df.columns:
            create_data = self.df['Order_Create_Time'].dropna()
            if not create_data.empty:
                min_date = create_data.min().strftime('%Y-%m-%d')
                max_date = create_data.max().strftime('%Y-%m-%d')
                return min_date, max_date
        return '', ''
    
    def get_lock_date_range(self) -> Tuple[str, str]:
        """è·å–é”å•æ—¶é—´èŒƒå›´ï¼Œå¦‚æœæ²¡æœ‰é”å•æ•°æ®åˆ™è¿”å›é»˜è®¤èŒƒå›´"""
        if 'Lock_Time' in self.df.columns:
            lock_data = self.df['Lock_Time'].dropna()
            if not lock_data.empty:
                min_date = lock_data.min().strftime('%Y-%m-%d')
                max_date = lock_data.max().strftime('%Y-%m-%d')
                return min_date, max_date
        
        # å¦‚æœæ²¡æœ‰é”å•æ•°æ®ï¼Œè¿”å›åŸºäºè®¢å•åˆ›å»ºæ—¶é—´çš„é»˜è®¤èŒƒå›´
        order_min, order_max = self.get_date_range()
        return order_min, order_max
    
    def filter_sample(self, start_date: str = '', end_date: str = '', vehicle_types: List[str] = None, 
                     include_refund: bool = False, refund_start_date: str = '', refund_end_date: str = '',
                     pre_vehicle_model_types: List[str] = None, parent_regions: List[str] = None,
                     vehicle_groups: List[str] = None, refund_only: bool = False, 
                     locked_only: bool = False, order_create_start_date: str = '', order_create_end_date: str = '',
                     lock_start_date: str = '', lock_end_date: str = '',
                     exclude_refund: bool = False, exclude_locked: bool = False,
                     include_invalid_id: bool = True,
                     battery_types: List[str] = None, repeat_buyer_only: bool = False, exclude_repeat_buyer: bool = False) -> pd.DataFrame:
        """ç­›é€‰æ ·æœ¬æ•°æ®"""
        # ä»å®Œæ•´æ•°æ®å¼€å§‹
        mask = pd.Series([True] * len(self.df), index=self.df.index)
        
        # 1. å°è®¢æ—¶é—´èŒƒå›´ç­›é€‰
        if start_date and end_date:
            mask = mask & (self.df['Intention_Payment_Time'] >= start_date) & \
                   (self.df['Intention_Payment_Time'] <= end_date)
        
        # 2. å°è®¢é€€è®¢æ—¶é—´èŒƒå›´ç­›é€‰
        if refund_start_date and refund_end_date and 'intention_refund_time' in self.df.columns:
            refund_mask = (self.df['intention_refund_time'] >= refund_start_date) & \
                         (self.df['intention_refund_time'] <= refund_end_date)
            mask = mask & refund_mask
        
        # 3. è®¢å•åˆ›å»ºæ—¶é—´èŒƒå›´ç­›é€‰
        if order_create_start_date and order_create_end_date and 'Order_Create_Time' in self.df.columns:
            create_mask = (self.df['Order_Create_Time'] >= order_create_start_date) & \
                         (self.df['Order_Create_Time'] <= order_create_end_date)
            mask = mask & create_mask
        
        # 4. é”å•æ—¶é—´èŒƒå›´ç­›é€‰
        if lock_start_date and lock_end_date and 'Lock_Time' in self.df.columns:
            lock_mask = (self.df['Lock_Time'] >= lock_start_date) & \
                       (self.df['Lock_Time'] <= lock_end_date)
            mask = mask & lock_mask
        
        # 5. äº§å“åˆ†ç±»ç­›é€‰ï¼ˆåŸºäºProduct Nameï¼‰
        if pre_vehicle_model_types and 'Product Name' in self.df.columns:
            product_mask = pd.Series([False] * len(self.df), index=self.df.index)
            
            for category in pre_vehicle_model_types:
                if category == "å¢ç¨‹":
                    # äº§å“åç§°ä¸­åŒ…å«"æ–°ä¸€ä»£"å’Œæ•°å­—52æˆ–66çš„ä¸ºå¢ç¨‹
                    category_mask = (
                        self.df['Product Name'].str.contains('æ–°ä¸€ä»£', na=False) & 
                        (self.df['Product Name'].str.contains('52', na=False) | 
                         self.df['Product Name'].str.contains('66', na=False))
                    )
                elif category == "çº¯ç”µ":
                    # å…¶ä»–äº§å“ä¸ºçº¯ç”µ
                    category_mask = ~(
                        self.df['Product Name'].str.contains('æ–°ä¸€ä»£', na=False) & 
                        (self.df['Product Name'].str.contains('52', na=False) | 
                         self.df['Product Name'].str.contains('66', na=False))
                    )
                else:
                    category_mask = pd.Series([False] * len(self.df), index=self.df.index)
                
                product_mask = product_mask | category_mask
            
            mask = mask & product_mask
        
        # 6. Parent Region Nameç­›é€‰
        # 5.1 èº«ä»½è¯å·å¼‚å¸¸æ£€æµ‹ï¼ˆä»…åœ¨æä¾› Buyer Identity No å­—æ®µæ—¶ç”Ÿæ•ˆï¼‰
        if (not include_invalid_id) and ('Buyer Identity No' in self.df.columns):
            id_series = self.df['Buyer Identity No']
            # æ£€æµ‹å¼‚å¸¸èº«ä»½è¯å·ï¼šç©ºå€¼ã€é•¿åº¦ä¸è¶³18ä½ã€æ ¡éªŒå¤±è´¥
            def is_valid_id_card(id_val):
                if pd.isna(id_val):
                    return False  # ç©ºå€¼è§†ä¸ºå¼‚å¸¸
                id_str = str(id_val).strip()
                if id_str == '' or len(id_str) != 18:
                    return False  # ç©ºå­—ç¬¦ä¸²æˆ–é•¿åº¦ä¸è¶³18ä½è§†ä¸ºå¼‚å¸¸
                return validate_id_card(id_str)  # æ ¡éªŒå¤±è´¥è§†ä¸ºå¼‚å¸¸
            
            # ä¿ç•™æ­£å¸¸èº«ä»½è¯å·ï¼Œå‰”é™¤å¼‚å¸¸èº«ä»½è¯å·
            validity_mask = id_series.apply(is_valid_id_card)
            mask = mask & validity_mask
        
        if parent_regions and 'Parent Region Name' in self.df.columns:
            mask = mask & (self.df['Parent Region Name'].isin(parent_regions))
        
        # 7. è½¦å‹åˆ†ç»„ç­›é€‰
        if vehicle_groups and 'è½¦å‹åˆ†ç»„' in self.df.columns:
            mask = mask & (self.df['è½¦å‹åˆ†ç»„'].isin(vehicle_groups))
        elif vehicle_types and 'è½¦å‹åˆ†ç»„' in self.df.columns:  # ä¿æŒå‘åå…¼å®¹
            mask = mask & (self.df['è½¦å‹åˆ†ç»„'].isin(vehicle_types))
            
        # 8. ç”µæ± ç±»å‹ç­›é€‰
        if battery_types and 'Product Name' in self.df.columns:
            battery_type_mapping = self.get_battery_type_mapping()
            if battery_type_mapping:
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶åˆ—æ¥æ ‡è®°ç”µæ± ç±»å‹
                temp_df = self.df.copy()
                temp_df['battery_type'] = temp_df['Product Name'].map(battery_type_mapping)
                # ç­›é€‰æŒ‡å®šç”µæ± ç±»å‹çš„æ•°æ®
                battery_mask = temp_df['battery_type'].isin(battery_types)
                mask = mask & battery_mask
        
        sample_data = self.df[mask].copy()
        
        # 8. æ˜¯å¦é€€è®¢ç­›é€‰
        if refund_only and 'intention_refund_time' in self.df.columns:
            sample_data = sample_data[sample_data['intention_refund_time'].notna()]
        elif include_refund and 'intention_refund_time' in self.df.columns:  # ä¿æŒå‘åå…¼å®¹
            sample_data = sample_data[sample_data['intention_refund_time'].notna()]
        
        # 9. æ˜¯å¦é”å•ç­›é€‰
        if locked_only and 'Lock_Time' in self.df.columns:
            sample_data = sample_data[sample_data['Lock_Time'].notna()]
        
        # 10. æ’é™¤é€€è®¢æ•°æ®
        if exclude_refund and 'intention_refund_time' in self.df.columns:
            sample_data = sample_data[sample_data['intention_refund_time'].isna()]
        
        # 11. æ’é™¤é”å•æ•°æ®
        if exclude_locked and 'Lock_Time' in self.df.columns:
            sample_data = sample_data[sample_data['Lock_Time'].isna()]
        
        # 12. å¤è´­ç”¨æˆ·ç­›é€‰
        if (repeat_buyer_only or exclude_repeat_buyer) and 'Buyer Identity No' in self.df.columns and 'Invoice_Upload_Time' in self.df.columns:
            # æ£€æŸ¥äº’æ–¥æ€§ï¼šå¦‚æœåŒæ—¶è®¾ç½®ä¸¤ä¸ªé€‰é¡¹ï¼Œè¿”å›ç©ºç»“æœ
            if repeat_buyer_only and exclude_repeat_buyer:
                # åŒæ—¶è®¾ç½®ä¸¤ä¸ªé€‰é¡¹æ—¶è¿”å›ç©ºDataFrame
                sample_data = sample_data.iloc[0:0]  # è¿”å›ç©ºDataFrameä½†ä¿æŒåˆ—ç»“æ„
            else:
                # å¤è´­ç”¨æˆ·çš„åˆ¤æ–­æ ‡å‡†ï¼š
                # 1. ä¸€ä¸ªä¹°å®¶æœ‰å¤šä¸ªè®¢å•ï¼ˆåŸºäºèº«ä»½è¯å·ï¼‰
                # 2. ä¸”è¿™æ‰¹è®¢å•ä¸­å«æœ‰"Invoice_Upload_Time"
                # 3. å¹¶ä¸”ï¼Œè¯¥"Invoice_Upload_Time"è¿˜åº”è¯¥<ç”¨æˆ·æ§ä»¶é€‰æ‹©çš„"é”å•å¼€å§‹æ—¥æœŸ"
                
                # è·å–é”å•å¼€å§‹æ—¥æœŸï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä½¿ç”¨å½“å‰ç­›é€‰çš„å¼€å§‹æ—¥æœŸ
                reference_date = lock_start_date if lock_start_date else start_date
                
                if reference_date:
                    # æ‰¾å‡ºå¤è´­ç”¨æˆ·çš„èº«ä»½è¯å·
                    repeat_buyer_ids = set()
                    
                    # æŒ‰èº«ä»½è¯å·åˆ†ç»„ï¼Œæ‰¾å‡ºæœ‰å¤šä¸ªè®¢å•çš„ä¹°å®¶
                    buyer_groups = self.df.groupby('Buyer Identity No')
                    for buyer_id, group in buyer_groups:
                        if len(group) > 1:  # æœ‰å¤šä¸ªè®¢å•
                            # æ£€æŸ¥æ˜¯å¦æœ‰Invoice_Upload_Timeä¸”æ—©äºå‚è€ƒæ—¥æœŸ
                            invoice_times = group['Invoice_Upload_Time'].dropna()
                            if len(invoice_times) > 0:
                                # æ£€æŸ¥æ˜¯å¦æœ‰Invoice_Upload_Timeæ—©äºå‚è€ƒæ—¥æœŸ
                                early_invoices = invoice_times[invoice_times < reference_date]
                                if len(early_invoices) > 0:
                                    repeat_buyer_ids.add(buyer_id)
                    
                    # æ ¹æ®é€‰æ‹©çš„æ¨¡å¼è¿›è¡Œç­›é€‰
                    if repeat_buyer_only and repeat_buyer_ids:
                        # ä»…ä¿ç•™å¤è´­ç”¨æˆ·
                        sample_data = sample_data[sample_data['Buyer Identity No'].isin(repeat_buyer_ids)]
                    elif exclude_repeat_buyer and repeat_buyer_ids:
                        # æ’é™¤å¤è´­ç”¨æˆ·
                        sample_data = sample_data[~sample_data['Buyer Identity No'].isin(repeat_buyer_ids)]
        
        return sample_data
    
    def analyze_region_distribution(self, sample_a: pd.DataFrame, sample_b: pd.DataFrame, 
                                  parent_regions_filter: List[str] = None) -> List[Dict]:
        """åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹"""
        anomalies = []
        
        # å¦‚æœæœ‰Parent Regionç­›é€‰ï¼Œåªæ£€æµ‹ç›¸å…³çš„åœ°åŒº
        if parent_regions_filter:
            # åªæ£€æµ‹ç­›é€‰èŒƒå›´å†…çš„Provinceå’ŒCity
            filtered_sample_a = sample_a[sample_a['Parent Region Name'].isin(parent_regions_filter)]
            filtered_sample_b = sample_b[sample_b['Parent Region Name'].isin(parent_regions_filter)]
            
            # æ£€æŸ¥Provinceå’ŒCityç»´åº¦
            region_columns = ['License Province', 'license_city_level', 'License City']
            samples_to_check = [(filtered_sample_a, filtered_sample_b, 'filtered')]
        else:
            # æ£€æŸ¥æ‰€æœ‰åœ°åŒºç»´åº¦
            region_columns = ['Parent Region Name', 'License Province', 'license_city_level', 'License City']
            samples_to_check = [(sample_a, sample_b, 'all')]
        
        for sample_a_check, sample_b_check, scope in samples_to_check:
            for region_col in region_columns:
                if region_col not in sample_a_check.columns or region_col not in sample_b_check.columns:
                    continue
                
                # è®¡ç®—åˆ†å¸ƒ
                dist_a = sample_a_check[region_col].value_counts(normalize=True)
                dist_b = sample_b_check[region_col].value_counts(normalize=True)
                
                # è®¡ç®—ç»å¯¹æ•°é‡
                count_a = sample_a_check[region_col].value_counts()
                count_b = sample_b_check[region_col].value_counts()
                
                # è·å–æ‰€æœ‰åœ°åŒº
                all_regions = set(dist_a.index) | set(dist_b.index)
                
                # æ£€æŸ¥å¼‚å¸¸ï¼ˆä½¿ç”¨"æˆ–"é€»è¾‘ï¼šç»“æ„å æ¯”å˜åŒ– OR å¯¹æ¯”ç¯æ¯”å˜åŒ–ï¼‰
                for region in all_regions:
                    ratio_a = dist_a.get(region, 0)
                    ratio_b = dist_b.get(region, 0)
                    abs_a = count_a.get(region, 0)
                    abs_b = count_b.get(region, 0)
                    
                    # æ¡ä»¶1ï¼šç»“æ„å æ¯”å¼‚å¸¸ï¼ˆå æ¯”è¶…è¿‡1%ä¸”å˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
                    structure_anomaly = False
                    if ratio_b > 0 and ratio_a > 0.01:
                        change_rate = abs(ratio_a - ratio_b) / ratio_b
                        if change_rate > 0.1:
                            structure_anomaly = True
                    
                    # æ¡ä»¶2ï¼šç¯æ¯”å˜åŒ–å¼‚å¸¸ï¼ˆåŸºäºç»å¯¹å€¼çš„ç¯æ¯”å˜åŒ–ï¼‰
                    comparison_anomaly = False
                    if abs_b > 0:
                        abs_relative_change = abs((abs_a - abs_b) / abs_b)
                        # æ£€æµ‹ç¯æ¯”å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µï¼ˆåŒ…æ‹¬100%å¢é•¿ï¼‰
                        if abs_relative_change > 0.5:
                            comparison_anomaly = True
                    elif abs_a > 0:  # æ–°å‡ºç°çš„æƒ…å†µä¹Ÿç®—ç¯æ¯”å¼‚å¸¸
                        comparison_anomaly = True
                    
                    # æ¡ä»¶3ï¼šæ–°å‡ºç°çš„åœ°åŒº
                    new_region_anomaly = ratio_a > 0.01 and ratio_b == 0
                    
                    # "æˆ–"é€»è¾‘ï¼šæ»¡è¶³ä»»ä¸€æ¡ä»¶å³ä¸ºå¼‚å¸¸
                    if structure_anomaly or comparison_anomaly or new_region_anomaly:
                        if ratio_b > 0:
                             change_rate = abs(ratio_a - ratio_b) / ratio_b
                             # è®¡ç®—ç»å¯¹å€¼çš„ç¯æ¯”å˜åŒ–
                             if abs_b > 0:
                                 relative_change = (abs_a - abs_b) / abs_b
                             else:
                                 relative_change = float('inf') if abs_a > 0 else 0
                             
                             anomaly_type = []
                             if structure_anomaly:
                                 anomaly_type.append("ç»“æ„å æ¯”")
                             if comparison_anomaly:
                                 anomaly_type.append("å¯¹æ¯”ç¯æ¯”")
                             
                             anomalies.append({
                                 'type': 'åœ°åŒºåˆ†å¸ƒ',
                                 'item': f'[{region_col}] {region}',
                                 'anomaly_type': '/'.join(anomaly_type),
                                 'sample_a_abs': abs_a,
                                 'sample_b_abs': abs_b,
                                 'sample_a_ratio': ratio_a,
                                 'sample_b_ratio': ratio_b,
                                 'change': ratio_a - ratio_b,
                                 'relative_change': relative_change
                             })
                        else:  # æ–°å‡ºç°çš„åœ°åŒº
                            anomalies.append({
                                'type': 'åœ°åŒºåˆ†å¸ƒ',
                                'item': f'[{region_col}] {region}',
                                'anomaly_type': 'æ–°å¢åœ°åŒº',
                                'sample_a_abs': abs_a,
                                'sample_b_abs': abs_b,
                                'sample_a_ratio': ratio_a,
                                'sample_b_ratio': ratio_b,
                                'change': ratio_a,
                                'relative_change': float('inf')
                            })
        
        return anomalies
    
    def analyze_channel_structure(self, sample_a: pd.DataFrame, sample_b: pd.DataFrame) -> List[Dict]:
        """æ¸ é“ç»“æ„å¼‚å¸¸æ£€æµ‹"""
        anomalies = []
        
        # ä½¿ç”¨ä¸main.pyä¸€è‡´çš„å­—æ®µå
        channel_columns = ['first_middle_channel_name', 'Channel', 'Sub_Channel', 'Dealer_Name']
        
        for channel_col in channel_columns:
            if channel_col not in sample_a.columns or channel_col not in sample_b.columns:
                continue
            
            # è®¡ç®—åˆ†å¸ƒ
            dist_a = sample_a[channel_col].value_counts(normalize=True)
            dist_b = sample_b[channel_col].value_counts(normalize=True)
            
            # è®¡ç®—ç»å¯¹æ•°é‡
            count_a = sample_a[channel_col].value_counts()
            count_b = sample_b[channel_col].value_counts()
            
            # è·å–æ‰€æœ‰æ¸ é“
            all_channels = set(dist_a.index) | set(dist_b.index)
            
            # æ£€æŸ¥å¼‚å¸¸ï¼ˆä½¿ç”¨"æˆ–"é€»è¾‘ï¼šç»“æ„å æ¯”å˜åŒ– OR å¯¹æ¯”ç¯æ¯”å˜åŒ–ï¼‰
            for channel in all_channels:
                ratio_a = dist_a.get(channel, 0)
                ratio_b = dist_b.get(channel, 0)
                abs_a = count_a.get(channel, 0)
                abs_b = count_b.get(channel, 0)
                
                # æ¡ä»¶1ï¼šç»“æ„å æ¯”å¼‚å¸¸ï¼ˆå æ¯”è¶…è¿‡1%ä¸”å˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
                structure_anomaly = False
                if ratio_b > 0 and ratio_a > 0.01:
                    change_rate = abs(ratio_a - ratio_b) / ratio_b
                    if change_rate > 0.15:
                        structure_anomaly = True
                
                # æ¡ä»¶2ï¼šç¯æ¯”å˜åŒ–å¼‚å¸¸ï¼ˆåŸºäºç»å¯¹å€¼çš„ç¯æ¯”å˜åŒ–ï¼‰
                comparison_anomaly = False
                if abs_b > 0:
                    abs_relative_change = abs((abs_a - abs_b) / abs_b)
                    # æ£€æµ‹ç¯æ¯”å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µï¼ˆåŒ…æ‹¬100%å¢é•¿ï¼‰
                    if abs_relative_change > 0.5:
                        comparison_anomaly = True
                elif abs_a > 0:  # æ–°å‡ºç°çš„æƒ…å†µä¹Ÿç®—ç¯æ¯”å¼‚å¸¸
                    comparison_anomaly = True
                
                # æ¡ä»¶3ï¼šæ–°å‡ºç°çš„æ¸ é“
                new_channel_anomaly = ratio_a > 0.01 and ratio_b == 0
                
                # "æˆ–"é€»è¾‘ï¼šæ»¡è¶³ä»»ä¸€æ¡ä»¶å³ä¸ºå¼‚å¸¸
                if structure_anomaly or comparison_anomaly or new_channel_anomaly:
                    if ratio_b > 0:
                         change_rate = abs(ratio_a - ratio_b) / ratio_b
                         # è®¡ç®—ç»å¯¹å€¼çš„ç¯æ¯”å˜åŒ–
                         if abs_b > 0:
                             relative_change = (abs_a - abs_b) / abs_b
                         else:
                             relative_change = float('inf') if abs_a > 0 else 0
                         
                         anomaly_type = []
                         if structure_anomaly:
                             anomaly_type.append("ç»“æ„å æ¯”")
                         if comparison_anomaly:
                             anomaly_type.append("å¯¹æ¯”ç¯æ¯”")
                         
                         anomalies.append({
                             'type': 'æ¸ é“ç»“æ„',
                             'item': f'[{channel_col}] {channel}',
                             'anomaly_type': '/'.join(anomaly_type),
                             'sample_a_abs': abs_a,
                             'sample_b_abs': abs_b,
                             'sample_a_ratio': ratio_a,
                             'sample_b_ratio': ratio_b,
                             'change': ratio_a - ratio_b,
                             'relative_change': relative_change
                         })
                    else:  # æ–°å‡ºç°çš„æ¸ é“
                        anomalies.append({
                            'type': 'æ¸ é“ç»“æ„',
                            'item': f'[{channel_col}] {channel}',
                            'anomaly_type': 'æ–°å¢æ¸ é“',
                            'sample_a_abs': abs_a,
                            'sample_b_abs': abs_b,
                            'sample_a_ratio': ratio_a,
                            'sample_b_ratio': ratio_b,
                            'change': ratio_a,
                            'relative_change': float('inf')
                        })
        
        return anomalies
    
    def analyze_demographic_structure(self, sample_a: pd.DataFrame, sample_b: pd.DataFrame) -> List[Dict]:
        """äººç¾¤ç»“æ„å¼‚å¸¸æ£€æµ‹"""
        anomalies = []
        
        # æ€§åˆ«åˆ†å¸ƒæ£€æŸ¥
        if 'order_gender' in sample_a.columns and 'order_gender' in sample_b.columns:
            gender_dist_a = sample_a['order_gender'].value_counts(normalize=True)
            gender_dist_b = sample_b['order_gender'].value_counts(normalize=True)
            
            # è®¡ç®—ç»å¯¹æ•°é‡
            gender_count_a = sample_a['order_gender'].value_counts()
            gender_count_b = sample_b['order_gender'].value_counts()
            
            for gender in set(gender_dist_a.index) | set(gender_dist_b.index):
                ratio_a = gender_dist_a.get(gender, 0)
                ratio_b = gender_dist_b.get(gender, 0)
                abs_a = gender_count_a.get(gender, 0)
                abs_b = gender_count_b.get(gender, 0)
                
                # æ¡ä»¶1ï¼šç»“æ„å æ¯”å¼‚å¸¸ï¼ˆå˜åŒ–å¹…åº¦è¶…è¿‡10%ï¼‰
                structure_anomaly = False
                if ratio_b > 0:
                    change_rate = abs(ratio_a - ratio_b) / ratio_b
                    if change_rate > 0.1:
                        structure_anomaly = True
                
                # æ¡ä»¶2ï¼šç¯æ¯”å˜åŒ–å¼‚å¸¸ï¼ˆåŸºäºç»å¯¹å€¼çš„ç¯æ¯”å˜åŒ–ï¼‰
                comparison_anomaly = False
                if abs_b > 0:
                    abs_relative_change = abs((abs_a - abs_b) / abs_b)
                    # æ£€æµ‹ç¯æ¯”å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µï¼ˆåŒ…æ‹¬100%å¢é•¿ï¼‰
                    if abs_relative_change > 0.5:
                        comparison_anomaly = True
                elif abs_a > 0:  # æ–°å‡ºç°çš„æƒ…å†µä¹Ÿç®—ç¯æ¯”å¼‚å¸¸
                    comparison_anomaly = True
                
                # "æˆ–"é€»è¾‘ï¼šæ»¡è¶³ä»»ä¸€æ¡ä»¶å³ä¸ºå¼‚å¸¸
                if structure_anomaly or comparison_anomaly:
                     change_rate = abs(ratio_a - ratio_b) / ratio_b
                     # è®¡ç®—ç»å¯¹å€¼çš„ç¯æ¯”å˜åŒ–
                     if abs_b > 0:
                         relative_change = (abs_a - abs_b) / abs_b
                     else:
                         relative_change = float('inf') if abs_a > 0 else 0
                     
                     anomaly_type = []
                     if structure_anomaly:
                         anomaly_type.append("ç»“æ„å æ¯”")
                     if comparison_anomaly:
                         anomaly_type.append("å¯¹æ¯”ç¯æ¯”")
                     
                     anomalies.append({
                         'type': 'äººç¾¤ç»“æ„',
                         'item': f'{gender}æ€§åˆ«',
                         'anomaly_type': '/'.join(anomaly_type),
                         'sample_a_abs': abs_a,
                         'sample_b_abs': abs_b,
                         'sample_a_ratio': ratio_a,
                         'sample_b_ratio': ratio_b,
                         'change': ratio_a - ratio_b,
                         'relative_change': relative_change
                     })
        
        # å¹´é¾„åˆ†å¸ƒæ£€æŸ¥ - ä½¿ç”¨buyer_ageå­—æ®µåˆ›å»ºå¹´é¾„æ®µ
        if 'buyer_age' in sample_a.columns and 'buyer_age' in sample_b.columns:
            # åˆ›å»ºå¹´é¾„æ®µ
            def create_age_groups(df):
                df = df.copy()
                df['age_group'] = pd.cut(df['buyer_age'], 
                                       bins=[0, 25, 35, 45, 55, 100], 
                                       labels=['25å²ä»¥ä¸‹', '25-35å²', '35-45å²', '45-55å²', '55å²ä»¥ä¸Š'],
                                       right=False)
                return df
            
            sample_a_with_age = create_age_groups(sample_a)
            sample_b_with_age = create_age_groups(sample_b)
            
            age_dist_a = sample_a_with_age['age_group'].value_counts(normalize=True)
            age_dist_b = sample_b_with_age['age_group'].value_counts(normalize=True)
            
            # è®¡ç®—ç»å¯¹æ•°é‡
            age_count_a = sample_a_with_age['age_group'].value_counts()
            age_count_b = sample_b_with_age['age_group'].value_counts()
            
            for age_group in set(age_dist_a.index) | set(age_dist_b.index):
                if pd.isna(age_group):  # è·³è¿‡NaNå€¼
                    continue
                ratio_a = age_dist_a.get(age_group, 0)
                ratio_b = age_dist_b.get(age_group, 0)
                abs_a = age_count_a.get(age_group, 0)
                abs_b = age_count_b.get(age_group, 0)
                
                # æ¡ä»¶1ï¼šç»“æ„å æ¯”å¼‚å¸¸ï¼ˆå æ¯”è¶…è¿‡1%ä¸”å˜åŒ–å¹…åº¦è¶…è¿‡15%ï¼‰
                structure_anomaly = False
                if ratio_b > 0 and ratio_a > 0.01:
                    change_rate = abs(ratio_a - ratio_b) / ratio_b
                    if change_rate > 0.15:
                        structure_anomaly = True
                
                # æ¡ä»¶2ï¼šç¯æ¯”å˜åŒ–å¼‚å¸¸ï¼ˆåŸºäºç»å¯¹å€¼çš„ç¯æ¯”å˜åŒ–ï¼‰
                comparison_anomaly = False
                if abs_b > 0:
                    abs_relative_change = abs((abs_a - abs_b) / abs_b)
                    # æ£€æµ‹ç¯æ¯”å˜åŒ–è¶…è¿‡50%çš„æƒ…å†µï¼ˆåŒ…æ‹¬100%å¢é•¿ï¼‰
                    if abs_relative_change > 0.5:
                        comparison_anomaly = True
                elif abs_a > 0:  # æ–°å‡ºç°çš„æƒ…å†µä¹Ÿç®—ç¯æ¯”å¼‚å¸¸
                    comparison_anomaly = True
                
                # "æˆ–"é€»è¾‘ï¼šæ»¡è¶³ä»»ä¸€æ¡ä»¶å³ä¸ºå¼‚å¸¸
                if structure_anomaly or comparison_anomaly:
                     change_rate = abs(ratio_a - ratio_b) / ratio_b
                     # è®¡ç®—ç»å¯¹å€¼çš„ç¯æ¯”å˜åŒ–
                     if abs_b > 0:
                         relative_change = (abs_a - abs_b) / abs_b
                     else:
                         relative_change = float('inf') if abs_a > 0 else 0
                     
                     anomaly_type = []
                     if structure_anomaly:
                         anomaly_type.append("ç»“æ„å æ¯”")
                     if comparison_anomaly:
                         anomaly_type.append("å¯¹æ¯”ç¯æ¯”")
                     
                     anomalies.append({
                         'type': 'äººç¾¤ç»“æ„',
                         'item': f'{age_group}å¹´é¾„æ®µ',
                         'anomaly_type': '/'.join(anomaly_type),
                         'sample_a_abs': abs_a,
                         'sample_b_abs': abs_b,
                         'sample_a_ratio': ratio_a,
                         'sample_b_ratio': ratio_b,
                         'change': ratio_a - ratio_b,
                         'relative_change': relative_change
                     })
        
        return anomalies
    
    def analyze_sales_agent_comparison(self, sample_a: pd.DataFrame, sample_b: pd.DataFrame) -> List[Dict]:
        """é”€å”®ä»£ç†åˆ†æå¯¹æ¯”"""
        try:
            # è¯»å–é”€å”®ä»£ç†æ•°æ®
            sales_info_path = "/Users/zihao_/Documents/coding/dataset/formatted/sales_info_data.json"
            try:
                with open(sales_info_path, 'r', encoding='utf-8') as f:
                    import json
                    sales_info_json = json.load(f)
                    sales_info_data = sales_info_json.get('data', [])
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–é”€å”®ä»£ç†æ•°æ®æ–‡ä»¶: {str(e)}")
                return []
            
            # æ„å»ºé”€å”®ä»£ç†æŸ¥æ‰¾é›†åˆ
            sales_agents_lookup = set()
            if isinstance(sales_info_data, list):
                for item in sales_info_data:
                    if isinstance(item, dict):
                        member_name = str(item.get('Member Name', '')).strip() if item.get('Member Name') else ''
                        member_code = str(item.get('Member Code', '')).strip() if item.get('Member Code') else ''
                        id_card = str(item.get('Id Card', '')).strip() if item.get('Id Card') else ''
                        
                        if member_name and member_code and id_card:
                            sales_agents_lookup.add((member_name, member_code, id_card))
            
            def analyze_sample_sales_agent(sample_df):
                if len(sample_df) == 0:
                    return {'total_orders': 0, 'total_unique_buyers': 0, 'agent_orders': 0, 'agent_ratio': 0.0, 'repeat_buyer_orders': 0, 'repeat_buyer_ratio': 0.0, 'unique_repeat_buyers': 0, 'repeat_buyer_orders_combo': 0, 'repeat_buyer_ratio_combo': 0.0, 'unique_repeat_buyers_combo': 0}
                
                # é¢„å¤„ç†å­—æ®µ
                sample_df = sample_df.copy()
                sample_df['clean_store_agent_name'] = sample_df['Store Agent Name'].fillna('').astype(str).str.strip()
                sample_df['clean_store_agent_id'] = sample_df['Store Agent Id'].fillna('').astype(str).str.strip()
                sample_df['clean_buyer_identity'] = sample_df['Buyer Identity No'].fillna('').astype(str).str.strip()
                
                # åˆ›å»ºç»„åˆå­—æ®µç”¨äºåŒ¹é…
                agent_combos = list(zip(
                    sample_df['clean_store_agent_name'],
                    sample_df['clean_store_agent_id'], 
                    sample_df['clean_buyer_identity']
                ))
                
                # è®¡ç®—åŒ¹é…çš„è®¢å•æ•°
                matched_combos = set(agent_combos) & sales_agents_lookup
                agent_orders = sum(1 for combo in agent_combos if combo in matched_combos)
                
                total_orders = len(sample_df)
                agent_ratio = agent_orders / total_orders if total_orders > 0 else 0.0
                
                # é‡å¤ä¹°å®¶åˆ†æ - å£å¾„1ï¼šä»…åŸºäºèº«ä»½è¯å·
                buyer_identity_counts = sample_df['Buyer Identity No'].value_counts()
                repeat_buyers = buyer_identity_counts[buyer_identity_counts >= 2]
                repeat_buyer_orders = repeat_buyers.sum()
                repeat_buyer_ratio = repeat_buyer_orders / total_orders if total_orders > 0 else 0.0
                unique_repeat_buyers = len(repeat_buyers)
                
                # è®¡ç®—æ€»ä¹°å®¶æ•°é‡ï¼ˆåŸºäºèº«ä»½è¯å·ï¼‰
                total_unique_buyers = sample_df['Buyer Identity No'].nunique()
                
                # é‡å¤ä¹°å®¶åˆ†æ - å£å¾„2ï¼šèº«ä»½è¯å·+æ‰‹æœºå·åŒé‡åŒ¹é…
                # åˆ›å»ºèº«ä»½è¯å·+æ‰‹æœºå·çš„ç»„åˆé”®
                sample_df_clean = sample_df.dropna(subset=['Buyer Identity No', 'Buyer Cell Phone'])
                buyer_combo_key = sample_df_clean['Buyer Identity No'].astype(str) + '_' + sample_df_clean['Buyer Cell Phone'].astype(str)
                buyer_combo_counts = buyer_combo_key.value_counts()
                repeat_buyers_combo = buyer_combo_counts[buyer_combo_counts >= 2]
                repeat_buyer_orders_combo = repeat_buyers_combo.sum()
                repeat_buyer_ratio_combo = repeat_buyer_orders_combo / total_orders if total_orders > 0 else 0.0
                unique_repeat_buyers_combo = len(repeat_buyers_combo)
                
                return {
                    'total_orders': total_orders,
                    'total_unique_buyers': total_unique_buyers,
                    'agent_orders': agent_orders,
                    'agent_ratio': agent_ratio,
                    'repeat_buyer_orders': repeat_buyer_orders,
                    'repeat_buyer_ratio': repeat_buyer_ratio,
                    'unique_repeat_buyers': unique_repeat_buyers,
                    'repeat_buyer_orders_combo': repeat_buyer_orders_combo,
                    'repeat_buyer_ratio_combo': repeat_buyer_ratio_combo,
                    'unique_repeat_buyers_combo': unique_repeat_buyers_combo
                }
            
            # åˆ†æä¸¤ä¸ªæ ·æœ¬
            result_a = analyze_sample_sales_agent(sample_a)
            result_b = analyze_sample_sales_agent(sample_b)
            
            return [{
                'type': 'é”€å”®ä»£ç†åˆ†æ',
                'sample_a': result_a,
                'sample_b': result_b
            }]
            
        except Exception as e:
            logger.error(f"é”€å”®ä»£ç†åˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def analyze_time_interval_comparison(self, sample_a: pd.DataFrame, sample_b: pd.DataFrame) -> List[Dict]:
        """æ—¶é—´é—´éš”åˆ†æå¯¹æ¯”"""
        try:
            def analyze_sample_time_intervals(sample_df):
                if len(sample_df) == 0:
                    return {'payment_to_refund': {}, 'payment_to_assign': {}, 'payment_to_lock': {}}
                
                sample_df = sample_df.copy()
                
                # ç¡®ä¿æ—¶é—´åˆ—ä¸ºdatetimeç±»å‹
                time_columns = ['Intention_Payment_Time', 'intention_refund_time', 'first_assign_time', 'Lock_Time']
                for col in time_columns:
                    if col in sample_df.columns:
                        sample_df[col] = pd.to_datetime(sample_df[col], errors='coerce')
                
                # è®¡ç®—æ”¯ä»˜åˆ°é€€æ¬¾çš„æ—¶é—´é—´éš”
                payment_to_refund_stats = {}
                if 'intention_refund_time' in sample_df.columns:
                    valid_refund_mask = sample_df['intention_refund_time'].notna() & sample_df['Intention_Payment_Time'].notna()
                    if valid_refund_mask.any():
                        intervals = (sample_df.loc[valid_refund_mask, 'intention_refund_time'] - 
                                   sample_df.loc[valid_refund_mask, 'Intention_Payment_Time']).dt.days
                        if len(intervals) > 0:
                            payment_to_refund_stats = {
                                'count': len(intervals),
                                'mean': float(intervals.mean()),
                                'median': float(intervals.median()),
                                'std': float(intervals.std()) if len(intervals) > 1 else 0.0
                            }
                
                # è®¡ç®—æ”¯ä»˜åˆ°åˆ†é…çš„æ—¶é—´é—´éš”
                payment_to_assign_stats = {}
                if 'first_assign_time' in sample_df.columns:
                    valid_assign_mask = sample_df['first_assign_time'].notna() & sample_df['Intention_Payment_Time'].notna()
                    if valid_assign_mask.any():
                        intervals = (sample_df.loc[valid_assign_mask, 'first_assign_time'] - 
                                   sample_df.loc[valid_assign_mask, 'Intention_Payment_Time']).dt.days
                        if len(intervals) > 0:
                            payment_to_assign_stats = {
                                'count': len(intervals),
                                'mean': float(intervals.mean()),
                                'median': float(intervals.median()),
                                'std': float(intervals.std()) if len(intervals) > 1 else 0.0
                            }
                
                # è®¡ç®—æ”¯ä»˜åˆ°é”å•ï¼ˆlock timeï¼‰çš„æ—¶é—´é—´éš”
                payment_to_lock_stats = {}
                if 'Lock_Time' in sample_df.columns:
                    valid_lock_mask = sample_df['Lock_Time'].notna() & sample_df['Intention_Payment_Time'].notna()
                    if valid_lock_mask.any():
                        intervals = (sample_df.loc[valid_lock_mask, 'Lock_Time'] - 
                                   sample_df.loc[valid_lock_mask, 'Intention_Payment_Time']).dt.days
                        if len(intervals) > 0:
                            payment_to_lock_stats = {
                                'count': len(intervals),
                                'mean': float(intervals.mean()),
                                'median': float(intervals.median()),
                                'std': float(intervals.std()) if len(intervals) > 1 else 0.0
                            }
                
                return {
                    'payment_to_refund': payment_to_refund_stats,
                    'payment_to_assign': payment_to_assign_stats,
                    'payment_to_lock': payment_to_lock_stats
                }
            
            # åˆ†æä¸¤ä¸ªæ ·æœ¬
            result_a = analyze_sample_time_intervals(sample_a)
            result_b = analyze_sample_time_intervals(sample_b)
            
            return [{
                'type': 'æ—¶é—´é—´éš”åˆ†æ',
                'sample_a': result_a,
                'sample_b': result_b
            }]
            
        except Exception as e:
            logger.error(f"æ—¶é—´é—´éš”åˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def generate_comparison_report(self, sample_a: pd.DataFrame, sample_b: pd.DataFrame, 
                                 sample_a_desc: str, sample_b_desc: str, 
                                 parent_regions_filter: List[str] = None,
                                 sample_a_label: str = "æ ·æœ¬A",
                                 sample_b_label: str = "æ ·æœ¬B") -> Tuple[str, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š"""
        # æ‰§è¡Œä¸‰ç§å¼‚å¸¸æ£€æµ‹
        region_anomalies = self.analyze_region_distribution(sample_a, sample_b, parent_regions_filter)
        channel_anomalies = self.analyze_channel_structure(sample_a, sample_b)
        demographic_anomalies = self.analyze_demographic_structure(sample_a, sample_b)
        
        # æ‰§è¡Œé”€å”®ä»£ç†åˆ†æå’Œæ—¶é—´é—´éš”åˆ†æ
        sales_agent_results = self.analyze_sales_agent_comparison(sample_a, sample_b)
        time_interval_results = self.analyze_time_interval_comparison(sample_a, sample_b)

        # åŠ¨æ€æ ·æœ¬åç§°ç”¨äºåˆ—åå’Œå±•ç¤ºï¼ˆå·²ä»è°ƒç”¨æ–¹ä¼ å…¥ï¼‰ï¼Œè‹¥ä¸ºç©ºåˆ™å›é€€åˆ°é»˜è®¤
        sample_a_label = sample_a_label or "æ ·æœ¬A"
        sample_b_label = sample_b_label or "æ ·æœ¬B"
        
        # åˆå¹¶æ‰€æœ‰å¼‚å¸¸æ•°æ®
        all_anomalies = region_anomalies + channel_anomalies + demographic_anomalies
        
        # åˆ›å»ºå¼‚å¸¸æ£€æµ‹ç»“æœè¡¨æ ¼
        anomaly_data = []
        
        for anomaly in all_anomalies:
            # ä¸ºå æ¯”å˜åŒ–æ·»åŠ é¢œè‰²æ ‡è¯†
            change_value = anomaly['change']
            if change_value > 0:
                change_display = f"<span style='color: red;'>+{change_value:.2%}</span>"
            elif change_value < 0:
                change_display = f"<span style='color: green;'>{change_value:.2%}</span>"
            else:
                change_display = f"{change_value:.2%}"
            
            # ä¸ºç¯æ¯”å˜åŒ–æ·»åŠ é¢œè‰²æ ‡è¯†
            relative_change_value = anomaly['relative_change']
            if relative_change_value == float('inf'):
                relative_change_display = "æ–°å¢"
            elif relative_change_value > 0:
                relative_change_display = f"<span style='color: red;'>+{relative_change_value:.1%}</span>"
            elif relative_change_value < 0:
                relative_change_display = f"<span style='color: green;'>{relative_change_value:.1%}</span>"
            else:
                relative_change_display = f"{relative_change_value:.1%}"
            
            anomaly_data.append({
                'å¼‚å¸¸ç±»å‹': anomaly['type'],
                'å¼‚å¸¸é¡¹ç›®': anomaly['item'],
                'å¼‚å¸¸å­ç±»': anomaly['anomaly_type'],
                f"{sample_a_label}ç»å¯¹å€¼": f"{anomaly['sample_a_abs']:,}",
                f"{sample_b_label}ç»å¯¹å€¼": f"{anomaly['sample_b_abs']:,}",
                f"{sample_a_label}å æ¯”": f"{anomaly['sample_a_ratio']:.2%}",
                f"{sample_b_label}å æ¯”": f"{anomaly['sample_b_ratio']:.2%}",
                'å æ¯”å˜åŒ–': change_display,
                'ç¯æ¯”å˜åŒ–': relative_change_display,
                'é£é™©ç­‰çº§': 'âš ï¸ ä¸­ç­‰'
            })
        
        # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œæ·»åŠ æ­£å¸¸çŠ¶æ€
        if not anomaly_data:
            anomaly_data.append({
                'å¼‚å¸¸ç±»å‹': 'æ•´ä½“è¯„ä¼°',
                'å¼‚å¸¸é¡¹ç›®': 'æ— å¼‚å¸¸',
                'å¼‚å¸¸å­ç±»': 'æ­£å¸¸',
                f"{sample_a_label}ç»å¯¹å€¼": '-',
                f"{sample_b_label}ç»å¯¹å€¼": '-',
                f"{sample_a_label}å æ¯”": '-',
                f"{sample_b_label}å æ¯”": '-',
                'å æ¯”å˜åŒ–': '-',
                'ç¯æ¯”å˜åŒ–': '-',
                'é£é™©ç­‰çº§': 'âœ… æ­£å¸¸'
            })
        
        # åˆ›å»ºDataFrame
        anomaly_df = pd.DataFrame(anomaly_data)
        
        # ç”Ÿæˆæ–‡å­—æŠ¥å‘Š
        total_anomalies = len(all_anomalies)
        
        report = f"""# ABå¯¹æ¯”åˆ†ææŠ¥å‘Š

## ğŸ“Š æ ·æœ¬ä¿¡æ¯
- **{sample_a_desc}** (å…±{len(sample_a):,}æ¡è®°å½•)
- **{sample_b_desc}** (å…±{len(sample_b):,}æ¡è®°å½•)
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“ˆ åˆ†æç»“æœç»Ÿè®¡
- **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸**: {len(region_anomalies)}ä¸ª
- **æ¸ é“ç»“æ„å¼‚å¸¸**: {len(channel_anomalies)}ä¸ª
- **äººç¾¤ç»“æ„å¼‚å¸¸**: {len(demographic_anomalies)}ä¸ª
- **æ€»å¼‚å¸¸æ•°é‡**: {total_anomalies}ä¸ª

## ğŸ¯ ç»¼åˆè¯„ä¼°
"""
        
        if total_anomalies == 0:
            report += "âœ… **æ•´ä½“è¯„ä¼°**: ä¸¤ä¸ªæ ·æœ¬ä¹‹é—´æœªå‘ç°æ˜¾è‘—å¼‚å¸¸ï¼Œç»“æ„åˆ†å¸ƒåŸºæœ¬ä¸€è‡´ã€‚\n"
        elif total_anomalies <= 5:
            report += f"âš ï¸ **æ•´ä½“è¯„ä¼°**: å‘ç°{total_anomalies}ä¸ªå¼‚å¸¸ç‚¹ï¼Œéœ€è¦å…³æ³¨ä½†æ•´ä½“é£é™©å¯æ§ã€‚\n"
        else:
            report += f"âŒ **æ•´ä½“è¯„ä¼°**: å‘ç°{total_anomalies}ä¸ªå¼‚å¸¸ç‚¹ï¼Œå­˜åœ¨è¾ƒå¤§ç»“æ„æ€§å·®å¼‚ï¼Œå»ºè®®æ·±å…¥åˆ†æã€‚\n"
        
        report += "\n## ğŸ’¡ å»ºè®®æªæ–½\n\n"
        if region_anomalies:
            report += "- **åœ°åŒºåˆ†å¸ƒ**: å…³æ³¨åœ°åŒºåˆ†å¸ƒå˜åŒ–çš„ä¸šåŠ¡åŸå› ï¼Œç¡®è®¤æ˜¯å¦ä¸ºæ­£å¸¸çš„å¸‚åœºç­–ç•¥è°ƒæ•´\n"
        if channel_anomalies:
            report += "- **æ¸ é“ç»“æ„**: åˆ†ææ¸ é“å˜åŒ–å¯¹ä¸šåŠ¡çš„å½±å“ï¼Œè¯„ä¼°æ¸ é“æ•ˆç‡å’Œè´¨é‡\n"
        if demographic_anomalies:
            report += "- **äººç¾¤ç»“æ„**: å…³æ³¨ç›®æ ‡å®¢ç¾¤çš„å˜åŒ–ï¼Œè°ƒæ•´è¥é”€ç­–ç•¥å’Œäº§å“å®šä½\n"
        
        if total_anomalies == 0:
            report += "- **æŒç»­ç›‘æ§**: å»ºè®®å®šæœŸè¿›è¡ŒABå¯¹æ¯”åˆ†æï¼ŒåŠæ—¶å‘ç°æ½œåœ¨å¼‚å¸¸\n"
        
        # ç”Ÿæˆé”€å”®ä»£ç†åˆ†æå¯¹æ¯”è¡¨æ ¼
        sales_agent_data = []
        if sales_agent_results:
            result = sales_agent_results[0]
            sample_a_result = result['sample_a']
            sample_b_result = result['sample_b']
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'æ€»è®¢å•æ•°',
                sample_a_label: f"{sample_a_result['total_orders']:,}",
                sample_b_label: f"{sample_b_result['total_orders']:,}",
                'å·®å¼‚': f"{sample_a_result['total_orders'] - sample_b_result['total_orders']:+,}"
            })
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'ä¹°å®¶æ•°é‡',
                sample_a_label: f"{sample_a_result['total_unique_buyers']:,}",
                sample_b_label: f"{sample_b_result['total_unique_buyers']:,}",
                'å·®å¼‚': f"{sample_a_result['total_unique_buyers'] - sample_b_result['total_unique_buyers']:+,}"
            })
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'é”€å”®ä»£ç†è®¢å•æ•°',
                sample_a_label: f"{sample_a_result['agent_orders']:,}",
                sample_b_label: f"{sample_b_result['agent_orders']:,}",
                'å·®å¼‚': f"{sample_a_result['agent_orders'] - sample_b_result['agent_orders']:+,}"
            })
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'é”€å”®ä»£ç†è®¢å•æ¯”ä¾‹',
                sample_a_label: f"{sample_a_result['agent_ratio']:.2%}",
                sample_b_label: f"{sample_b_result['agent_ratio']:.2%}",
                'å·®å¼‚': f"{sample_a_result['agent_ratio'] - sample_b_result['agent_ratio']:+.2%}"
            })
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'é‡å¤ä¹°å®¶è®¢å•æ•°',
                sample_a_label: f"{sample_a_result['repeat_buyer_orders']:,}",
                sample_b_label: f"{sample_b_result['repeat_buyer_orders']:,}",
                'å·®å¼‚': f"{sample_a_result['repeat_buyer_orders'] - sample_b_result['repeat_buyer_orders']:+,}"
            })
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'é‡å¤ä¹°å®¶è®¢å•æ¯”ä¾‹',
                sample_a_label: f"{sample_a_result['repeat_buyer_ratio']:.2%}",
                sample_b_label: f"{sample_b_result['repeat_buyer_ratio']:.2%}",
                'å·®å¼‚': f"{sample_a_result['repeat_buyer_ratio'] - sample_b_result['repeat_buyer_ratio']:+.2%}"
            })
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'é‡å¤ä¹°å®¶æ•°é‡',
                sample_a_label: f"{sample_a_result['unique_repeat_buyers']:,}",
                sample_b_label: f"{sample_b_result['unique_repeat_buyers']:,}",
                'å·®å¼‚': f"{sample_a_result['unique_repeat_buyers'] - sample_b_result['unique_repeat_buyers']:+,}"
            })
            
            # æ–°å¢ï¼šèº«ä»½è¯å·+æ‰‹æœºå·åŒé‡åŒ¹é…çš„é‡å¤ä¹°å®¶æŒ‡æ ‡
            sales_agent_data.append({
                'æŒ‡æ ‡': 'é‡å¤ä¹°å®¶è®¢å•æ•°(èº«ä»½è¯+æ‰‹æœº)',
                sample_a_label: f"{sample_a_result['repeat_buyer_orders_combo']:,}",
                sample_b_label: f"{sample_b_result['repeat_buyer_orders_combo']:,}",
                'å·®å¼‚': f"{sample_a_result['repeat_buyer_orders_combo'] - sample_b_result['repeat_buyer_orders_combo']:+,}"
            })
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'é‡å¤ä¹°å®¶è®¢å•æ¯”ä¾‹(èº«ä»½è¯+æ‰‹æœº)',
                sample_a_label: f"{sample_a_result['repeat_buyer_ratio_combo']:.2%}",
                sample_b_label: f"{sample_b_result['repeat_buyer_ratio_combo']:.2%}",
                'å·®å¼‚': f"{sample_a_result['repeat_buyer_ratio_combo'] - sample_b_result['repeat_buyer_ratio_combo']:+.2%}"
            })
            
            sales_agent_data.append({
                'æŒ‡æ ‡': 'é‡å¤ä¹°å®¶æ•°é‡(èº«ä»½è¯+æ‰‹æœº)',
                sample_a_label: f"{sample_a_result['unique_repeat_buyers_combo']:,}",
                sample_b_label: f"{sample_b_result['unique_repeat_buyers_combo']:,}",
                'å·®å¼‚': f"{sample_a_result['unique_repeat_buyers_combo'] - sample_b_result['unique_repeat_buyers_combo']:+,}"
            })
        else:
            sales_agent_data.append({
                'æŒ‡æ ‡': 'æ•°æ®è·å–å¤±è´¥',
                sample_a_label: '-',
                sample_b_label: '-',
                'å·®å¼‚': '-'
            })
        
        sales_agent_df = pd.DataFrame(sales_agent_data)
        
        # ç”Ÿæˆæ—¶é—´é—´éš”åˆ†æå¯¹æ¯”è¡¨æ ¼
        time_interval_data = []
        if time_interval_results:
            result = time_interval_results[0]
            sample_a_result = result['sample_a']
            sample_b_result = result['sample_b']
            
            # æ”¯ä»˜åˆ°é€€æ¬¾æ—¶é—´é—´éš”
            refund_a = sample_a_result.get('payment_to_refund', {})
            refund_b = sample_b_result.get('payment_to_refund', {})
            
            if refund_a and refund_b:
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°é€€æ¬¾-æ ·æœ¬æ•°',
                    sample_a_label: f"{refund_a.get('count', 0):,}",
                    sample_b_label: f"{refund_b.get('count', 0):,}",
                    'å·®å¼‚': f"{refund_a.get('count', 0) - refund_b.get('count', 0):+,}"
                })
                
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°é€€æ¬¾-å¹³å‡å¤©æ•°',
                    sample_a_label: f"{refund_a.get('mean', 0):.1f}",
                    sample_b_label: f"{refund_b.get('mean', 0):.1f}",
                    'å·®å¼‚': f"{refund_a.get('mean', 0) - refund_b.get('mean', 0):+.1f}"
                })
                
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°é€€æ¬¾-ä¸­ä½æ•°å¤©æ•°',
                    sample_a_label: f"{refund_a.get('median', 0):.1f}",
                    sample_b_label: f"{refund_b.get('median', 0):.1f}",
                    'å·®å¼‚': f"{refund_a.get('median', 0) - refund_b.get('median', 0):+.1f}"
                })
            
            # æ”¯ä»˜åˆ°åˆ†é…æ—¶é—´é—´éš”
            assign_a = sample_a_result.get('payment_to_assign', {})
            assign_b = sample_b_result.get('payment_to_assign', {})
            
            if assign_a and assign_b:
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°åˆ†é…-æ ·æœ¬æ•°',
                    sample_a_label: f"{assign_a.get('count', 0):,}",
                    sample_b_label: f"{assign_b.get('count', 0):,}",
                    'å·®å¼‚': f"{assign_a.get('count', 0) - assign_b.get('count', 0):+,}"
                })
                
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°åˆ†é…-å¹³å‡å¤©æ•°',
                    sample_a_label: f"{assign_a.get('mean', 0):.1f}",
                    sample_b_label: f"{assign_b.get('mean', 0):.1f}",
                    'å·®å¼‚': f"{assign_a.get('mean', 0) - assign_b.get('mean', 0):+.1f}"
                })
                
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°åˆ†é…-ä¸­ä½æ•°å¤©æ•°',
                    sample_a_label: f"{assign_a.get('median', 0):.1f}",
                    sample_b_label: f"{assign_b.get('median', 0):.1f}",
                    'å·®å¼‚': f"{assign_a.get('median', 0) - assign_b.get('median', 0):+.1f}"
                })
            
            # æ”¯ä»˜åˆ°é”å•æ—¶é—´é—´éš”
            lock_a = sample_a_result.get('payment_to_lock', {})
            lock_b = sample_b_result.get('payment_to_lock', {})
            
            if lock_a and lock_b:
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°é”å•-æ ·æœ¬æ•°',
                    sample_a_label: f"{lock_a.get('count', 0):,}",
                    sample_b_label: f"{lock_b.get('count', 0):,}",
                    'å·®å¼‚': f"{lock_a.get('count', 0) - lock_b.get('count', 0):+,}"
                })
                
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°é”å•-å¹³å‡å¤©æ•°',
                    sample_a_label: f"{lock_a.get('mean', 0):.1f}",
                    sample_b_label: f"{lock_b.get('mean', 0):.1f}",
                    'å·®å¼‚': f"{lock_a.get('mean', 0) - lock_b.get('mean', 0):+.1f}"
                })
                
                time_interval_data.append({
                    'æ—¶é—´é—´éš”ç±»å‹': 'æ”¯ä»˜åˆ°é”å•-ä¸­ä½æ•°å¤©æ•°',
                    sample_a_label: f"{lock_a.get('median', 0):.1f}",
                    sample_b_label: f"{lock_b.get('median', 0):.1f}",
                    'å·®å¼‚': f"{lock_a.get('median', 0) - lock_b.get('median', 0):+.1f}"
                })
        
        if not time_interval_data:
            time_interval_data.append({
                'æ—¶é—´é—´éš”ç±»å‹': 'æ— æœ‰æ•ˆæ•°æ®',
                sample_a_label: '-',
                sample_b_label: '-',
                'å·®å¼‚': '-'
            })
        
        time_interval_df = pd.DataFrame(time_interval_data)
        
        return report, anomaly_df, sales_agent_df, time_interval_df

# åˆ›å»ºåˆ†æå™¨å®ä¾‹
analyzer = ABComparisonAnalyzer()

def run_analysis(start_date_a, end_date_a, refund_start_date_a, refund_end_date_a, 
                       order_create_start_date_a, order_create_end_date_a, lock_start_date_a, lock_end_date_a,
                       pre_vehicle_model_types_a, parent_regions_a, vehicle_types_a, include_invalid_id_a, refund_only_a, locked_only_a,
                       exclude_refund_a, exclude_locked_a, battery_types_a, repeat_buyer_only_a, exclude_repeat_buyer_a,
                       start_date_b, end_date_b, refund_start_date_b, refund_end_date_b,
                       order_create_start_date_b, order_create_end_date_b, lock_start_date_b, lock_end_date_b,
                       pre_vehicle_model_types_b, parent_regions_b, vehicle_types_b, include_invalid_id_b, refund_only_b, locked_only_b,
                       exclude_refund_b, exclude_locked_b, battery_types_b, repeat_buyer_only_b, exclude_repeat_buyer_b):
    """æ‰§è¡ŒABå¯¹æ¯”åˆ†æ"""
    try:
        # ç­›é€‰æ ·æœ¬A
        sample_a = analyzer.filter_sample(
            start_date=start_date_a, end_date=end_date_a,
            refund_start_date=refund_start_date_a, refund_end_date=refund_end_date_a,
            order_create_start_date=order_create_start_date_a, order_create_end_date=order_create_end_date_a,
            lock_start_date=lock_start_date_a, lock_end_date=lock_end_date_a,
            pre_vehicle_model_types=pre_vehicle_model_types_a if pre_vehicle_model_types_a else None,
            parent_regions=parent_regions_a if parent_regions_a else None,
            vehicle_groups=vehicle_types_a if vehicle_types_a else None,
            refund_only=refund_only_a,
            locked_only=locked_only_a,
            exclude_refund=exclude_refund_a,
            exclude_locked=exclude_locked_a,
            include_invalid_id=include_invalid_id_a,
            battery_types=battery_types_a if battery_types_a else None,
            repeat_buyer_only=repeat_buyer_only_a,
            exclude_repeat_buyer=exclude_repeat_buyer_a
        )
        # åŠ¨æ€æ„å»ºæ ·æœ¬Aåç§°ï¼ˆä¸å«æ—¶é—´å‘¨æœŸï¼Œé»˜è®¤æˆ–å…¨éƒ¨ä¸åŠ å…¥ï¼‰
        def add_segment(seg_list, title, vals):
            if vals and not ("å…¨éƒ¨" in vals or "All" in vals):
                seg_list.append(f"{title}:{','.join(vals)}")

        segments_a = []
        add_segment(segments_a, "è½¦å‹", vehicle_types_a)
        add_segment(segments_a, "äº§å“", pre_vehicle_model_types_a)
        add_segment(segments_a, "ç”µæ± ", battery_types_a)
        add_segment(segments_a, "åŒºåŸŸ", parent_regions_a)
        if refund_only_a:
            segments_a.append("ä»…é€€è®¢")
        if locked_only_a:
            segments_a.append("ä»…é”å•")
        sample_a_label = " | ".join(segments_a)
        sample_a_desc = sample_a_label if sample_a_label else "æ ·æœ¬A"
        
        # ç­›é€‰æ ·æœ¬B
        sample_b = analyzer.filter_sample(
            start_date=start_date_b, end_date=end_date_b,
            refund_start_date=refund_start_date_b, refund_end_date=refund_end_date_b,
            order_create_start_date=order_create_start_date_b, order_create_end_date=order_create_end_date_b,
            lock_start_date=lock_start_date_b, lock_end_date=lock_end_date_b,
            pre_vehicle_model_types=pre_vehicle_model_types_b if pre_vehicle_model_types_b else None,
            parent_regions=parent_regions_b if parent_regions_b else None,
            vehicle_groups=vehicle_types_b if vehicle_types_b else None,
            refund_only=refund_only_b,
            locked_only=locked_only_b,
            exclude_refund=exclude_refund_b,
            exclude_locked=exclude_locked_b,
            include_invalid_id=include_invalid_id_b,
            battery_types=battery_types_b if battery_types_b else None,
            repeat_buyer_only=repeat_buyer_only_b,
            exclude_repeat_buyer=exclude_repeat_buyer_b
        )
        # åŠ¨æ€æ„å»ºæ ·æœ¬Båç§°ï¼ˆä¸å«æ—¶é—´å‘¨æœŸï¼Œé»˜è®¤æˆ–å…¨éƒ¨ä¸åŠ å…¥ï¼‰
        segments_b = []
        add_segment(segments_b, "è½¦å‹", vehicle_types_b)
        add_segment(segments_b, "äº§å“", pre_vehicle_model_types_b)
        add_segment(segments_b, "ç”µæ± ", battery_types_b)
        add_segment(segments_b, "åŒºåŸŸ", parent_regions_b)
        if refund_only_b:
            segments_b.append("ä»…é€€è®¢")
        if locked_only_b:
            segments_b.append("ä»…é”å•")
        sample_b_label = " | ".join(segments_b)
        sample_b_desc = sample_b_label if sample_b_label else "æ ·æœ¬B"
        
        if len(sample_a) == 0:
            name_a = sample_a_label or "æ ·æœ¬A"
            empty_df = pd.DataFrame({'é”™è¯¯': [f"{name_a} æ•°æ®ä¸ºç©ºï¼Œè¯·è°ƒæ•´ç­›é€‰æ¡ä»¶"]})
            return f"âŒ {name_a} æ•°æ®ä¸ºç©ºï¼Œè¯·è°ƒæ•´ç­›é€‰æ¡ä»¶", empty_df, empty_df, empty_df
        
        if len(sample_b) == 0:
            name_b = sample_b_label or "æ ·æœ¬B"
            empty_df = pd.DataFrame({'é”™è¯¯': [f"{name_b} æ•°æ®ä¸ºç©ºï¼Œè¯·è°ƒæ•´ç­›é€‰æ¡ä»¶"]})
            return f"âŒ {name_b} æ•°æ®ä¸ºç©ºï¼Œè¯·è°ƒæ•´ç­›é€‰æ¡ä»¶", empty_df, empty_df, empty_df
        
        # è·å–Parent Regionç­›é€‰æ¡ä»¶ï¼ˆå–ä¸¤ä¸ªæ ·æœ¬çš„äº¤é›†ï¼‰
        parent_regions_filter = None
        if parent_regions_a and parent_regions_b:
            # å¦‚æœä¸¤ä¸ªæ ·æœ¬éƒ½æœ‰Parent Regionç­›é€‰ï¼Œå–äº¤é›†
            parent_regions_filter = list(set(parent_regions_a) & set(parent_regions_b))
        elif parent_regions_a:
            parent_regions_filter = parent_regions_a
        elif parent_regions_b:
            parent_regions_filter = parent_regions_b
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        report, anomaly_df, sales_agent_df, time_interval_df = analyzer.generate_comparison_report(
            sample_a, sample_b, sample_a_desc, sample_b_desc, parent_regions_filter,
            sample_a_label=sample_a_label, sample_b_label=sample_b_label
        )
        
        return report, anomaly_df, sales_agent_df, time_interval_df
        
    except Exception as e:
        logger.error(f"ABå¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}")
        error_df = pd.DataFrame({'é”™è¯¯': [f"åˆ†æå¤±è´¥: {str(e)}"]})
        return f"âŒ åˆ†æå¤±è´¥: {str(e)}", error_df, error_df, error_df

# è·å–æ•°æ®ä¿¡æ¯
vehicle_types = analyzer.get_vehicle_types()
pre_vehicle_model_types = analyzer.get_pre_vehicle_model_types()
parent_regions = analyzer.get_parent_regions()
min_date, max_date = analyzer.get_date_range()
refund_min_date, refund_max_date = analyzer.get_refund_date_range()
order_create_min_date, order_create_max_date = analyzer.get_order_create_date_range()
lock_min_date, lock_max_date = analyzer.get_lock_date_range()

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="ABå¯¹æ¯”åˆ†æå·¥å…·", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ” ABå¯¹æ¯”åˆ†æå·¥å…·")
    gr.Markdown("ç”¨äºå¯¹æ¯”ä¸¤ä¸ªæ ·æœ¬ä¹‹é—´çš„å·®å¼‚ï¼Œå‘ç°åœ°åŒºåˆ†å¸ƒã€æ¸ é“ç»“æ„å’Œäººç¾¤ç»“æ„å¼‚å¸¸")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("## ğŸ“Š æ ·æœ¬Aé…ç½®")
            
            with gr.Group():
                gr.Markdown("### å°è®¢æ—¶é—´èŒƒå›´")
                with gr.Row():
                    start_date_a = gr.Textbox(label="å¼€å§‹æ—¥æœŸ", value=min_date, placeholder="YYYY-MM-DD")
                    end_date_a = gr.Textbox(label="ç»“æŸæ—¥æœŸ", value=max_date, placeholder="YYYY-MM-DD")
            
            with gr.Group():
                gr.Markdown("### é€€è®¢æ—¶é—´èŒƒå›´")
                with gr.Row():
                    refund_start_date_a = gr.Textbox(label="é€€è®¢å¼€å§‹æ—¥æœŸ", value="", placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
                    refund_end_date_a = gr.Textbox(label="é€€è®¢ç»“æŸæ—¥æœŸ", value="", placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
            
            with gr.Group():
                gr.Markdown("### è®¢å•åˆ›å»ºæ—¶é—´èŒƒå›´")
                with gr.Row():
                    order_create_start_date_a = gr.Textbox(label="åˆ›å»ºå¼€å§‹æ—¥æœŸ", value="", placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
                    order_create_end_date_a = gr.Textbox(label="åˆ›å»ºç»“æŸæ—¥æœŸ", value="", placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
            
            with gr.Group():
                gr.Markdown("### é”å•æ—¶é—´èŒƒå›´")
                with gr.Row():
                    lock_start_date_a = gr.Textbox(label="é”å•å¼€å§‹æ—¥æœŸ", value=lock_min_date, placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
                    lock_end_date_a = gr.Textbox(label="é”å•ç»“æŸæ—¥æœŸ", value=lock_max_date, placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
            
            pre_vehicle_model_types_a = gr.CheckboxGroup(choices=pre_vehicle_model_types, label="äº§å“åˆ†ç±»ï¼ˆå¢ç¨‹/çº¯ç”µï¼‰", value=[])
            
            # æ·»åŠ ç”µæ± ç±»å‹é€‰æ‹©ç»„ä»¶
            battery_types = analyzer.get_battery_types()
            battery_types_a = gr.CheckboxGroup(choices=battery_types, label="ç”µæ± ç±»å‹åˆ†ç±»", value=[])
            parent_regions_a = gr.Dropdown(choices=parent_regions, label="Parent Region Name", multiselect=True, value=None)
            vehicle_types_a = gr.CheckboxGroup(choices=vehicle_types, label="è½¦å‹é€‰æ‹©", value=[])
            include_invalid_id_a = gr.Checkbox(label="åŒ…å«å¼‚å¸¸èº«ä»½è¯å·", value=True)
            repeat_buyer_only_a = gr.Checkbox(label="ä»…å¤è´­ç”¨æˆ·", value=False)
            exclude_repeat_buyer_a = gr.Checkbox(label="æ’é™¤å¤è´­ç”¨æˆ·", value=False)
            refund_only_a = gr.Checkbox(label="ä»…é€€è®¢æ•°æ®", value=False)
            locked_only_a = gr.Checkbox(label="ä»…é”å•æ•°æ®", value=False)
            exclude_refund_a = gr.Checkbox(label="æ’é™¤é€€è®¢æ•°æ®", value=False)
            exclude_locked_a = gr.Checkbox(label="æ’é™¤é”å•æ•°æ®", value=False)
        
        with gr.Column(scale=1):
            gr.Markdown("## ğŸ“Š æ ·æœ¬Bé…ç½®")
            
            with gr.Group():
                gr.Markdown("### å°è®¢æ—¶é—´èŒƒå›´")
                with gr.Row():
                    start_date_b = gr.Textbox(label="å¼€å§‹æ—¥æœŸ", value=min_date, placeholder="YYYY-MM-DD")
                    end_date_b = gr.Textbox(label="ç»“æŸæ—¥æœŸ", value=max_date, placeholder="YYYY-MM-DD")
            
            with gr.Group():
                gr.Markdown("### é€€è®¢æ—¶é—´èŒƒå›´")
                with gr.Row():
                    refund_start_date_b = gr.Textbox(label="é€€è®¢å¼€å§‹æ—¥æœŸ", value="", placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
                    refund_end_date_b = gr.Textbox(label="é€€è®¢ç»“æŸæ—¥æœŸ", value="", placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
            
            with gr.Group():
                gr.Markdown("### è®¢å•åˆ›å»ºæ—¶é—´èŒƒå›´")
                with gr.Row():
                    order_create_start_date_b = gr.Textbox(label="åˆ›å»ºå¼€å§‹æ—¥æœŸ", value="", placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
                    order_create_end_date_b = gr.Textbox(label="åˆ›å»ºç»“æŸæ—¥æœŸ", value="", placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
            
            with gr.Group():
                gr.Markdown("### é”å•æ—¶é—´èŒƒå›´")
                with gr.Row():
                    lock_start_date_b = gr.Textbox(label="é”å•å¼€å§‹æ—¥æœŸ", value=lock_min_date, placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
                    lock_end_date_b = gr.Textbox(label="é”å•ç»“æŸæ—¥æœŸ", value=lock_max_date, placeholder="YYYY-MM-DDï¼ˆå¯é€‰ï¼‰")
            
            pre_vehicle_model_types_b = gr.CheckboxGroup(choices=pre_vehicle_model_types, label="äº§å“åˆ†ç±»ï¼ˆå¢ç¨‹/çº¯ç”µï¼‰", value=[])
            
            # æ·»åŠ ç”µæ± ç±»å‹é€‰æ‹©ç»„ä»¶
            battery_types_b = gr.CheckboxGroup(choices=battery_types, label="ç”µæ± ç±»å‹åˆ†ç±»", value=[])
            parent_regions_b = gr.Dropdown(choices=parent_regions, label="Parent Region Name", multiselect=True, value=None)
            vehicle_types_b = gr.CheckboxGroup(choices=vehicle_types, label="è½¦å‹é€‰æ‹©", value=[])
            include_invalid_id_b = gr.Checkbox(label="åŒ…å«å¼‚å¸¸èº«ä»½è¯å·", value=True)
            repeat_buyer_only_b = gr.Checkbox(label="ä»…å¤è´­ç”¨æˆ·", value=False)
            exclude_repeat_buyer_b = gr.Checkbox(label="æ’é™¤å¤è´­ç”¨æˆ·", value=False)
            refund_only_b = gr.Checkbox(label="ä»…é€€è®¢æ•°æ®", value=False)
            locked_only_b = gr.Checkbox(label="ä»…é”å•æ•°æ®", value=False)
            exclude_refund_b = gr.Checkbox(label="æ’é™¤é€€è®¢æ•°æ®", value=False)
            exclude_locked_b = gr.Checkbox(label="æ’é™¤é”å•æ•°æ®", value=False)
    
    with gr.Row():
        analyze_btn = gr.Button("ğŸš€ å¼€å§‹åˆ†æ", variant="primary", size="lg")
    
    with gr.Row():
        with gr.Column(scale=1):
            output = gr.Markdown(label="åˆ†æç»“æœ")
        with gr.Column(scale=4):
            anomaly_table = gr.DataFrame(
                label="å¼‚å¸¸æ•°æ®è¯¦æƒ…",
                interactive=False,
                wrap=True,
                datatype=["str", "str", "str", "str", "str", "str", "str", "html", "html", "str"]
            )
    
    with gr.Row():
        with gr.Column(scale=1):
            sales_agent_table = gr.DataFrame(
                label="é”€å”®ä»£ç†åˆ†æå¯¹æ¯”",
                interactive=False,
                wrap=True,
                datatype=["str", "str", "str", "html"]
            )
        with gr.Column(scale=1):
            time_interval_table = gr.DataFrame(
                label="æ—¶é—´é—´éš”åˆ†æå¯¹æ¯”",
                interactive=False,
                wrap=True
            )
    
    # ç»‘å®šåˆ†æå‡½æ•°
    analyze_btn.click(
        fn=run_analysis,
        inputs=[
            start_date_a, end_date_a, refund_start_date_a, refund_end_date_a,
            order_create_start_date_a, order_create_end_date_a, lock_start_date_a, lock_end_date_a,
            pre_vehicle_model_types_a, parent_regions_a, vehicle_types_a, include_invalid_id_a,
            refund_only_a, locked_only_a, exclude_refund_a, exclude_locked_a, battery_types_a, repeat_buyer_only_a, exclude_repeat_buyer_a,
            start_date_b, end_date_b, refund_start_date_b, refund_end_date_b,
            order_create_start_date_b, order_create_end_date_b, lock_start_date_b, lock_end_date_b,
            pre_vehicle_model_types_b, parent_regions_b, vehicle_types_b, include_invalid_id_b,
            refund_only_b, locked_only_b, exclude_refund_b, exclude_locked_b, battery_types_b, repeat_buyer_only_b, exclude_repeat_buyer_b
        ],
        outputs=[output, anomaly_table, sales_agent_table, time_interval_table]
    )
    
    # æ·»åŠ ä½¿ç”¨è¯´æ˜
    with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
        gr.Markdown("""
        ### åŠŸèƒ½è¯´æ˜
        1. **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹**: å¯¹æ¯”ä¸¤ä¸ªæ ·æœ¬åœ¨ä¸åŒåœ°åŒºçš„è®¢å•åˆ†å¸ƒå·®å¼‚
        2. **æ¸ é“ç»“æ„å¼‚å¸¸æ£€æµ‹**: åˆ†ææ¸ é“é”€é‡å æ¯”çš„å˜åŒ–æƒ…å†µ
        3. **äººç¾¤ç»“æ„å¼‚å¸¸æ£€æµ‹**: æ£€æµ‹æ€§åˆ«æ¯”ä¾‹å’Œå¹´é¾„æ®µç»“æ„çš„å·®å¼‚
        
        ### ä½¿ç”¨æ­¥éª¤
        1. é…ç½®æ ·æœ¬Açš„ç­›é€‰æ¡ä»¶ï¼ˆæ—¶é—´èŒƒå›´ã€è½¦å‹ã€æ˜¯å¦åŒ…å«é€€è®¢ï¼‰
        2. é…ç½®æ ·æœ¬Bçš„ç­›é€‰æ¡ä»¶
        3. ç‚¹å‡»"å¼€å§‹åˆ†æ"æŒ‰é’®
        4. æŸ¥çœ‹åˆ†æç»“æœå’Œå»ºè®®æªæ–½
        
        ### å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ï¼ˆ"æˆ–"é€»è¾‘ï¼‰
        **åœ°åŒºåˆ†å¸ƒå¼‚å¸¸**ï¼š
        - ç»“æ„å æ¯”å¼‚å¸¸ï¼šå æ¯”>1%ä¸”å˜åŒ–å¹…åº¦>10% OR
        - ç¯æ¯”å˜åŒ–å¼‚å¸¸ï¼šç»å¯¹å€¼ç¯æ¯”å˜åŒ–>50%ï¼ˆåŒ…æ‹¬100%å¢é•¿ï¼‰

        **æ¸ é“ç»“æ„å¼‚å¸¸**ï¼š
        - ç»“æ„å æ¯”å¼‚å¸¸ï¼šå æ¯”>1%ä¸”å˜åŒ–å¹…åº¦>15% OR
        - ç¯æ¯”å˜åŒ–å¼‚å¸¸ï¼šç»å¯¹å€¼ç¯æ¯”å˜åŒ–>50%ï¼ˆåŒ…æ‹¬100%å¢é•¿ï¼‰

        **äººç¾¤ç»“æ„å¼‚å¸¸**ï¼š
        - æ€§åˆ«åˆ†å¸ƒï¼šå˜åŒ–å¹…åº¦>10% OR ç»å¯¹å€¼ç¯æ¯”å˜åŒ–>50%
        - å¹´é¾„åˆ†å¸ƒï¼šå æ¯”>1%ä¸”å˜åŒ–å¹…åº¦>15% OR ç»å¯¹å€¼ç¯æ¯”å˜åŒ–>50%
        """)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7866, share=False)