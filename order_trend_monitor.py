#!/usr/bin/env python3
"""
è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹è„šæœ¬
ç”¨äºå¤„ç†è®¢å•è§‚å¯Ÿæ•°æ®å¹¶è¿›è¡ŒåŸºæœ¬æè¿°æ€§åˆ†æ
åŒ…å«æ•°æ®åŠ è½½ã€å¤„ç†å’Œè®¡ç®—é€»è¾‘
"""

import pandas as pd
import pyarrow.parquet as pq
from typing import Dict, Any, List
import logging
from pathlib import Path
import datetime
import os
import numpy as np
import duckdb

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…¨å±€ DuckDB è¿æ¥
_db_connection = None

def get_db_connection():
    """è·å– DuckDB æ•°æ®åº“è¿æ¥ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _db_connection
    if _db_connection is None:
        _db_connection = duckdb.connect(':memory:')
        logger.info("åˆ›å»ºæ–°çš„ DuckDB å†…å­˜æ•°æ®åº“è¿æ¥")
    return _db_connection

def initialize_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
    conn = get_db_connection()
    
    # åˆ›å»ºåŸå§‹æ•°æ®è¡¨
    conn.execute("""
        CREATE TABLE IF NOT EXISTS raw_order_data (
            "æ—¥(Intention Payment Time)" TIMESTAMP,
            "æ—¥(Order Create Time)" TIMESTAMP,
            "æ—¥(Lock Time)" TIMESTAMP,
            "æ—¥(intention_refund_time)" TIMESTAMP,
            "æ—¥(Actual Refund Time)" TIMESTAMP,
            "DATE([Invoice Upload Time])" TIMESTAMP,
            "Parent Region Name" VARCHAR,
            "Province Name" VARCHAR,
            "first_middle_channel_name" VARCHAR,
            "å¹³å‡å€¼ å¼€ç¥¨ä»·æ ¼" DOUBLE
        )
    """)
    
    # åˆ›å»ºèšåˆæ•°æ®è¡¨
    conn.execute("""
        CREATE TABLE IF NOT EXISTS processed_order_data (
            date DATE,
            region VARCHAR,
            province VARCHAR,
            channel VARCHAR,
            è®¢å•æ•° INTEGER,
            å°è®¢æ•° INTEGER,
            é”å•æ•° INTEGER,
            å¼€ç¥¨ä»·æ ¼ DOUBLE,
            é€€è®¢æ•° INTEGER
        )
    """)
    
    logger.info("æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")

def parse_chinese_date(date_str):
    """è§£æä¸­æ–‡æ—¥æœŸæ ¼å¼ï¼Œå¦‚'2025å¹´8æœˆ25æ—¥'"""
    if pd.isna(date_str) or date_str == 'nan':
        return pd.NaT
    try:
        # å¤„ç†ä¸­æ–‡æ—¥æœŸæ ¼å¼
        if 'å¹´' in str(date_str) and 'æœˆ' in str(date_str) and 'æ—¥' in str(date_str):
            date_str = str(date_str).replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')
            return pd.to_datetime(date_str)
        else:
            return pd.to_datetime(date_str, errors='coerce')
    except:
        return pd.NaT

def load_real_order_data():
    """ä½¿ç”¨ DuckDB æ‡’åŠ è½½è®¢å•è§‚å¯Ÿæ•°æ®"""
    conn = get_db_connection()
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½è¿‡æ•°æ®
    result = conn.execute("SELECT COUNT(*) FROM processed_order_data").fetchone()
    if result[0] > 0:
        logger.info("æ•°æ®å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›")
        return conn.execute("SELECT * FROM processed_order_data").df()
    
    logger.info("å¼€å§‹åŠ è½½åŸå§‹æ•°æ®åˆ° DuckDB...")
    
    # ä½¿ç”¨ DuckDB ç›´æ¥è¯»å– parquet æ–‡ä»¶
    parquet_path = '/Users/zihao_/Documents/coding/dataset/formatted/order_observation_data_merged.parquet'
    
    # æ¸…ç©ºåŸå§‹æ•°æ®è¡¨
    conn.execute("DELETE FROM raw_order_data")
    
    # ç›´æ¥ä» parquet æ–‡ä»¶æ’å…¥æ•°æ®åˆ° DuckDB
    conn.execute(f"""
        INSERT INTO raw_order_data 
        SELECT 
            "æ—¥(Intention Payment Time)",
            "æ—¥(Order Create Time)",
            "æ—¥(Lock Time)",
            "æ—¥(intention_refund_time)",
            "æ—¥(Actual Refund Time)",
            "DATE([Invoice Upload Time])",
            "Parent Region Name",
            "Province Name",
            "first_middle_channel_name",
            "å¹³å‡å€¼ å¼€ç¥¨ä»·æ ¼"
        FROM read_parquet('{parquet_path}')
        WHERE "Parent Region Name" IS NOT NULL 
        AND "Province Name" IS NOT NULL 
        AND "first_middle_channel_name" IS NOT NULL
    """)
    
    logger.info("åŸå§‹æ•°æ®åŠ è½½å®Œæˆï¼Œå¼€å§‹èšåˆå¤„ç†...")
    
    # ä½¿ç”¨ SQL è¿›è¡Œæ•°æ®èšåˆå¤„ç†
    conn.execute("""
        INSERT INTO processed_order_data
        WITH date_series AS (
            SELECT DISTINCT date_trunc('day', coalesce(
                "æ—¥(Order Create Time)",
                "æ—¥(Intention Payment Time)", 
                "æ—¥(Lock Time)",
                "DATE([Invoice Upload Time])",
                "æ—¥(Actual Refund Time)",
                "æ—¥(intention_refund_time)"
            )) as date
            FROM raw_order_data
            WHERE coalesce(
                "æ—¥(Order Create Time)",
                "æ—¥(Intention Payment Time)", 
                "æ—¥(Lock Time)",
                "DATE([Invoice Upload Time])",
                "æ—¥(Actual Refund Time)",
                "æ—¥(intention_refund_time)"
            ) IS NOT NULL
        ),
        combinations AS (
            SELECT DISTINCT 
                "Parent Region Name" as region,
                "Province Name" as province,
                "first_middle_channel_name" as channel
            FROM raw_order_data
        )
        SELECT 
            ds.date::DATE as date,
            c.region,
            c.province,
            c.channel,
            COUNT(CASE WHEN date_trunc('day', "æ—¥(Order Create Time)") = ds.date THEN 1 END) as è®¢å•æ•°,
            COUNT(CASE WHEN date_trunc('day', "æ—¥(Intention Payment Time)") = ds.date THEN 1 END) as å°è®¢æ•°,
            COUNT(CASE WHEN date_trunc('day', "æ—¥(Lock Time)") = ds.date THEN 1 END) as é”å•æ•°,
            AVG(CASE WHEN date_trunc('day', "DATE([Invoice Upload Time])") = ds.date AND "å¹³å‡å€¼ å¼€ç¥¨ä»·æ ¼" IS NOT NULL 
                THEN "å¹³å‡å€¼ å¼€ç¥¨ä»·æ ¼" END) as å¼€ç¥¨ä»·æ ¼,
            COUNT(CASE WHEN (date_trunc('day', "æ—¥(Actual Refund Time)") = ds.date OR 
                           date_trunc('day', "æ—¥(intention_refund_time)") = ds.date) THEN 1 END) as é€€è®¢æ•°
        FROM date_series ds
        CROSS JOIN combinations c
        LEFT JOIN raw_order_data r ON (
            c.region = r."Parent Region Name" AND 
            c.province = r."Province Name" AND 
            c.channel = r."first_middle_channel_name" AND
            ds.date IN (
                date_trunc('day', r."æ—¥(Order Create Time)"),
                date_trunc('day', r."æ—¥(Intention Payment Time)"),
                date_trunc('day', r."æ—¥(Lock Time)"),
                date_trunc('day', r."DATE([Invoice Upload Time])"),
                date_trunc('day', r."æ—¥(Actual Refund Time)"),
                date_trunc('day', r."æ—¥(intention_refund_time)")
            )
        )
        GROUP BY ds.date, c.region, c.province, c.channel
        HAVING (è®¢å•æ•° + å°è®¢æ•° + é”å•æ•° + é€€è®¢æ•°) > 0
        ORDER BY ds.date, c.region, c.province, c.channel
    """)
    
    logger.info("æ•°æ®èšåˆå¤„ç†å®Œæˆ")
    
    # è·å–å¤„ç†åçš„æ•°æ®
    processed_data = conn.execute("SELECT * FROM processed_order_data").df()
    
    # ä¿å­˜èšåˆç»“æœåˆ°å·¥ä½œåŒºæ–‡ä»¶
    output_file = "/Users/zihao_/Documents/github/W35_workflow/processed_order_data.parquet"
    processed_data.to_parquet(output_file, index=False)
    logger.info(f"èšåˆç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # è¿”å›å¤„ç†åçš„æ•°æ®
    return processed_data

def load_order_data(data_path: str) -> pd.DataFrame:
    """
    åŠ è½½è®¢å•æ•°æ®
    """
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {data_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        # åŠ è½½parquetæ–‡ä»¶
        data = pd.read_parquet(data_path)
        logger.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
        
        return data
        
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        raise

def filter_and_aggregate_data(df, region, province, channel, metric, start_date=None, end_date=None):
    """ä½¿ç”¨ DuckDB ç­›é€‰å’Œèšåˆæ•°æ®"""
    conn = get_db_connection()
    
    # æ˜ å°„æ–°çš„å­—æ®µåç§°
    metric_mapping = {
        "order_volume": "è®¢å•æ•°",
        "small_order_volume": "å°è®¢æ•°", 
        "lock_volume": "é”å•æ•°",
        "avg_price": "å¼€ç¥¨ä»·æ ¼",
        "refund_volume": "é€€è®¢æ•°"
    }
    
    actual_metric = metric_mapping.get(metric, metric)
    
    # æ„å»º WHERE æ¡ä»¶
    where_conditions = []
    if region != "å…¨éƒ¨":
        where_conditions.append(f"region = '{region}'")
    if province != "å…¨éƒ¨":
        where_conditions.append(f"province = '{province}'")
    if channel != "å…¨éƒ¨":
        where_conditions.append(f"channel = '{channel}'")
    if start_date:
        where_conditions.append(f"date >= '{start_date}'")
    if end_date:
        where_conditions.append(f"date <= '{end_date}'")
    
    where_clause = "WHERE " + " AND ".join(where_conditions) if where_conditions else ""
    
    # ä½¿ç”¨ SQL è¿›è¡ŒèšåˆæŸ¥è¯¢
    query = f"""
        WITH aggregated AS (
            SELECT 
                date,
                SUM("{actual_metric}") as "{actual_metric}"
            FROM processed_order_data
            {where_clause}
            GROUP BY date
            ORDER BY date
        ),
        with_change_rate AS (
            SELECT 
                date,
                "{actual_metric}",
                (("{actual_metric}" - LAG("{actual_metric}") OVER (ORDER BY date)) / 
                 NULLIF(LAG("{actual_metric}") OVER (ORDER BY date), 0)) * 100 as change_rate
            FROM aggregated
        )
        SELECT * FROM with_change_rate
    """
    
    grouped = conn.execute(query).df()
    
    return grouped, actual_metric

def detect_data_anomaly(df, region, province, channel, metric, threshold=1.5, start_date=None, end_date=None):
    """ä½¿ç”¨ DuckDB æ£€æµ‹æ•°æ®å¼‚åŠ¨"""
    conn = get_db_connection()
    
    # æ˜ å°„æ–°çš„å­—æ®µåç§°
    metric_mapping = {
        "order_volume": "è®¢å•æ•°",
        "small_order_volume": "å°è®¢æ•°", 
        "lock_volume": "é”å•æ•°",
        "avg_price": "å¼€ç¥¨ä»·æ ¼",
        "refund_volume": "é€€è®¢æ•°"
    }
    
    actual_metric = metric_mapping.get(metric, metric)
    
    # æ„å»º WHERE æ¡ä»¶
    where_conditions = []
    if region != "å…¨éƒ¨":
        where_conditions.append(f"region = '{region}'")
    if province != "å…¨éƒ¨":
        where_conditions.append(f"province = '{province}'")
    if channel != "å…¨éƒ¨":
        where_conditions.append(f"channel = '{channel}'")
    if start_date:
        where_conditions.append(f"date >= '{start_date}'")
    if end_date:
        where_conditions.append(f"date <= '{end_date}'")
    
    where_clause = "WHERE " + " AND ".join(where_conditions) if where_conditions else ""
    
    # ä½¿ç”¨ SQL æ£€æµ‹å¼‚å¸¸å€¼
    query = f"""
        WITH aggregated AS (
            SELECT 
                date,
                SUM("{actual_metric}") as "{actual_metric}"
            FROM processed_order_data
            {where_clause}
            GROUP BY date
        ),
        stats AS (
            SELECT 
                AVG("{actual_metric}") as mean_val,
                STDDEV("{actual_metric}") as std_val
            FROM aggregated
        ),
        anomalies AS (
            SELECT 
                a.date,
                a."{actual_metric}",
                s.mean_val,
                s.std_val,
                (a."{actual_metric}" - s.mean_val) / s.std_val as z_score
            FROM aggregated a
            CROSS JOIN stats s
            WHERE a."{actual_metric}" > s.mean_val + {threshold} * s.std_val
            ORDER BY a.date
        )
        SELECT * FROM anomalies
    """
    
    anomalies = conn.execute(query).df()
    
    if anomalies.empty:
        return "æš‚æ— æ˜¾è‘—å¼‚å¸¸"
    
    # æ ¼å¼åŒ–è¾“å‡º
    result_lines = []
    for _, row in anomalies.iterrows():
        result_lines.append(f"æ—¥æœŸ: {row['date']}, {actual_metric}: {row[actual_metric]:.2f}, Z-Score: {row['z_score']:.2f}")
    
    return "\n".join(result_lines)

def analyze_data_structure(data: pd.DataFrame) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®ç»“æ„å’ŒåŸºæœ¬ä¿¡æ¯
    """
    logger.info("å¼€å§‹åˆ†ææ•°æ®ç»“æ„...")
    
    # åŸºæœ¬ä¿¡æ¯
    basic_info = {
        'shape': data.shape,
        'total_rows': len(data),
        'total_columns': len(data.columns),
        'memory_usage_mb': data.memory_usage(deep=True).sum() / 1024 / 1024
    }
    
    # åˆ—ä¿¡æ¯
    column_info = []
    for col in data.columns:
        col_info = {
            'column_name': col,
            'data_type': str(data[col].dtype),
            'non_null_count': data[col].count(),
            'null_count': data[col].isnull().sum(),
            'null_percentage': (data[col].isnull().sum() / len(data)) * 100,
            'unique_count': data[col].nunique()
        }
        
        # å¦‚æœæ˜¯æ•°å€¼ç±»å‹ï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if pd.api.types.is_numeric_dtype(data[col]):
            col_info.update({
                'min_value': data[col].min(),
                'max_value': data[col].max(),
                'mean_value': data[col].mean(),
                'std_value': data[col].std()
            })
        
        column_info.append(col_info)
    
    # æ•°æ®ç±»å‹åˆ†ç±»
    numeric_columns = data.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()
    categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()
    datetime_columns = data.select_dtypes(include=['datetime64']).columns.tolist()
    
    # ç¼ºå¤±å€¼ç»Ÿè®¡
    missing_values = data.isnull().sum()
    missing_cells = missing_values.sum()
    
    # é‡å¤è¡Œæ£€æŸ¥
    duplicate_rows = data.duplicated().sum()
    
    # æ•°æ®å®Œæ•´æ€§
    total_cells = data.shape[0] * data.shape[1]
    data_completeness = ((total_cells - missing_cells) / total_cells) * 100 if total_cells > 0 else 0
    
    analysis_result = {
        'basic_info': basic_info,
        'column_info': column_info,
        'numeric_columns': numeric_columns,
        'categorical_columns': categorical_columns,
        'datetime_columns': datetime_columns,
        'missing_values': missing_values,
        'missing_cells': missing_cells,
        'duplicate_rows': duplicate_rows,
        'data_completeness': data_completeness
    }
    
    logger.info("æ•°æ®ç»“æ„åˆ†æå®Œæˆ")
    return analysis_result

def generate_order_trend_report(data: pd.DataFrame, analysis: Dict[str, Any], data_path: str) -> None:
    """
    ç”Ÿæˆè®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹æŠ¥å‘Š
    """
    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.datetime.now().isoformat()
    
    # æ„å»ºæŠ¥å‘Šå†…å®¹
    report_content = f"""# è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹æŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {current_time}
- **æ•°æ®æ–‡ä»¶**: {data_path}
- **åˆ†æç±»å‹**: è®¢å•è§‚å¯Ÿæ•°æ®åŸºæœ¬æè¿°æ€§åˆ†æ

---

# æ•°æ®åŸºæœ¬ä¿¡æ¯

## æ•°æ®æ¦‚è§ˆ
- **æ•°æ®å½¢çŠ¶**: {analysis['basic_info']['shape'][0]:,} è¡Œ Ã— {analysis['basic_info']['shape'][1]} åˆ—
- **å†…å­˜ä½¿ç”¨**: {analysis['basic_info']['memory_usage_mb']:.2f} MB
- **æ•°æ®å®Œæ•´æ€§**: {analysis['data_completeness']:.2f}%
- **é‡å¤è¡Œæ•°**: {analysis['duplicate_rows']:,}

## æ•°æ®ç±»å‹åˆ†å¸ƒ
- **æ•°å€¼åˆ—**: {len(analysis['numeric_columns'])} ä¸ª
- **åˆ†ç±»åˆ—**: {len(analysis['categorical_columns'])} ä¸ª
- **æ—¥æœŸåˆ—**: {len(analysis['datetime_columns'])} ä¸ª

## å­—æ®µä¿¡æ¯è¯¦æƒ…

### å…¨éƒ¨å­—æ®µåˆ—è¡¨

| åºå· | å­—æ®µåç§° | æ•°æ®ç±»å‹ | éç©ºæ•°é‡ | ç¼ºå¤±ç‡ | å”¯ä¸€å€¼æ•°é‡ |
|------|----------|----------|----------|--------|------------|
"""
    
    # æ·»åŠ å­—æ®µä¿¡æ¯è¡¨æ ¼
    for i, col_info in enumerate(analysis['column_info'], 1):
        report_content += f"| {i} | {col_info['column_name']} | {col_info['data_type']} | {col_info['non_null_count']:,} | {col_info['null_percentage']:.2f}% | {col_info['unique_count']:,} |\n"
    
    # æ•°å€¼åˆ—ç»Ÿè®¡ä¿¡æ¯
    if analysis['numeric_columns']:
        report_content += "\n### æ•°å€¼åˆ—ç»Ÿè®¡ä¿¡æ¯\n\n"
        report_content += "| å­—æ®µåç§° | æœ€å°å€¼ | æœ€å¤§å€¼ | å¹³å‡å€¼ | æ ‡å‡†å·® |\n"
        report_content += "|----------|--------|--------|--------|--------|\n"
        
        for col_info in analysis['column_info']:
            if col_info['column_name'] in analysis['numeric_columns']:
                min_val = col_info.get('min_value', 'N/A')
                max_val = col_info.get('max_value', 'N/A')
                mean_val = col_info.get('mean_value', 'N/A')
                std_val = col_info.get('std_value', 'N/A')
                
                # æ ¼å¼åŒ–æ•°å€¼æ˜¾ç¤º
                if isinstance(min_val, (int, float)):
                    min_val = f"{min_val:,.2f}" if isinstance(min_val, float) else f"{min_val:,}"
                if isinstance(max_val, (int, float)):
                    max_val = f"{max_val:,.2f}" if isinstance(max_val, float) else f"{max_val:,}"
                if isinstance(mean_val, (int, float)):
                    mean_val = f"{mean_val:,.2f}"
                if isinstance(std_val, (int, float)):
                    std_val = f"{std_val:,.2f}"
                
                report_content += f"| {col_info['column_name']} | {min_val} | {max_val} | {mean_val} | {std_val} |\n"
    else:
        report_content += "\n### æ•°å€¼åˆ—ç»Ÿè®¡ä¿¡æ¯\n\næ— æ•°å€¼åˆ—\n"
    
    # åˆ†ç±»åˆ—ä¿¡æ¯
    if analysis['categorical_columns']:
        report_content += "\n### åˆ†ç±»åˆ—è¯¦æƒ…\n\n"
        report_content += ', '.join(analysis['categorical_columns'])
        report_content += "\n"
    
    # æ—¥æœŸåˆ—ä¿¡æ¯
    if analysis['datetime_columns']:
        report_content += "\n### æ—¥æœŸåˆ—è¯¦æƒ…\n\n"
        report_content += ', '.join(analysis['datetime_columns'])
        report_content += "\n"
    
    # æ•°æ®è´¨é‡åˆ†æ
    report_content += "\n## æ•°æ®è´¨é‡åˆ†æ\n\n"
    
    # ç¼ºå¤±å€¼åˆ†æ
    if analysis['missing_cells'] > 0:
        report_content += "### ç¼ºå¤±å€¼åˆ†æ\n\n"
        report_content += "| å­—æ®µåç§° | ç¼ºå¤±æ•°é‡ | ç¼ºå¤±æ¯”ä¾‹ |\n"
        report_content += "|----------|----------|----------|\n"
        
        for col, missing_count in analysis['missing_values'].items():
            if missing_count > 0:
                missing_pct = (missing_count / len(data)) * 100
                report_content += f"| {col} | {missing_count:,} | {missing_pct:.2f}% |\n"
    else:
        report_content += "### ç¼ºå¤±å€¼æ£€æŸ¥\n\nâœ… æœªå‘ç°ç¼ºå¤±å€¼\n"
    
    # é‡å¤æ•°æ®æ£€æŸ¥
    if analysis['duplicate_rows'] > 0:
        report_content += f"\n### é‡å¤æ•°æ®æ£€æŸ¥\n\nâš ï¸ å‘ç° {analysis['duplicate_rows']:,} è¡Œé‡å¤æ•°æ®\n"
    else:
        report_content += "\n### é‡å¤æ•°æ®æ£€æŸ¥\n\nâœ… æœªå‘ç°é‡å¤æ•°æ®\n"
    
    # æ•°æ®è´¨é‡è¯„çº§
    data_completeness = analysis['data_completeness']
    if data_completeness >= 95:
        quality_grade = "ä¼˜ç§€ âœ…"
    elif data_completeness >= 90:
        quality_grade = "è‰¯å¥½ âš ï¸"
    elif data_completeness >= 80:
        quality_grade = "ä¸€èˆ¬ âš ï¸"
    else:
        quality_grade = "è¾ƒå·® âŒ"
    
    report_content += f"\n## æ•°æ®è´¨é‡è¯„ä¼°\n\n- **æ•´ä½“è¯„çº§**: {quality_grade}\n- **å®Œæ•´æ€§å¾—åˆ†**: {data_completeness:.2f}%\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/order_trend_analysis_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“Š è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # åŒæ—¶æ‰“å°ç®€è¦æ‘˜è¦åˆ°æ§åˆ¶å°
    print("\n" + "="*80)
    print("ğŸ“Š è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹ - æ•°æ®åŸºæœ¬æè¿°æ€§ä¿¡æ¯")
    print("="*80)
    print(f"\nğŸ“‹ æ•°æ®æ¦‚è§ˆ: {analysis['basic_info']['shape'][0]:,} è¡Œ Ã— {analysis['basic_info']['shape'][1]} åˆ—")
    print(f"ğŸ“ˆ æ•°æ®å®Œæ•´æ€§: {analysis['data_completeness']:.2f}%")
    print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    print("="*80)

def main():
    """
    ä¸»å‡½æ•°
    """
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_path = "/Users/zihao_/Documents/coding/dataset/formatted/order_observation_data_merged.parquet"
    
    try:
        logger.info("å¯åŠ¨è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹è„šæœ¬...")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        initialize_database()
        
        # åŠ è½½å¹¶èšåˆæ•°æ®ï¼ˆè¿™ä¼šç”Ÿæˆèšåˆæ•°æ®æ–‡ä»¶ï¼‰
        aggregated_data = load_real_order_data()
        
        # åŠ è½½åŸå§‹æ•°æ®ç”¨äºåˆ†æ
        data = load_order_data(data_path)
        
        # åˆ†ææ•°æ®ç»“æ„
        analysis = analyze_data_structure(data)
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_order_trend_report(data, analysis, data_path)
        
        logger.info("è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹è„šæœ¬æ‰§è¡Œå®Œæˆ")
        
    except Exception as e:
        logger.error(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {str(e)}")
        print(f"\nâŒ é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()