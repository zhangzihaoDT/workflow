#!/usr/bin/env python3
"""
è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹è„šæœ¬
ç”¨äºå¤„ç†è®¢å•è§‚å¯Ÿæ•°æ®å¹¶è¿›è¡ŒåŸºæœ¬æè¿°æ€§åˆ†æ
"""

import pandas as pd
import pyarrow.parquet as pq
from typing import Dict, Any, List
import logging
from pathlib import Path
import datetime
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_order_data(data_path: str) -> pd.DataFrame:
    """
    åŠ è½½è®¢å•æ•°æ®
    """
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {data_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        # åŠ è½½parquetæ–‡ä»¶
        data = pd.read_parquet(data_path)
        logger.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
        
        return data
        
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        raise

def analyze_data_structure(data: pd.DataFrame) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®ç»“æ„å’ŒåŸºæœ¬ä¿¡æ¯
    """
    logger.info("å¼€å§‹åˆ†ææ•°æ®ç»“æ„...")
    
    # åŸºæœ¬ä¿¡æ¯
    basic_info = {
        'shape': data.shape,
        'total_rows': len(data),
        'total_columns': len(data.columns),
        'memory_usage_mb': data.memory_usage(deep=True).sum() / 1024 / 1024
    }
    
    # åˆ—ä¿¡æ¯
    column_info = []
    for col in data.columns:
        col_info = {
            'column_name': col,
            'data_type': str(data[col].dtype),
            'non_null_count': data[col].count(),
            'null_count': data[col].isnull().sum(),
            'null_percentage': (data[col].isnull().sum() / len(data)) * 100,
            'unique_count': data[col].nunique()
        }
        
        # å¦‚æœæ˜¯æ•°å€¼ç±»å‹ï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if pd.api.types.is_numeric_dtype(data[col]):
            col_info.update({
                'min_value': data[col].min(),
                'max_value': data[col].max(),
                'mean_value': data[col].mean(),
                'std_value': data[col].std()
            })
        
        column_info.append(col_info)
    
    # æ•°æ®ç±»å‹åˆ†ç±»
    numeric_columns = data.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()
    categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()
    datetime_columns = data.select_dtypes(include=['datetime64']).columns.tolist()
    
    # ç¼ºå¤±å€¼ç»Ÿè®¡
    missing_values = data.isnull().sum()
    missing_cells = missing_values.sum()
    
    # é‡å¤è¡Œæ£€æŸ¥
    duplicate_rows = data.duplicated().sum()
    
    # æ•°æ®å®Œæ•´æ€§
    total_cells = data.shape[0] * data.shape[1]
    data_completeness = ((total_cells - missing_cells) / total_cells) * 100 if total_cells > 0 else 0
    
    analysis_result = {
        'basic_info': basic_info,
        'column_info': column_info,
        'numeric_columns': numeric_columns,
        'categorical_columns': categorical_columns,
        'datetime_columns': datetime_columns,
        'missing_values': missing_values,
        'missing_cells': missing_cells,
        'duplicate_rows': duplicate_rows,
        'data_completeness': data_completeness
    }
    
    logger.info("æ•°æ®ç»“æ„åˆ†æå®Œæˆ")
    return analysis_result

def generate_order_trend_report(data: pd.DataFrame, analysis: Dict[str, Any], data_path: str) -> None:
    """
    ç”Ÿæˆè®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹æŠ¥å‘Š
    """
    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.datetime.now().isoformat()
    
    # æ„å»ºæŠ¥å‘Šå†…å®¹
    report_content = f"""# è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹æŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ
- **ç”Ÿæˆæ—¶é—´**: {current_time}
- **æ•°æ®æ–‡ä»¶**: {data_path}
- **åˆ†æç±»å‹**: è®¢å•è§‚å¯Ÿæ•°æ®åŸºæœ¬æè¿°æ€§åˆ†æ

---

# æ•°æ®åŸºæœ¬ä¿¡æ¯

## æ•°æ®æ¦‚è§ˆ
- **æ•°æ®å½¢çŠ¶**: {analysis['basic_info']['shape'][0]:,} è¡Œ Ã— {analysis['basic_info']['shape'][1]} åˆ—
- **å†…å­˜ä½¿ç”¨**: {analysis['basic_info']['memory_usage_mb']:.2f} MB
- **æ•°æ®å®Œæ•´æ€§**: {analysis['data_completeness']:.2f}%
- **é‡å¤è¡Œæ•°**: {analysis['duplicate_rows']:,}

## æ•°æ®ç±»å‹åˆ†å¸ƒ
- **æ•°å€¼åˆ—**: {len(analysis['numeric_columns'])} ä¸ª
- **åˆ†ç±»åˆ—**: {len(analysis['categorical_columns'])} ä¸ª
- **æ—¥æœŸåˆ—**: {len(analysis['datetime_columns'])} ä¸ª

## å­—æ®µä¿¡æ¯è¯¦æƒ…

### å…¨éƒ¨å­—æ®µåˆ—è¡¨

| åºå· | å­—æ®µåç§° | æ•°æ®ç±»å‹ | éç©ºæ•°é‡ | ç¼ºå¤±ç‡ | å”¯ä¸€å€¼æ•°é‡ |
|------|----------|----------|----------|--------|------------|
"""
    
    # æ·»åŠ å­—æ®µä¿¡æ¯è¡¨æ ¼
    for i, col_info in enumerate(analysis['column_info'], 1):
        report_content += f"| {i} | {col_info['column_name']} | {col_info['data_type']} | {col_info['non_null_count']:,} | {col_info['null_percentage']:.2f}% | {col_info['unique_count']:,} |\n"
    
    # æ•°å€¼åˆ—ç»Ÿè®¡ä¿¡æ¯
    if analysis['numeric_columns']:
        report_content += "\n### æ•°å€¼åˆ—ç»Ÿè®¡ä¿¡æ¯\n\n"
        report_content += "| å­—æ®µåç§° | æœ€å°å€¼ | æœ€å¤§å€¼ | å¹³å‡å€¼ | æ ‡å‡†å·® |\n"
        report_content += "|----------|--------|--------|--------|--------|\n"
        
        for col_info in analysis['column_info']:
            if col_info['column_name'] in analysis['numeric_columns']:
                min_val = col_info.get('min_value', 'N/A')
                max_val = col_info.get('max_value', 'N/A')
                mean_val = col_info.get('mean_value', 'N/A')
                std_val = col_info.get('std_value', 'N/A')
                
                # æ ¼å¼åŒ–æ•°å€¼æ˜¾ç¤º
                if isinstance(min_val, (int, float)):
                    min_val = f"{min_val:,.2f}" if isinstance(min_val, float) else f"{min_val:,}"
                if isinstance(max_val, (int, float)):
                    max_val = f"{max_val:,.2f}" if isinstance(max_val, float) else f"{max_val:,}"
                if isinstance(mean_val, (int, float)):
                    mean_val = f"{mean_val:,.2f}"
                if isinstance(std_val, (int, float)):
                    std_val = f"{std_val:,.2f}"
                
                report_content += f"| {col_info['column_name']} | {min_val} | {max_val} | {mean_val} | {std_val} |\n"
    else:
        report_content += "\n### æ•°å€¼åˆ—ç»Ÿè®¡ä¿¡æ¯\n\næ— æ•°å€¼åˆ—\n"
    
    # åˆ†ç±»åˆ—ä¿¡æ¯
    if analysis['categorical_columns']:
        report_content += "\n### åˆ†ç±»åˆ—è¯¦æƒ…\n\n"
        report_content += ', '.join(analysis['categorical_columns'])
        report_content += "\n"
    
    # æ—¥æœŸåˆ—ä¿¡æ¯
    if analysis['datetime_columns']:
        report_content += "\n### æ—¥æœŸåˆ—è¯¦æƒ…\n\n"
        report_content += ', '.join(analysis['datetime_columns'])
        report_content += "\n"
    
    # æ•°æ®è´¨é‡åˆ†æ
    report_content += "\n## æ•°æ®è´¨é‡åˆ†æ\n\n"
    
    # ç¼ºå¤±å€¼åˆ†æ
    if analysis['missing_cells'] > 0:
        report_content += "### ç¼ºå¤±å€¼åˆ†æ\n\n"
        report_content += "| å­—æ®µåç§° | ç¼ºå¤±æ•°é‡ | ç¼ºå¤±æ¯”ä¾‹ |\n"
        report_content += "|----------|----------|----------|\n"
        
        for col, missing_count in analysis['missing_values'].items():
            if missing_count > 0:
                missing_pct = (missing_count / len(data)) * 100
                report_content += f"| {col} | {missing_count:,} | {missing_pct:.2f}% |\n"
    else:
        report_content += "### ç¼ºå¤±å€¼æ£€æŸ¥\n\nâœ… æœªå‘ç°ç¼ºå¤±å€¼\n"
    
    # é‡å¤æ•°æ®æ£€æŸ¥
    if analysis['duplicate_rows'] > 0:
        report_content += f"\n### é‡å¤æ•°æ®æ£€æŸ¥\n\nâš ï¸ å‘ç° {analysis['duplicate_rows']:,} è¡Œé‡å¤æ•°æ®\n"
    else:
        report_content += "\n### é‡å¤æ•°æ®æ£€æŸ¥\n\nâœ… æœªå‘ç°é‡å¤æ•°æ®\n"
    
    # æ•°æ®è´¨é‡è¯„çº§
    data_completeness = analysis['data_completeness']
    if data_completeness >= 95:
        quality_grade = "ä¼˜ç§€ âœ…"
    elif data_completeness >= 90:
        quality_grade = "è‰¯å¥½ âš ï¸"
    elif data_completeness >= 80:
        quality_grade = "ä¸€èˆ¬ âš ï¸"
    else:
        quality_grade = "è¾ƒå·® âŒ"
    
    report_content += f"\n## æ•°æ®è´¨é‡è¯„ä¼°\n\n- **æ•´ä½“è¯„çº§**: {quality_grade}\n- **å®Œæ•´æ€§å¾—åˆ†**: {data_completeness:.2f}%\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/zihao_/Documents/github/W35_workflow/order_trend_analysis_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"\nğŸ“Š è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # åŒæ—¶æ‰“å°ç®€è¦æ‘˜è¦åˆ°æ§åˆ¶å°
    print("\n" + "="*80)
    print("ğŸ“Š è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹ - æ•°æ®åŸºæœ¬æè¿°æ€§ä¿¡æ¯")
    print("="*80)
    print(f"\nğŸ“‹ æ•°æ®æ¦‚è§ˆ: {analysis['basic_info']['shape'][0]:,} è¡Œ Ã— {analysis['basic_info']['shape'][1]} åˆ—")
    print(f"ğŸ“ˆ æ•°æ®å®Œæ•´æ€§: {analysis['data_completeness']:.2f}%")
    print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    print("="*80)

def main():
    """
    ä¸»å‡½æ•°
    """
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_path = "/Users/zihao_/Documents/coding/dataset/formatted/order_observation_data.parquet"
    
    try:
        logger.info("å¯åŠ¨è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹è„šæœ¬...")
        
        # åŠ è½½æ•°æ®
        data = load_order_data(data_path)
        
        # åˆ†ææ•°æ®ç»“æ„
        analysis = analyze_data_structure(data)
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_order_trend_report(data, analysis, data_path)
        
        logger.info("è®¢å•è¶‹åŠ¿çº¿ç›‘æµ‹è„šæœ¬æ‰§è¡Œå®Œæˆ")
        
    except Exception as e:
        logger.error(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {str(e)}")
        print(f"\nâŒ é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()